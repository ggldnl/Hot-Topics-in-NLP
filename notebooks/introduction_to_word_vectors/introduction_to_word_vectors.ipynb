{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo6--yCcFeyD"
      },
      "source": [
        "# Word Embeddings in Python with gensim\n",
        "\n",
        "Word2Vec is a group of related methods to learn word embeddings using shallow neural networks. Given a large corpus of text, these techniques allow to learn a representation for each word such that words that share common contexts in the corpus are located close to each other in the vector space.\n",
        "###Skip-Gram Model\n",
        "It is one of the two word2vec methods to build word embeddings. We want words that appear in the same context to have similar representations, so we will use as training instance a given word and as its label a word that appears in the same context.\n",
        "\n",
        "The model has the following architecture:\n",
        "\n",
        "![Model architecture](https://miro.medium.com/max/3138/0*FTfdlZ7yDBoQ8c9W.png)\n",
        "\n",
        "The weights of the dense layers will be updated during training, and we will use the weights of the first layer as word embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget # to download data\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQkf8D533vFS",
        "outputId": "64920938-2f67-4f23-90f5-431bc3102b30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=7833e875e601a7926be2c26c703aa23efe5c41f09c5acf99c49aa9eaa33cbd79\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
            "2023-10-25 05:46:12.779005: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-10-25 05:46:12.779067: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-10-25 05:46:12.779106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-10-25 05:46:12.788497: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-10-25 05:46:14.024404: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUNtF-G6FeyJ"
      },
      "source": [
        "## Load Libraries\n",
        "\n",
        "We're using a new library called `gensim`.  It's a great library for modeling text and comes with pre-trained models that you can easily use in other contexts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-20T00:07:48.246163Z",
          "start_time": "2019-01-20T00:07:46.748980Z"
        },
        "id": "BhvSJETNFeyN"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import gensim\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import wget\n",
        "import spacy\n",
        "import scipy.stats\n",
        "\n",
        "from tqdm import tqdm\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import re\n",
        "from collections import defaultdict\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "root_folder = '/content/drive/My Drive/nlp_data/' # to save checkpoints"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "gmOcaYzZ3arL",
        "outputId": "6c8c1f54-3e8c-44c3-ea6a-40e754e07e44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-b31eddbc94ae>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mroot_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/My Drive/nlp_data/'\u001b[0m \u001b[0;31m# to save checkpoints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    130\u001b[0m   )\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG0SBtX6FeyX"
      },
      "source": [
        "### Simple preprocessing example\n",
        "\n",
        "Preprocessing is a crucial step when using Word2Vec or other word embedding techniques. It prepares the text data in a way that enhances the quality of learned embeddings, reduces noise, and improves the efficiency of training and the overall performance of natural language processing tasks such as text classification, sentiment analysis, and document retrieval.\n",
        "\n",
        "1. **Convert to lower:** This step involves changing all the characters in the text to their lowercase counterparts. It ensures that the text is case-insensitive, making it easier for further text processing.\n",
        "\n",
        "2. **Remove punctuations/symbols/numbers (but it is your choice, stopwords):** In this stage, you eliminate any punctuation marks, symbols, and numbers from the text. The specific choice of whether to remove stopwords depends on your text processing goals. Stopwords are commonly used words (e.g., \"the,\" \"and,\" \"in\") that are often removed to focus on more meaningful content words.\n",
        "\n",
        "3. **Normalize the words (lemmatize and stem the words):** Normalizing words typically involves two processes:\n",
        "\n",
        "   - **Lemmatization:** This is the process of reducing words to their base or dictionary form, known as the lemma. For example, \"running\" becomes \"run,\" \"better\" becomes \"good,\" and \"mice\" becomes \"mouse.\" Lemmatization helps reduce words to their core meaning.\n",
        "\n",
        "   - **Stemming:** Stemming involves removing prefixes or suffixes from words to obtain their root form (stem). For instance, \"jumping\" becomes \"jump,\" \"swimming\" becomes \"swim,\" and \"flies\" (as a verb) becomes \"fli.\" Stemming is a more aggressive reduction of words compared to lemmatization and may result in words that are not valid English words but can be useful for information retrieval tasks.\n",
        "4. **Remove unfrequent words:** Remove words that appear only once in the dataset\n",
        "\n",
        "Combining all these steps can help in cleaning and preparing text for various natural language processing (NLP) tasks, such as text classification, information retrieval, or sentiment analysis. The choice between lemmatization and stemming depends on the specific requirements of your NLP application."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRXwaQiE6024",
        "outputId": "06709773-5319-44a3-a11f-a59444138390"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " 'if',\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves'}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "regexp_alphbetic = re.compile('[^a-zA-Z]+')\n",
        "\n",
        "def preprocess_text(sentence, stopwords, lemmatize=True):\n",
        "  doc = nlp(sentence)\n",
        "  sentence_tokens = []\n",
        "  for token in doc:\n",
        "    token_text = token.lemma_ if lemmatize else token.text\n",
        "    token_text = token_text.lower()\n",
        "    # skip stopwords and NON alphanumeric\n",
        "    if token_text in stopwords or regexp_alphbetic.search(token_text):\n",
        "      continue\n",
        "    sentence_tokens.append(token_text)\n",
        "  return sentence_tokens\n"
      ],
      "metadata": {
        "id": "bGOwmaxK5-b-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-20T00:07:48.265144Z",
          "start_time": "2019-01-20T00:07:48.249322Z"
        },
        "id": "U78OdGE6Feya",
        "outputId": "eb1db907-5b26-4096-cecc-f079289b3297",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "documents = [\n",
        "    \"Perfect is the enemy of good.\",\n",
        "    \"I'm still learning.\",\n",
        "    \"Life is a journey, not a destination.\",\n",
        "    \"Learning is not attained by chance, it must be sought for with ardor and attended to with diligence.\",\n",
        "    \"Yesterday I was clever, so I changed the world. Today I am wise, so I am changing myself.\",\n",
        "    \"Be curious, not judgmental.\",\n",
        "    \"You don't have to be great to start, but you have to start to be great.,\"\n",
        "    \"Be stubborn about your goals and flexible about your methods.\",\n",
        "    \"Nothing will work unless you do.\",\n",
        "    \"Never give up on a dream just because of the time it will take to accomplish it. The time will pass anyway.\",\n",
        "    \"Anyone who stops learning is old, whether at twenty or eighty.\",\n",
        "    \"Tell me and I forget. Teach me and I remember. Involve me and I learn.\",\n",
        "    \"Change is the end result of all true learning.\",\n",
        "    \"Live as if you were to die tomorrow. Learn as if you were to live forever.\",\n",
        "    \"A learning curve is essential to growth.\",\n",
        "]\n",
        "\n",
        "# remove common words and tokenize\n",
        "texts = [preprocess_text(sentence, stop_words, lemmatize=True) for sentence in documents]\n",
        "\n",
        "## remove words that appear only once\n",
        "frequency = defaultdict(int)\n",
        "for text in texts:\n",
        "    for token in text:\n",
        "        frequency[token] += 1\n",
        "\n",
        "texts = [[token for token in text if frequency[token] > 1]\n",
        "         for text in texts]\n",
        "texts"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['be', 'the', 'of'],\n",
              " ['i', 'be', 'learn'],\n",
              " ['be', 'a', 'not', 'a'],\n",
              " ['learning', 'be', 'not', 'it', 'be', 'with', 'and', 'to', 'with'],\n",
              " ['i', 'be', 'so', 'i', 'change', 'the', 'i', 'be', 'so', 'i', 'be', 'change'],\n",
              " ['be', 'not'],\n",
              " ['you',\n",
              "  'do',\n",
              "  'not',\n",
              "  'have',\n",
              "  'to',\n",
              "  'be',\n",
              "  'great',\n",
              "  'to',\n",
              "  'start',\n",
              "  'you',\n",
              "  'have',\n",
              "  'to',\n",
              "  'start',\n",
              "  'to',\n",
              "  'be',\n",
              "  'great',\n",
              "  'about',\n",
              "  'your',\n",
              "  'and',\n",
              "  'about',\n",
              "  'your'],\n",
              " ['will', 'you', 'do'],\n",
              " ['a', 'of', 'the', 'time', 'it', 'will', 'to', 'it', 'the', 'time', 'will'],\n",
              " ['learn', 'be'],\n",
              " ['i', 'and', 'i', 'i', 'and', 'i', 'i', 'and', 'i', 'learn'],\n",
              " ['change', 'be', 'the', 'of', 'learning'],\n",
              " ['live',\n",
              "  'as',\n",
              "  'if',\n",
              "  'you',\n",
              "  'be',\n",
              "  'to',\n",
              "  'learn',\n",
              "  'as',\n",
              "  'if',\n",
              "  'you',\n",
              "  'be',\n",
              "  'to',\n",
              "  'live'],\n",
              " ['a', 'learning', 'be', 'to']]"
            ]
          },
          "metadata": {},
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzV1F5nqFeyj"
      },
      "source": [
        "### Creating our Word2Vec Model\n",
        "\n",
        "`gensim` makes it easy to train a Word2Vec model.  All training requires is passing in the corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-20T00:07:48.314818Z",
          "start_time": "2019-01-20T00:07:48.268115Z"
        },
        "id": "Wk-yATt6Feyl",
        "outputId": "83722a2c-9938-4f38-af0b-7cb384715335",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = gensim.models.Word2Vec(texts, vector_size=10, window=2, min_count=1)\n",
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<gensim.models.word2vec.Word2Vec at 0x7bbe442080d0>"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-20T00:07:48.326029Z",
          "start_time": "2019-01-20T00:07:48.318240Z"
        },
        "id": "_-i1FfX8Feys",
        "outputId": "6d1b5f52-e062-4926-8505-64f2eef58001",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.wv[\"live\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.0560662 ,  0.01728904, -0.00880932,  0.06781744,  0.03992345,\n",
              "        0.04525375,  0.0145028 , -0.02689059, -0.04391827, -0.01024627],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQSRHPLJFeyz"
      },
      "source": [
        "And we can find the most similar words too.  Obviously, our dataset is too small and we won't find anything too interesting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-20T00:07:48.341900Z",
          "start_time": "2019-01-20T00:07:48.329322Z"
        },
        "id": "az8FYssqFey1",
        "outputId": "674a5e27-ad0d-4bc7-c6bb-117b2ec385db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.wv.most_similar('live')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('you', 0.46276724338531494),\n",
              " ('will', 0.43382689356803894),\n",
              " ('learning', 0.42391228675842285),\n",
              " ('start', 0.39573609828948975),\n",
              " ('not', 0.3559991419315338),\n",
              " ('of', 0.21979232132434845),\n",
              " ('as', 0.21175067126750946),\n",
              " ('about', 0.17295706272125244),\n",
              " ('have', 0.12536422908306122),\n",
              " ('i', 0.11380045861005783)]"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a_7KuYYFezG"
      },
      "source": [
        "### Loading an existing corpus\n",
        "\n",
        "We can load some existing text and train a model on it.  In this case, we're going to use which is a small subset of Wikipedia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-20T00:07:48.533077Z",
          "start_time": "2019-01-20T00:07:48.345058Z"
        },
        "id": "1wc2Gi_rFezI"
      },
      "source": [
        "# wiki_10k.txt for short runs\n",
        "\n",
        "url_data = 'https://github.com/dbamman/anlp19/raw/master/data/wiki.10K.txt'\n",
        "train_data_path = wget.download(url_data)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts =  []\n",
        "\n",
        "#we limit the documents for time constrains\n",
        "MAX_DOCUMENTS = 1000\n",
        "\n",
        "with open(train_data_path) as fr:\n",
        "  count = 0\n",
        "  for line in tqdm(fr):\n",
        "    texts.append(preprocess_text(line, stop_words, lemmatize=True))\n",
        "    count += 1\n",
        "    if count > MAX_DOCUMENTS:\n",
        "      break\n",
        "\n",
        "print(len(texts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJjULtmo90lT",
        "outputId": "ddda023e-556c-4a4b-cfe1-32b4a8d44438"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1000it [02:01,  8.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## remove words that appear only once\n",
        "frequency = defaultdict(int)\n",
        "for text in texts:\n",
        "    for token in text:\n",
        "        frequency[token] += 1\n",
        "\n",
        "texts = [[token for token in text if frequency[token] > 1]\n",
        "         for text in texts]"
      ],
      "metadata": {
        "id": "izTURJQE5LL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-20T00:09:11.533094Z",
          "start_time": "2019-01-20T00:07:48.536503Z"
        },
        "id": "1MJQDoLYFezO",
        "outputId": "be5c72f5-1514-417c-e3b0-b725d6b593f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Using small numbers here, probably want to use a bigger corpus, bigger dimensions, and more iterations.\n",
        "model = gensim.models.Word2Vec(texts, vector_size=10, window=4, epochs=20, min_count=1)\n",
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<gensim.models.word2vec.Word2Vec at 0x7bbe490f2cb0>"
            ]
          },
          "metadata": {},
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sr_QsuclFezU"
      },
      "source": [
        "We can get slightly better results (but we really should be using a much bigger corpus)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-20T00:09:11.580898Z",
          "start_time": "2019-01-20T00:09:11.536816Z"
        },
        "id": "aRD9XLxMFezV",
        "outputId": "2ec74591-3f1a-4ab2-bf8f-166a0be1332c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.wv.most_similar(\"road\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('route', 0.9823903441429138),\n",
              " ('street', 0.9532665014266968),\n",
              " ('cross', 0.9493829011917114),\n",
              " ('omaha', 0.9481728076934814),\n",
              " ('spruce', 0.9461341500282288),\n",
              " ('junction', 0.9428761601448059),\n",
              " ('divisions', 0.9417228698730469),\n",
              " ('freeway', 0.9403142333030701),\n",
              " ('west', 0.9383276104927063),\n",
              " ('extend', 0.9378702640533447)]"
            ]
          },
          "metadata": {},
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-20T00:09:11.593176Z",
          "start_time": "2019-01-20T00:09:11.584425Z"
        },
        "id": "M6hKtEnJFeza",
        "outputId": "04910cd9-9879-43e3-95d7-a778fe4630ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.wv['road']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.25712892, -3.496962  ,  4.9211335 ,  0.5291255 , -1.7015144 ,\n",
              "       -1.9528776 ,  1.0085207 ,  4.5755672 , -2.9167597 ,  0.04388363],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moUaHD6YFezg"
      },
      "source": [
        "### Loading a pre-trained model\n",
        "\n",
        "We can also use the gensim to automatically download and load a pre-trained model, or alternatively load it from disk.  Since the pre-trained model has much more data, the vectors encode some semantic meaning."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n"
      ],
      "metadata": {
        "id": "6ee2CwFzBdNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-20T00:10:00.578988Z",
          "start_time": "2019-01-20T00:09:36.525757Z"
        },
        "id": "hj3GX9K8Fezt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c5b9fde-19f7-475d-9f34-9e82991bbd64"
      },
      "source": [
        "model_pretrained = api.load(\"glove-wiki-gigaword-50\")\n",
        "model_pretrained"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<gensim.models.keyedvectors.KeyedVectors at 0x7bbe490f2aa0>"
            ]
          },
          "metadata": {},
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-20T00:10:00.847470Z",
          "start_time": "2019-01-20T00:10:00.582233Z"
        },
        "id": "l56WPbt-Fezz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "651f974e-eaaa-440e-8df5-9b5d7d42f539"
      },
      "source": [
        "model_pretrained.most_similar('road')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('bridge', 0.8527506589889526),\n",
              " ('highway', 0.8253951072692871),\n",
              " ('route', 0.8184633255004883),\n",
              " ('lane', 0.8131610751152039),\n",
              " ('junction', 0.8032940030097961),\n",
              " ('roads', 0.7938767075538635),\n",
              " ('along', 0.779608428478241),\n",
              " ('west', 0.7775814533233643),\n",
              " ('intersection', 0.772043764591217),\n",
              " ('park', 0.7659005522727966)]"
            ]
          },
          "metadata": {},
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-20T00:10:00.859049Z",
          "start_time": "2019-01-20T00:10:00.850204Z"
        },
        "id": "oVsR7xchFez3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c7bb37e-fafc-48c4-c67d-ca3d7d453109"
      },
      "source": [
        "model_pretrained['road']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.10042 ,  1.06    ,  0.24829 ,  0.014362, -0.783   , -0.12697 ,\n",
              "       -0.85894 , -0.16042 ,  0.59427 , -1.069   , -1.2221  , -0.61181 ,\n",
              "       -1.1446  , -1.3356  , -0.93968 ,  0.37353 ,  0.75405 ,  0.37777 ,\n",
              "       -0.52882 ,  0.024955,  0.31032 ,  0.083344, -0.59232 ,  0.83623 ,\n",
              "        0.65468 , -1.1154  ,  0.47597 ,  0.77803 ,  0.84934 , -0.82595 ,\n",
              "        2.8725  , -0.1032  ,  0.25725 ,  0.074587,  0.95345 , -0.027788,\n",
              "        0.60115 , -0.15205 , -0.50584 ,  0.58003 , -0.58731 , -0.72368 ,\n",
              "       -0.057061, -0.28228 , -0.42823 ,  0.21001 ,  0.22496 , -1.2876  ,\n",
              "        0.87487 , -0.6231  ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 204
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiZbTr2tFez8"
      },
      "source": [
        "### SimLex999 evaluation\n",
        "\n",
        "As we saw in the slides, we can visualize the distance between words using T-SNE."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "simlex_data = wget.download(\"https://fh295.github.io/SimLex-999.zip\")"
      ],
      "metadata": {
        "id": "RzmDzeawGCXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract zip file"
      ],
      "metadata": {
        "id": "8OxDHxxvHZpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "simple_simlex_path = \"simlex999.txt\"\n",
        "\n",
        "simplex_pairs = dict()\n",
        "with zipfile.ZipFile(simlex_data, 'r') as zip, open(simple_simlex_path, \"wb\") as fw:\n",
        "   with zip.open('SimLex-999/SimLex-999.txt') as myfile:\n",
        "    next(myfile)\n",
        "    for line in myfile:\n",
        "      w1, w2, pos, score, *_ = line.strip().split()\n",
        "      simplex_pairs[(w1.decode('utf-8'), w2.decode('utf-8'))] = float(score)"
      ],
      "metadata": {
        "id": "PCp6758UGHb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simplex_pairs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32k6_x5mG2Ul",
        "outputId": "6afa86b7-ddd2-421c-cbed-b3ab45ce4b41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('old', 'new'): 1.58,\n",
              " ('smart', 'intelligent'): 9.2,\n",
              " ('hard', 'difficult'): 8.77,\n",
              " ('happy', 'cheerful'): 9.55,\n",
              " ('hard', 'easy'): 0.95,\n",
              " ('fast', 'rapid'): 8.75,\n",
              " ('happy', 'glad'): 9.17,\n",
              " ('short', 'long'): 1.23,\n",
              " ('stupid', 'dumb'): 9.58,\n",
              " ('weird', 'strange'): 8.93,\n",
              " ('wide', 'narrow'): 1.03,\n",
              " ('bad', 'awful'): 8.42,\n",
              " ('easy', 'difficult'): 0.58,\n",
              " ('bad', 'terrible'): 7.78,\n",
              " ('hard', 'simple'): 1.38,\n",
              " ('smart', 'dumb'): 0.55,\n",
              " ('insane', 'crazy'): 9.57,\n",
              " ('happy', 'mad'): 0.95,\n",
              " ('large', 'huge'): 9.47,\n",
              " ('hard', 'tough'): 8.05,\n",
              " ('new', 'fresh'): 6.83,\n",
              " ('sharp', 'dull'): 0.6,\n",
              " ('quick', 'rapid'): 9.7,\n",
              " ('dumb', 'foolish'): 6.67,\n",
              " ('wonderful', 'terrific'): 8.63,\n",
              " ('strange', 'odd'): 9.02,\n",
              " ('happy', 'angry'): 1.28,\n",
              " ('narrow', 'broad'): 1.18,\n",
              " ('simple', 'easy'): 9.4,\n",
              " ('old', 'fresh'): 0.87,\n",
              " ('apparent', 'obvious'): 8.47,\n",
              " ('inexpensive', 'cheap'): 8.72,\n",
              " ('nice', 'generous'): 5.0,\n",
              " ('weird', 'normal'): 0.72,\n",
              " ('weird', 'odd'): 9.2,\n",
              " ('bad', 'immoral'): 7.62,\n",
              " ('sad', 'funny'): 0.95,\n",
              " ('wonderful', 'great'): 8.05,\n",
              " ('guilty', 'ashamed'): 6.38,\n",
              " ('beautiful', 'wonderful'): 6.5,\n",
              " ('confident', 'sure'): 8.27,\n",
              " ('dumb', 'dense'): 7.27,\n",
              " ('large', 'big'): 9.55,\n",
              " ('nice', 'cruel'): 0.67,\n",
              " ('impatient', 'anxious'): 6.03,\n",
              " ('big', 'broad'): 6.73,\n",
              " ('strong', 'proud'): 3.17,\n",
              " ('unnecessary', 'necessary'): 0.63,\n",
              " ('restless', 'young'): 1.6,\n",
              " ('dumb', 'intelligent'): 0.75,\n",
              " ('bad', 'great'): 0.35,\n",
              " ('difficult', 'simple'): 0.87,\n",
              " ('necessary', 'important'): 7.37,\n",
              " ('bad', 'terrific'): 0.65,\n",
              " ('mad', 'glad'): 1.45,\n",
              " ('honest', 'guilty'): 1.18,\n",
              " ('easy', 'tough'): 0.52,\n",
              " ('easy', 'flexible'): 4.1,\n",
              " ('certain', 'sure'): 8.42,\n",
              " ('essential', 'necessary'): 8.97,\n",
              " ('different', 'normal'): 1.08,\n",
              " ('sly', 'clever'): 7.25,\n",
              " ('crucial', 'important'): 8.82,\n",
              " ('harsh', 'cruel'): 8.18,\n",
              " ('childish', 'foolish'): 5.5,\n",
              " ('scarce', 'rare'): 9.17,\n",
              " ('friendly', 'generous'): 5.9,\n",
              " ('fragile', 'frigid'): 2.38,\n",
              " ('long', 'narrow'): 3.57,\n",
              " ('big', 'heavy'): 6.18,\n",
              " ('rough', 'frigid'): 2.47,\n",
              " ('bizarre', 'strange'): 9.37,\n",
              " ('illegal', 'immoral'): 4.28,\n",
              " ('bad', 'guilty'): 4.2,\n",
              " ('modern', 'ancient'): 0.73,\n",
              " ('new', 'ancient'): 0.23,\n",
              " ('dull', 'funny'): 0.55,\n",
              " ('happy', 'young'): 2.0,\n",
              " ('easy', 'big'): 1.12,\n",
              " ('great', 'awful'): 1.17,\n",
              " ('tiny', 'huge'): 0.6,\n",
              " ('polite', 'proper'): 7.63,\n",
              " ('modest', 'ashamed'): 2.65,\n",
              " ('exotic', 'rare'): 8.05,\n",
              " ('dumb', 'clever'): 1.17,\n",
              " ('delightful', 'wonderful'): 8.65,\n",
              " ('noticeable', 'obvious'): 8.48,\n",
              " ('afraid', 'anxious'): 5.07,\n",
              " ('formal', 'proper'): 8.02,\n",
              " ('dreary', 'dull'): 8.25,\n",
              " ('delightful', 'cheerful'): 6.58,\n",
              " ('unhappy', 'mad'): 5.95,\n",
              " ('sad', 'terrible'): 5.4,\n",
              " ('sick', 'crazy'): 3.57,\n",
              " ('violent', 'angry'): 6.98,\n",
              " ('laden', 'heavy'): 5.9,\n",
              " ('dirty', 'cheap'): 1.6,\n",
              " ('elastic', 'flexible'): 7.78,\n",
              " ('hard', 'dense'): 5.9,\n",
              " ('recent', 'new'): 7.05,\n",
              " ('bold', 'proud'): 3.97,\n",
              " ('sly', 'strange'): 1.97,\n",
              " ('strange', 'sly'): 2.07,\n",
              " ('dumb', 'rare'): 0.48,\n",
              " ('sly', 'tough'): 0.58,\n",
              " ('terrific', 'mad'): 0.4,\n",
              " ('modest', 'flexible'): 0.98,\n",
              " ('fresh', 'wide'): 0.4,\n",
              " ('huge', 'dumb'): 0.48,\n",
              " ('large', 'flexible'): 0.48,\n",
              " ('dirty', 'narrow'): 0.3,\n",
              " ('wife', 'husband'): 2.3,\n",
              " ('book', 'text'): 6.35,\n",
              " ('groom', 'bride'): 3.17,\n",
              " ('night', 'day'): 1.88,\n",
              " ('south', 'north'): 2.2,\n",
              " ('plane', 'airport'): 3.65,\n",
              " ('uncle', 'aunt'): 5.5,\n",
              " ('horse', 'mare'): 8.33,\n",
              " ('bottom', 'top'): 0.7,\n",
              " ('friend', 'buddy'): 8.78,\n",
              " ('student', 'pupil'): 9.35,\n",
              " ('world', 'globe'): 6.67,\n",
              " ('leg', 'arm'): 2.88,\n",
              " ('plane', 'jet'): 8.1,\n",
              " ('woman', 'man'): 3.33,\n",
              " ('horse', 'colt'): 7.07,\n",
              " ('actress', 'actor'): 7.12,\n",
              " ('teacher', 'instructor'): 9.25,\n",
              " ('movie', 'film'): 8.87,\n",
              " ('bird', 'hawk'): 7.85,\n",
              " ('word', 'dictionary'): 3.68,\n",
              " ('money', 'salary'): 7.88,\n",
              " ('dog', 'cat'): 1.75,\n",
              " ('area', 'region'): 9.47,\n",
              " ('navy', 'army'): 6.43,\n",
              " ('book', 'literature'): 7.53,\n",
              " ('clothes', 'closet'): 3.27,\n",
              " ('sunset', 'sunrise'): 2.47,\n",
              " ('child', 'adult'): 2.98,\n",
              " ('cow', 'cattle'): 9.52,\n",
              " ('book', 'story'): 5.63,\n",
              " ('winter', 'summer'): 2.38,\n",
              " ('taxi', 'cab'): 9.2,\n",
              " ('tree', 'maple'): 5.53,\n",
              " ('bed', 'bedroom'): 3.4,\n",
              " ('roof', 'ceiling'): 7.58,\n",
              " ('disease', 'infection'): 7.15,\n",
              " ('arm', 'shoulder'): 4.85,\n",
              " ('sheep', 'lamb'): 8.42,\n",
              " ('lady', 'gentleman'): 3.42,\n",
              " ('boat', 'anchor'): 2.25,\n",
              " ('priest', 'monk'): 6.28,\n",
              " ('toe', 'finger'): 4.68,\n",
              " ('river', 'stream'): 7.3,\n",
              " ('anger', 'fury'): 8.73,\n",
              " ('date', 'calendar'): 4.42,\n",
              " ('sea', 'ocean'): 8.27,\n",
              " ('second', 'minute'): 4.62,\n",
              " ('hand', 'thumb'): 3.88,\n",
              " ('wood', 'log'): 7.3,\n",
              " ('mud', 'dirt'): 7.32,\n",
              " ('hallway', 'corridor'): 9.28,\n",
              " ('way', 'manner'): 7.62,\n",
              " ('mouse', 'cat'): 1.12,\n",
              " ('cop', 'sheriff'): 9.05,\n",
              " ('death', 'burial'): 4.93,\n",
              " ('music', 'melody'): 6.98,\n",
              " ('beer', 'alcohol'): 7.5,\n",
              " ('mouth', 'lip'): 7.1,\n",
              " ('storm', 'hurricane'): 6.38,\n",
              " ('tax', 'income'): 2.38,\n",
              " ('flower', 'violet'): 6.95,\n",
              " ('paper', 'cardboard'): 5.38,\n",
              " ('floor', 'ceiling'): 1.73,\n",
              " ('beach', 'seashore'): 8.33,\n",
              " ('rod', 'curtain'): 3.03,\n",
              " ('hound', 'fox'): 2.38,\n",
              " ('street', 'alley'): 5.48,\n",
              " ('boat', 'deck'): 4.28,\n",
              " ('car', 'horn'): 2.57,\n",
              " ('friend', 'guest'): 4.25,\n",
              " ('employer', 'employee'): 3.65,\n",
              " ('hand', 'wrist'): 3.97,\n",
              " ('ball', 'cannon'): 2.58,\n",
              " ('alcohol', 'brandy'): 6.98,\n",
              " ('victory', 'triumph'): 8.98,\n",
              " ('telephone', 'booth'): 3.63,\n",
              " ('door', 'doorway'): 5.4,\n",
              " ('motel', 'inn'): 8.17,\n",
              " ('clothes', 'cloth'): 5.47,\n",
              " ('steak', 'meat'): 7.47,\n",
              " ('nail', 'thumb'): 3.55,\n",
              " ('band', 'orchestra'): 7.08,\n",
              " ('book', 'bible'): 5.0,\n",
              " ('business', 'industry'): 7.02,\n",
              " ('winter', 'season'): 6.27,\n",
              " ('decade', 'century'): 3.48,\n",
              " ('alcohol', 'gin'): 8.65,\n",
              " ('hat', 'coat'): 2.67,\n",
              " ('window', 'door'): 3.33,\n",
              " ('arm', 'wrist'): 3.57,\n",
              " ('house', 'apartment'): 5.8,\n",
              " ('glass', 'crystal'): 6.27,\n",
              " ('wine', 'brandy'): 5.15,\n",
              " ('creator', 'maker'): 9.62,\n",
              " ('dinner', 'breakfast'): 3.33,\n",
              " ('arm', 'muscle'): 3.72,\n",
              " ('bubble', 'suds'): 8.57,\n",
              " ('bread', 'flour'): 3.33,\n",
              " ('death', 'tragedy'): 5.8,\n",
              " ('absence', 'presence'): 0.4,\n",
              " ('gun', 'cannon'): 5.68,\n",
              " ('grass', 'blade'): 4.57,\n",
              " ('ball', 'basket'): 1.67,\n",
              " ('hose', 'garden'): 1.67,\n",
              " ('boy', 'kid'): 7.5,\n",
              " ('church', 'choir'): 2.95,\n",
              " ('clothes', 'drawer'): 3.02,\n",
              " ('tower', 'bell'): 1.9,\n",
              " ('father', 'parent'): 7.07,\n",
              " ('school', 'grade'): 4.42,\n",
              " ('parent', 'adult'): 5.37,\n",
              " ('bar', 'jail'): 1.9,\n",
              " ('car', 'highway'): 3.4,\n",
              " ('dictionary', 'definition'): 6.25,\n",
              " ('door', 'cellar'): 1.97,\n",
              " ('army', 'legion'): 5.95,\n",
              " ('metal', 'aluminum'): 7.25,\n",
              " ('chair', 'bench'): 6.67,\n",
              " ('cloud', 'fog'): 6.0,\n",
              " ('boy', 'son'): 6.75,\n",
              " ('water', 'ice'): 6.47,\n",
              " ('bed', 'blanket'): 3.02,\n",
              " ('attorney', 'lawyer'): 9.35,\n",
              " ('area', 'zone'): 8.33,\n",
              " ('business', 'company'): 9.02,\n",
              " ('clothes', 'fabric'): 5.87,\n",
              " ('sweater', 'jacket'): 7.15,\n",
              " ('money', 'capital'): 6.67,\n",
              " ('hand', 'foot'): 4.17,\n",
              " ('alcohol', 'cocktail'): 6.73,\n",
              " ('yard', 'inch'): 3.78,\n",
              " ('molecule', 'atom'): 6.45,\n",
              " ('lens', 'camera'): 4.28,\n",
              " ('meal', 'dinner'): 7.15,\n",
              " ('eye', 'tear'): 3.55,\n",
              " ('god', 'devil'): 1.8,\n",
              " ('loop', 'belt'): 3.1,\n",
              " ('rat', 'mouse'): 7.78,\n",
              " ('motor', 'engine'): 8.65,\n",
              " ('car', 'cab'): 7.42,\n",
              " ('cat', 'lion'): 6.75,\n",
              " ('size', 'magnitude'): 6.33,\n",
              " ('reality', 'fantasy'): 1.03,\n",
              " ('door', 'gate'): 5.25,\n",
              " ('cat', 'pet'): 5.95,\n",
              " ('tin', 'aluminum'): 6.42,\n",
              " ('bone', 'jaw'): 4.17,\n",
              " ('cereal', 'wheat'): 3.75,\n",
              " ('house', 'key'): 1.9,\n",
              " ('blood', 'flesh'): 4.28,\n",
              " ('door', 'corridor'): 3.73,\n",
              " ('god', 'spirit'): 7.3,\n",
              " ('capability', 'competence'): 7.62,\n",
              " ('abundance', 'plenty'): 8.97,\n",
              " ('sofa', 'chair'): 6.67,\n",
              " ('wall', 'brick'): 4.68,\n",
              " ('horn', 'drum'): 2.68,\n",
              " ('organ', 'liver'): 6.15,\n",
              " ('strength', 'might'): 7.07,\n",
              " ('phrase', 'word'): 5.48,\n",
              " ('band', 'parade'): 3.92,\n",
              " ('stomach', 'waist'): 5.9,\n",
              " ('cloud', 'storm'): 5.6,\n",
              " ('joy', 'pride'): 5.0,\n",
              " ('noise', 'rattle'): 6.17,\n",
              " ('rain', 'mist'): 5.97,\n",
              " ('beer', 'beverage'): 5.42,\n",
              " ('man', 'uncle'): 3.92,\n",
              " ('apple', 'juice'): 2.88,\n",
              " ('intelligence', 'logic'): 6.5,\n",
              " ('communication', 'language'): 7.47,\n",
              " ('mink', 'fur'): 6.83,\n",
              " ('mob', 'crowd'): 7.85,\n",
              " ('shore', 'coast'): 8.83,\n",
              " ('wire', 'cord'): 7.62,\n",
              " ('bird', 'turkey'): 6.58,\n",
              " ('bed', 'crib'): 7.3,\n",
              " ('competence', 'ability'): 7.5,\n",
              " ('cloud', 'haze'): 7.32,\n",
              " ('supper', 'meal'): 7.53,\n",
              " ('bar', 'cage'): 2.8,\n",
              " ('water', 'salt'): 1.3,\n",
              " ('sense', 'intuition'): 7.68,\n",
              " ('situation', 'condition'): 6.58,\n",
              " ('crime', 'theft'): 7.53,\n",
              " ('style', 'fashion'): 8.5,\n",
              " ('boundary', 'border'): 9.08,\n",
              " ('arm', 'body'): 4.05,\n",
              " ('boat', 'car'): 2.37,\n",
              " ('sandwich', 'lunch'): 6.3,\n",
              " ('bride', 'princess'): 2.8,\n",
              " ('heroine', 'hero'): 8.78,\n",
              " ('car', 'gauge'): 1.13,\n",
              " ('insect', 'bee'): 6.07,\n",
              " ('crib', 'cradle'): 8.55,\n",
              " ('animal', 'person'): 3.05,\n",
              " ('marijuana', 'herb'): 6.5,\n",
              " ('bed', 'hospital'): 0.92,\n",
              " ('cheek', 'tongue'): 4.52,\n",
              " ('disc', 'computer'): 3.2,\n",
              " ('curve', 'angle'): 3.33,\n",
              " ('grass', 'moss'): 5.0,\n",
              " ('school', 'law'): 1.13,\n",
              " ('foot', 'head'): 2.3,\n",
              " ('mother', 'guardian'): 6.5,\n",
              " ('orthodontist', 'dentist'): 8.27,\n",
              " ('alcohol', 'whiskey'): 7.27,\n",
              " ('mouth', 'tooth'): 6.3,\n",
              " ('breakfast', 'bacon'): 4.37,\n",
              " ('bathroom', 'bedroom'): 3.4,\n",
              " ('plate', 'bowl'): 5.23,\n",
              " ('meat', 'bacon'): 5.8,\n",
              " ('air', 'helium'): 3.63,\n",
              " ('worker', 'employer'): 5.37,\n",
              " ('body', 'chest'): 4.45,\n",
              " ('son', 'father'): 3.82,\n",
              " ('heart', 'surgery'): 1.08,\n",
              " ('woman', 'secretary'): 1.98,\n",
              " ('man', 'father'): 4.83,\n",
              " ('beach', 'island'): 5.6,\n",
              " ('story', 'topic'): 5.0,\n",
              " ('game', 'fun'): 3.42,\n",
              " ('weekend', 'week'): 4.0,\n",
              " ('couple', 'pair'): 8.33,\n",
              " ('woman', 'wife'): 5.72,\n",
              " ('sheep', 'cattle'): 4.77,\n",
              " ('purse', 'bag'): 8.33,\n",
              " ('ceiling', 'cathedral'): 2.42,\n",
              " ('bean', 'coffee'): 5.15,\n",
              " ('wood', 'paper'): 2.88,\n",
              " ('top', 'side'): 1.9,\n",
              " ('crime', 'fraud'): 5.65,\n",
              " ('pain', 'harm'): 5.38,\n",
              " ('lover', 'companion'): 5.97,\n",
              " ('evening', 'dusk'): 7.78,\n",
              " ('father', 'daughter'): 2.62,\n",
              " ('wine', 'liquor'): 7.85,\n",
              " ('cow', 'goat'): 2.93,\n",
              " ('belief', 'opinion'): 7.7,\n",
              " ('reality', 'illusion'): 1.42,\n",
              " ('pact', 'agreement'): 9.02,\n",
              " ('wealth', 'poverty'): 1.27,\n",
              " ('accident', 'emergency'): 4.93,\n",
              " ('battle', 'conquest'): 7.22,\n",
              " ('friend', 'teacher'): 2.62,\n",
              " ('illness', 'infection'): 6.9,\n",
              " ('game', 'trick'): 2.32,\n",
              " ('brother', 'son'): 3.48,\n",
              " ('aunt', 'nephew'): 3.1,\n",
              " ('worker', 'mechanic'): 4.92,\n",
              " ('doctor', 'orthodontist'): 5.58,\n",
              " ('oak', 'maple'): 6.03,\n",
              " ('bee', 'queen'): 3.27,\n",
              " ('car', 'bicycle'): 3.47,\n",
              " ('goal', 'quest'): 5.83,\n",
              " ('august', 'month'): 5.53,\n",
              " ('army', 'squad'): 5.08,\n",
              " ('cloud', 'weather'): 4.87,\n",
              " ('physician', 'doctor'): 8.88,\n",
              " ('canyon', 'valley'): 6.75,\n",
              " ('river', 'valley'): 1.67,\n",
              " ('sun', 'sky'): 2.27,\n",
              " ('target', 'arrow'): 3.25,\n",
              " ('chocolate', 'pie'): 2.27,\n",
              " ('circumstance', 'situation'): 7.85,\n",
              " ('opinion', 'choice'): 5.43,\n",
              " ('rhythm', 'melody'): 6.12,\n",
              " ('gut', 'nerve'): 4.93,\n",
              " ('day', 'dawn'): 5.47,\n",
              " ('cattle', 'beef'): 7.03,\n",
              " ('doctor', 'professor'): 4.65,\n",
              " ('arm', 'vein'): 3.65,\n",
              " ('room', 'bath'): 3.33,\n",
              " ('corporation', 'business'): 9.02,\n",
              " ('fun', 'football'): 1.97,\n",
              " ('hill', 'cliff'): 4.28,\n",
              " ('bone', 'ankle'): 3.82,\n",
              " ('apple', 'candy'): 2.08,\n",
              " ('helper', 'maid'): 5.58,\n",
              " ('leader', 'manager'): 7.27,\n",
              " ('lemon', 'tea'): 1.6,\n",
              " ('bee', 'ant'): 2.78,\n",
              " ('basketball', 'baseball'): 4.92,\n",
              " ('rice', 'bean'): 2.72,\n",
              " ('bed', 'furniture'): 6.08,\n",
              " ('emotion', 'passion'): 7.72,\n",
              " ('anarchy', 'chaos'): 7.93,\n",
              " ('crime', 'violation'): 7.12,\n",
              " ('machine', 'engine'): 5.58,\n",
              " ('beach', 'sea'): 4.68,\n",
              " ('alley', 'bowl'): 1.53,\n",
              " ('jar', 'bottle'): 7.83,\n",
              " ('strength', 'capability'): 5.28,\n",
              " ('seed', 'mustard'): 3.48,\n",
              " ('guitar', 'drum'): 3.78,\n",
              " ('opinion', 'idea'): 5.7,\n",
              " ('north', 'west'): 3.63,\n",
              " ('diet', 'salad'): 2.98,\n",
              " ('mother', 'wife'): 3.02,\n",
              " ('dad', 'mother'): 3.55,\n",
              " ('captain', 'sailor'): 5.0,\n",
              " ('meter', 'yard'): 5.6,\n",
              " ('beer', 'champagne'): 4.45,\n",
              " ('motor', 'boat'): 2.57,\n",
              " ('card', 'bridge'): 1.97,\n",
              " ('science', 'psychology'): 4.92,\n",
              " ('sinner', 'saint'): 1.6,\n",
              " ('destruction', 'construction'): 0.98,\n",
              " ('crowd', 'bunch'): 7.42,\n",
              " ('beach', 'reef'): 3.77,\n",
              " ('man', 'child'): 4.13,\n",
              " ('bread', 'cheese'): 1.95,\n",
              " ('champion', 'winner'): 8.73,\n",
              " ('celebration', 'ceremony'): 7.72,\n",
              " ('menu', 'order'): 3.62,\n",
              " ('king', 'princess'): 3.27,\n",
              " ('wealth', 'prestige'): 6.07,\n",
              " ('endurance', 'strength'): 6.58,\n",
              " ('danger', 'threat'): 8.78,\n",
              " ('god', 'priest'): 4.5,\n",
              " ('men', 'fraternity'): 3.13,\n",
              " ('buddy', 'companion'): 8.65,\n",
              " ('teacher', 'helper'): 4.28,\n",
              " ('body', 'stomach'): 3.93,\n",
              " ('tongue', 'throat'): 3.1,\n",
              " ('house', 'carpet'): 1.38,\n",
              " ('intelligence', 'skill'): 5.35,\n",
              " ('journey', 'conquest'): 4.72,\n",
              " ('god', 'prey'): 1.23,\n",
              " ('brother', 'soul'): 0.97,\n",
              " ('adversary', 'opponent'): 9.05,\n",
              " ('death', 'catastrophe'): 4.13,\n",
              " ('monster', 'demon'): 6.95,\n",
              " ('day', 'morning'): 4.87,\n",
              " ('man', 'victor'): 1.9,\n",
              " ('friend', 'guy'): 3.88,\n",
              " ('song', 'story'): 3.97,\n",
              " ('ray', 'sunshine'): 6.83,\n",
              " ('guy', 'stud'): 5.83,\n",
              " ('chicken', 'rice'): 1.43,\n",
              " ('box', 'elevator'): 1.32,\n",
              " ('butter', 'potato'): 1.22,\n",
              " ('apartment', 'furniture'): 1.28,\n",
              " ('lake', 'swamp'): 4.92,\n",
              " ('salad', 'vinegar'): 1.13,\n",
              " ('flower', 'bulb'): 4.48,\n",
              " ('cloud', 'mist'): 6.67,\n",
              " ('driver', 'pilot'): 6.28,\n",
              " ('sugar', 'honey'): 5.13,\n",
              " ('body', 'shoulder'): 2.88,\n",
              " ('idea', 'image'): 3.55,\n",
              " ('father', 'brother'): 4.2,\n",
              " ('moon', 'planet'): 5.87,\n",
              " ('ball', 'costume'): 2.32,\n",
              " ('rail', 'fence'): 5.22,\n",
              " ('room', 'bed'): 2.35,\n",
              " ('flower', 'bush'): 4.25,\n",
              " ('bone', 'knee'): 4.17,\n",
              " ('arm', 'knee'): 2.75,\n",
              " ('bottom', 'side'): 2.63,\n",
              " ('vessel', 'vein'): 5.15,\n",
              " ('cat', 'rabbit'): 2.37,\n",
              " ('meat', 'sandwich'): 2.35,\n",
              " ('belief', 'concept'): 5.08,\n",
              " ('intelligence', 'insight'): 5.9,\n",
              " ('attention', 'interest'): 7.22,\n",
              " ('attitude', 'confidence'): 4.35,\n",
              " ('right', 'justice'): 7.05,\n",
              " ('argument', 'agreement'): 1.45,\n",
              " ('depth', 'magnitude'): 6.12,\n",
              " ('medium', 'news'): 3.65,\n",
              " ('winner', 'candidate'): 2.78,\n",
              " ('birthday', 'date'): 5.08,\n",
              " ('fee', 'payment'): 7.15,\n",
              " ('bible', 'hymn'): 5.15,\n",
              " ('exit', 'doorway'): 5.5,\n",
              " ('man', 'sentry'): 3.25,\n",
              " ('aisle', 'hall'): 6.35,\n",
              " ('whiskey', 'gin'): 6.28,\n",
              " ('blood', 'marrow'): 3.4,\n",
              " ('oil', 'mink'): 1.23,\n",
              " ('floor', 'deck'): 5.55,\n",
              " ('roof', 'floor'): 2.62,\n",
              " ('door', 'floor'): 1.67,\n",
              " ('shoulder', 'head'): 3.42,\n",
              " ('wagon', 'carriage'): 7.7,\n",
              " ('car', 'carriage'): 5.13,\n",
              " ('elbow', 'ankle'): 3.13,\n",
              " ('wealth', 'fame'): 4.02,\n",
              " ('sorrow', 'shame'): 4.77,\n",
              " ('administration', 'management'): 7.25,\n",
              " ('communication', 'conversation'): 8.02,\n",
              " ('pollution', 'atmosphere'): 4.25,\n",
              " ('anatomy', 'biology'): 5.33,\n",
              " ('college', 'profession'): 3.12,\n",
              " ('book', 'topic'): 2.07,\n",
              " ('formula', 'equation'): 7.95,\n",
              " ('book', 'information'): 5.0,\n",
              " ('boy', 'partner'): 1.9,\n",
              " ('sky', 'universe'): 4.68,\n",
              " ('population', 'people'): 7.68,\n",
              " ('college', 'class'): 4.13,\n",
              " ('chief', 'mayor'): 4.85,\n",
              " ('rabbi', 'minister'): 7.62,\n",
              " ('meter', 'inch'): 5.08,\n",
              " ('polyester', 'cotton'): 5.63,\n",
              " ('lawyer', 'banker'): 1.88,\n",
              " ('violin', 'instrument'): 6.58,\n",
              " ('camp', 'cabin'): 4.2,\n",
              " ('pot', 'appliance'): 2.53,\n",
              " ('linen', 'fabric'): 7.47,\n",
              " ('whiskey', 'champagne'): 5.33,\n",
              " ('girl', 'child'): 5.38,\n",
              " ('cottage', 'cabin'): 7.72,\n",
              " ('bird', 'hen'): 7.03,\n",
              " ('racket', 'noise'): 8.1,\n",
              " ('sunset', 'evening'): 5.98,\n",
              " ('drizzle', 'rain'): 9.17,\n",
              " ('adult', 'baby'): 2.22,\n",
              " ('charcoal', 'coal'): 7.63,\n",
              " ('body', 'spine'): 4.78,\n",
              " ('head', 'nail'): 2.47,\n",
              " ('log', 'timber'): 8.05,\n",
              " ('spoon', 'cup'): 2.02,\n",
              " ('body', 'nerve'): 3.13,\n",
              " ('man', 'husband'): 5.32,\n",
              " ('bone', 'neck'): 2.53,\n",
              " ('frustration', 'anger'): 6.5,\n",
              " ('river', 'sea'): 5.72,\n",
              " ('task', 'job'): 8.87,\n",
              " ('club', 'society'): 5.23,\n",
              " ('reflection', 'image'): 7.27,\n",
              " ('prince', 'king'): 5.92,\n",
              " ('snow', 'weather'): 5.48,\n",
              " ('people', 'party'): 2.2,\n",
              " ('boy', 'brother'): 6.67,\n",
              " ('root', 'grass'): 3.55,\n",
              " ('brow', 'eye'): 3.82,\n",
              " ('money', 'pearl'): 2.1,\n",
              " ('money', 'diamond'): 3.42,\n",
              " ('vehicle', 'bus'): 6.47,\n",
              " ('cab', 'bus'): 5.6,\n",
              " ('house', 'barn'): 4.33,\n",
              " ('finger', 'palm'): 3.33,\n",
              " ('car', 'bridge'): 0.95,\n",
              " ('effort', 'difficulty'): 4.45,\n",
              " ('fact', 'insight'): 4.77,\n",
              " ('job', 'management'): 3.97,\n",
              " ('cancer', 'sickness'): 7.93,\n",
              " ('word', 'newspaper'): 2.47,\n",
              " ('composer', 'writer'): 6.58,\n",
              " ('actor', 'singer'): 4.52,\n",
              " ('shelter', 'hut'): 6.47,\n",
              " ('bathroom', 'kitchen'): 3.1,\n",
              " ('cabin', 'hut'): 6.53,\n",
              " ('door', 'kitchen'): 1.67,\n",
              " ('value', 'belief'): 7.07,\n",
              " ('wisdom', 'intelligence'): 7.47,\n",
              " ('ignorance', 'intelligence'): 1.5,\n",
              " ('happiness', 'luck'): 2.38,\n",
              " ('idea', 'scheme'): 6.75,\n",
              " ('mood', 'emotion'): 8.12,\n",
              " ('happiness', 'peace'): 6.03,\n",
              " ('despair', 'misery'): 7.22,\n",
              " ('logic', 'arithmetic'): 3.97,\n",
              " ('denial', 'confession'): 1.03,\n",
              " ('argument', 'criticism'): 5.08,\n",
              " ('aggression', 'hostility'): 8.48,\n",
              " ('hysteria', 'confusion'): 6.33,\n",
              " ('chemistry', 'theory'): 3.17,\n",
              " ('trial', 'verdict'): 3.33,\n",
              " ('comfort', 'safety'): 5.8,\n",
              " ('confidence', 'self'): 3.12,\n",
              " ('vision', 'perception'): 6.88,\n",
              " ('era', 'decade'): 5.4,\n",
              " ('biography', 'fiction'): 1.38,\n",
              " ('discussion', 'argument'): 5.48,\n",
              " ('code', 'symbol'): 6.03,\n",
              " ('danger', 'disease'): 3.0,\n",
              " ('accident', 'catastrophe'): 5.9,\n",
              " ('journey', 'trip'): 8.88,\n",
              " ('activity', 'movement'): 7.15,\n",
              " ('gossip', 'news'): 5.22,\n",
              " ('father', 'god'): 3.57,\n",
              " ('action', 'course'): 5.45,\n",
              " ('fever', 'illness'): 7.65,\n",
              " ('aviation', 'flight'): 8.18,\n",
              " ('game', 'action'): 4.85,\n",
              " ('molecule', 'air'): 3.05,\n",
              " ('home', 'state'): 2.58,\n",
              " ('word', 'literature'): 4.77,\n",
              " ('adult', 'guardian'): 6.9,\n",
              " ('newspaper', 'information'): 5.65,\n",
              " ('communication', 'television'): 5.6,\n",
              " ('cousin', 'uncle'): 4.63,\n",
              " ('author', 'reader'): 1.6,\n",
              " ('guy', 'partner'): 3.57,\n",
              " ('area', 'corner'): 2.07,\n",
              " ('ballad', 'song'): 7.53,\n",
              " ('wall', 'decoration'): 2.62,\n",
              " ('word', 'page'): 2.92,\n",
              " ('nurse', 'scientist'): 2.08,\n",
              " ('politician', 'president'): 7.38,\n",
              " ('president', 'mayor'): 5.68,\n",
              " ('book', 'essay'): 4.72,\n",
              " ('man', 'warrior'): 4.72,\n",
              " ('article', 'journal'): 6.18,\n",
              " ('breakfast', 'supper'): 4.4,\n",
              " ('crowd', 'parade'): 3.93,\n",
              " ('aisle', 'hallway'): 6.75,\n",
              " ('teacher', 'rabbi'): 4.37,\n",
              " ('hip', 'lip'): 1.43,\n",
              " ('book', 'article'): 5.43,\n",
              " ('room', 'cell'): 4.58,\n",
              " ('box', 'booth'): 3.8,\n",
              " ('daughter', 'kid'): 4.17,\n",
              " ('limb', 'leg'): 6.9,\n",
              " ('liver', 'lung'): 2.7,\n",
              " ('classroom', 'hallway'): 2.0,\n",
              " ('mountain', 'ledge'): 3.73,\n",
              " ('car', 'elevator'): 1.03,\n",
              " ('bed', 'couch'): 3.42,\n",
              " ('clothes', 'button'): 2.3,\n",
              " ('clothes', 'coat'): 5.35,\n",
              " ('kidney', 'organ'): 6.17,\n",
              " ('apple', 'sauce'): 1.43,\n",
              " ('chicken', 'steak'): 3.73,\n",
              " ('car', 'hose'): 0.87,\n",
              " ('tobacco', 'cigarette'): 7.5,\n",
              " ('student', 'professor'): 1.95,\n",
              " ('baby', 'daughter'): 5.0,\n",
              " ('pipe', 'cigar'): 6.03,\n",
              " ('milk', 'juice'): 4.05,\n",
              " ('box', 'cigar'): 1.25,\n",
              " ('apartment', 'hotel'): 3.33,\n",
              " ('cup', 'cone'): 3.17,\n",
              " ('horse', 'ox'): 3.02,\n",
              " ('throat', 'nose'): 2.8,\n",
              " ('bone', 'teeth'): 4.17,\n",
              " ('bone', 'elbow'): 3.78,\n",
              " ('bacon', 'bean'): 1.22,\n",
              " ('cup', 'jar'): 5.13,\n",
              " ('proof', 'fact'): 7.3,\n",
              " ('appointment', 'engagement'): 6.75,\n",
              " ('birthday', 'year'): 1.67,\n",
              " ('word', 'clue'): 2.53,\n",
              " ('author', 'creator'): 8.02,\n",
              " ('atom', 'carbon'): 3.1,\n",
              " ('archbishop', 'bishop'): 7.05,\n",
              " ('letter', 'paragraph'): 4.0,\n",
              " ('page', 'paragraph'): 3.03,\n",
              " ('steeple', 'chapel'): 7.08,\n",
              " ('muscle', 'bone'): 3.65,\n",
              " ('muscle', 'tongue'): 5.0,\n",
              " ('boy', 'soldier'): 2.15,\n",
              " ('belly', 'abdomen'): 8.13,\n",
              " ('guy', 'girl'): 3.33,\n",
              " ('bed', 'chair'): 3.5,\n",
              " ('clothes', 'jacket'): 5.15,\n",
              " ('gun', 'knife'): 3.65,\n",
              " ('tin', 'metal'): 5.63,\n",
              " ('bottle', 'container'): 7.93,\n",
              " ('hen', 'turkey'): 6.13,\n",
              " ('meat', 'bread'): 1.67,\n",
              " ('arm', 'bone'): 3.83,\n",
              " ('neck', 'spine'): 5.32,\n",
              " ('apple', 'lemon'): 4.05,\n",
              " ('agony', 'grief'): 7.63,\n",
              " ('assignment', 'task'): 8.7,\n",
              " ('night', 'dawn'): 2.95,\n",
              " ('dinner', 'soup'): 3.72,\n",
              " ('calf', 'bull'): 4.93,\n",
              " ('snow', 'storm'): 4.8,\n",
              " ('nail', 'hand'): 3.42,\n",
              " ('dog', 'horse'): 2.38,\n",
              " ('arm', 'neck'): 1.58,\n",
              " ('ball', 'glove'): 1.75,\n",
              " ('flu', 'fever'): 6.08,\n",
              " ('fee', 'salary'): 3.72,\n",
              " ('nerve', 'brain'): 3.88,\n",
              " ('beast', 'animal'): 7.83,\n",
              " ('dinner', 'chicken'): 2.85,\n",
              " ('girl', 'maid'): 2.93,\n",
              " ('child', 'boy'): 5.75,\n",
              " ('alcohol', 'wine'): 7.42,\n",
              " ('nose', 'mouth'): 3.73,\n",
              " ('street', 'car'): 2.38,\n",
              " ('bell', 'door'): 2.2,\n",
              " ('box', 'hat'): 1.3,\n",
              " ('belief', 'impression'): 5.95,\n",
              " ('bias', 'opinion'): 5.6,\n",
              " ('attention', 'awareness'): 8.73,\n",
              " ('anger', 'mood'): 4.1,\n",
              " ('elegance', 'style'): 5.72,\n",
              " ('beauty', 'age'): 1.58,\n",
              " ('book', 'theme'): 2.58,\n",
              " ('friend', 'mother'): 2.53,\n",
              " ('vitamin', 'iron'): 5.55,\n",
              " ('car', 'factory'): 2.75,\n",
              " ('pact', 'condition'): 2.45,\n",
              " ('chapter', 'choice'): 0.48,\n",
              " ('arithmetic', 'rhythm'): 2.35,\n",
              " ('winner', 'presence'): 1.08,\n",
              " ('belief', 'flower'): 0.4,\n",
              " ('winner', 'goal'): 3.23,\n",
              " ('trick', 'size'): 0.48,\n",
              " ('choice', 'vein'): 0.98,\n",
              " ('hymn', 'conquest'): 0.68,\n",
              " ('endurance', 'band'): 0.4,\n",
              " ('jail', 'choice'): 1.08,\n",
              " ('condition', 'boy'): 0.48,\n",
              " ('flower', 'endurance'): 0.4,\n",
              " ('hole', 'agreement'): 0.3,\n",
              " ('doctor', 'temper'): 0.48,\n",
              " ('fraternity', 'door'): 0.68,\n",
              " ('task', 'woman'): 0.68,\n",
              " ('fraternity', 'baseball'): 0.88,\n",
              " ('cent', 'size'): 0.4,\n",
              " ('presence', 'door'): 0.48,\n",
              " ('mouse', 'management'): 0.48,\n",
              " ('task', 'highway'): 0.48,\n",
              " ('liquor', 'century'): 0.4,\n",
              " ('task', 'straw'): 0.68,\n",
              " ('island', 'task'): 0.3,\n",
              " ('night', 'chapter'): 0.48,\n",
              " ('pollution', 'president'): 0.68,\n",
              " ('gun', 'trick'): 0.48,\n",
              " ('bath', 'trick'): 0.58,\n",
              " ('diet', 'apple'): 1.18,\n",
              " ('cent', 'wife'): 0.58,\n",
              " ('chapter', 'tail'): 0.3,\n",
              " ('course', 'stomach'): 0.58,\n",
              " ('hymn', 'straw'): 0.4,\n",
              " ('dentist', 'colonel'): 0.4,\n",
              " ('wife', 'straw'): 0.4,\n",
              " ('hole', 'wife'): 0.68,\n",
              " ('pupil', 'president'): 0.78,\n",
              " ('bath', 'wife'): 0.48,\n",
              " ('people', 'cent'): 0.48,\n",
              " ('formula', 'log'): 1.77,\n",
              " ('woman', 'fur'): 0.58,\n",
              " ('apple', 'sunshine'): 0.58,\n",
              " ('gun', 'dawn'): 1.18,\n",
              " ('meal', 'waist'): 0.98,\n",
              " ('camera', 'president'): 0.48,\n",
              " ('liquor', 'band'): 0.68,\n",
              " ('stomach', 'vein'): 2.35,\n",
              " ('gun', 'fur'): 0.3,\n",
              " ('couch', 'baseball'): 0.88,\n",
              " ('worker', 'camera'): 0.68,\n",
              " ('deck', 'mouse'): 0.48,\n",
              " ('rice', 'boy'): 0.4,\n",
              " ('people', 'gun'): 0.68,\n",
              " ('cliff', 'tail'): 0.3,\n",
              " ('ankle', 'window'): 0.3,\n",
              " ('princess', 'island'): 0.3,\n",
              " ('container', 'mouse'): 0.3,\n",
              " ('wagon', 'container'): 2.65,\n",
              " ('people', 'balloon'): 0.48,\n",
              " ('dollar', 'people'): 0.4,\n",
              " ('bath', 'balloon'): 0.4,\n",
              " ('stomach', 'bedroom'): 0.4,\n",
              " ('bicycle', 'bedroom'): 0.4,\n",
              " ('log', 'bath'): 0.4,\n",
              " ('bowl', 'tail'): 0.48,\n",
              " ('go', 'come'): 2.42,\n",
              " ('take', 'steal'): 6.18,\n",
              " ('listen', 'hear'): 8.17,\n",
              " ('think', 'rationalize'): 8.25,\n",
              " ('occur', 'happen'): 9.32,\n",
              " ('vanish', 'disappear'): 9.8,\n",
              " ('multiply', 'divide'): 1.75,\n",
              " ('plead', 'beg'): 9.08,\n",
              " ('begin', 'originate'): 8.2,\n",
              " ('protect', 'defend'): 9.13,\n",
              " ('kill', 'destroy'): 5.9,\n",
              " ('create', 'make'): 8.72,\n",
              " ('accept', 'reject'): 0.83,\n",
              " ('ignore', 'avoid'): 6.87,\n",
              " ('carry', 'bring'): 5.8,\n",
              " ('leave', 'enter'): 0.95,\n",
              " ('choose', 'elect'): 7.62,\n",
              " ('lose', 'fail'): 7.33,\n",
              " ('encourage', 'discourage'): 1.58,\n",
              " ('achieve', 'accomplish'): 8.57,\n",
              " ('make', 'construct'): 8.33,\n",
              " ('listen', 'obey'): 4.93,\n",
              " ('inform', 'notify'): 9.25,\n",
              " ('receive', 'give'): 1.47,\n",
              " ('borrow', 'beg'): 2.62,\n",
              " ('take', 'obtain'): 7.1,\n",
              " ('advise', 'recommend'): 8.1,\n",
              " ('imitate', 'portray'): 6.75,\n",
              " ('win', 'succeed'): 7.9,\n",
              " ('think', 'decide'): 5.13,\n",
              " ('greet', 'meet'): 6.17,\n",
              " ('agree', 'argue'): 0.77,\n",
              " ('enjoy', 'entertain'): 5.92,\n",
              " ('destroy', 'make'): 1.6,\n",
              " ('save', 'protect'): 6.58,\n",
              " ('give', 'lend'): 7.22,\n",
              " ('understand', 'know'): 7.47,\n",
              " ('take', 'receive'): 5.08,\n",
              " ('accept', 'acknowledge'): 6.88,\n",
              " ('decide', 'choose'): 8.87,\n",
              " ('accept', 'believe'): 6.75,\n",
              " ('keep', 'possess'): 8.27,\n",
              " ('roam', 'wander'): 8.83,\n",
              " ('succeed', 'fail'): 0.83,\n",
              " ('spend', 'save'): 0.55,\n",
              " ('leave', 'go'): 7.63,\n",
              " ('come', 'attend'): 8.1,\n",
              " ('know', 'believe'): 5.5,\n",
              " ('gather', 'meet'): 7.3,\n",
              " ('make', 'earn'): 7.62,\n",
              " ('forget', 'ignore'): 3.07,\n",
              " ('multiply', 'add'): 2.7,\n",
              " ('shrink', 'grow'): 0.23,\n",
              " ('arrive', 'leave'): 1.33,\n",
              " ('succeed', 'try'): 3.98,\n",
              " ('accept', 'deny'): 1.75,\n",
              " ('arrive', 'come'): 7.05,\n",
              " ('agree', 'differ'): 1.05,\n",
              " ('send', 'receive'): 1.08,\n",
              " ('win', 'dominate'): 5.68,\n",
              " ('add', 'divide'): 2.3,\n",
              " ('kill', 'choke'): 4.92,\n",
              " ('acquire', 'get'): 8.82,\n",
              " ('participate', 'join'): 7.7,\n",
              " ('leave', 'remain'): 2.53,\n",
              " ('go', 'enter'): 4.0,\n",
              " ('take', 'carry'): 5.23,\n",
              " ('forget', 'learn'): 1.18,\n",
              " ('appoint', 'elect'): 8.17,\n",
              " ('engage', 'marry'): 5.43,\n",
              " ('ask', 'pray'): 3.72,\n",
              " ('go', 'send'): 3.75,\n",
              " ('take', 'deliver'): 4.37,\n",
              " ('speak', 'hear'): 3.02,\n",
              " ('analyze', 'evaluate'): 8.03,\n",
              " ('argue', 'rationalize'): 4.2,\n",
              " ('lose', 'keep'): 1.05,\n",
              " ('compare', 'analyze'): 8.1,\n",
              " ('disorganize', 'organize'): 1.45,\n",
              " ('go', 'allow'): 3.62,\n",
              " ('take', 'possess'): 7.2,\n",
              " ('learn', 'listen'): 3.88,\n",
              " ('destroy', 'construct'): 0.92,\n",
              " ('create', 'build'): 8.48,\n",
              " ('steal', 'buy'): 1.13,\n",
              " ('kill', 'hang'): 4.45,\n",
              " ('forget', 'know'): 0.92,\n",
              " ('create', 'imagine'): 5.13,\n",
              " ('do', 'happen'): 4.23,\n",
              " ('win', 'accomplish'): 7.85,\n",
              " ('give', 'deny'): 1.43,\n",
              " ('deserve', 'earn'): 5.8,\n",
              " ('get', 'put'): 1.98,\n",
              " ('locate', 'find'): 8.73,\n",
              " ('appear', 'attend'): 6.28,\n",
              " ('know', 'comprehend'): 7.63,\n",
              " ('pretend', 'imagine'): 8.47,\n",
              " ('satisfy', 'please'): 7.67,\n",
              " ('cherish', 'keep'): 4.85,\n",
              " ('argue', 'differ'): 5.15,\n",
              " ('overcome', 'dominate'): 6.25,\n",
              " ('behave', 'obey'): 7.3,\n",
              " ('cooperate', 'participate'): 6.43,\n",
              " ('achieve', 'try'): 4.42,\n",
              " ('fail', 'discourage'): 3.33,\n",
              " ('begin', 'quit'): 1.28,\n",
              " ('say', 'participate'): 3.82,\n",
              " ('come', 'bring'): 2.42,\n",
              " ('declare', 'announce'): 9.08,\n",
              " ('read', 'comprehend'): 4.7,\n",
              " ('take', 'leave'): 2.47,\n",
              " ('proclaim', 'announce'): 8.18,\n",
              " ('acquire', 'obtain'): 8.57,\n",
              " ('conclude', 'decide'): 7.75,\n",
              " ('please', 'plead'): 2.98,\n",
              " ('argue', 'prove'): 4.83,\n",
              " ('ask', 'plead'): 6.47,\n",
              " ('find', 'disappear'): 0.77,\n",
              " ('inspect', 'examine'): 8.75,\n",
              " ('verify', 'justify'): 4.08,\n",
              " ('assume', 'predict'): 4.85,\n",
              " ('learn', 'evaluate'): 4.17,\n",
              " ('argue', 'justify'): 5.0,\n",
              " ('make', 'become'): 4.77,\n",
              " ('discover', 'originate'): 4.83,\n",
              " ('achieve', 'succeed'): 7.5,\n",
              " ('give', 'put'): 3.65,\n",
              " ('understand', 'listen'): 4.68,\n",
              " ('expand', 'grow'): 8.27,\n",
              " ('borrow', 'sell'): 1.73,\n",
              " ('keep', 'protect'): 5.4,\n",
              " ('explain', 'prove'): 4.1,\n",
              " ('assume', 'pretend'): 3.72,\n",
              " ('agree', 'please'): 4.13,\n",
              " ('forgive', 'forget'): 3.92,\n",
              " ('clarify', 'explain'): 8.33,\n",
              " ('understand', 'forgive'): 4.87,\n",
              " ('remind', 'forget'): 0.87,\n",
              " ('get', 'remain'): 1.6,\n",
              " ('realize', 'discover'): 7.47,\n",
              " ('require', 'inquire'): 1.82,\n",
              " ('ignore', 'ask'): 1.07,\n",
              " ('think', 'inquire'): 4.77,\n",
              " ('reject', 'avoid'): 4.78,\n",
              " ('argue', 'persuade'): 6.23,\n",
              " ('pursue', 'persuade'): 3.17,\n",
              " ('accept', 'forgive'): 3.73,\n",
              " ('do', 'quit'): 1.17,\n",
              " ('investigate', 'examine'): 8.1,\n",
              " ('discuss', 'explain'): 6.67,\n",
              " ('owe', 'lend'): 2.32,\n",
              " ('explore', 'discover'): 8.48,\n",
              " ('complain', 'argue'): 4.8,\n",
              " ('withdraw', 'reject'): 6.38,\n",
              " ('keep', 'borrow'): 2.25,\n",
              " ('beg', 'ask'): 6.0,\n",
              " ('arrange', 'organize'): 8.27,\n",
              " ('reduce', 'shrink'): 8.02,\n",
              " ('speak', 'acknowledge'): 4.67,\n",
              " ('give', 'borrow'): 2.22,\n",
              " ('kill', 'defend'): 2.63,\n",
              " ('disappear', 'shrink'): 5.8,\n",
              " ('deliver', 'carry'): 3.88,\n",
              " ('breathe', 'choke'): 1.37,\n",
              " ('acknowledge', 'notify'): 5.3,\n",
              " ('become', 'seem'): 2.63,\n",
              " ('pretend', 'seem'): 4.68,\n",
              " ('accomplish', 'become'): 4.0,\n",
              " ('contemplate', 'think'): 8.82,\n",
              " ('determine', 'predict'): 5.8,\n",
              " ('please', 'entertain'): 5.0,\n",
              " ('remain', 'retain'): 5.75,\n",
              " ('pretend', 'portray'): 7.03,\n",
              " ('forget', 'retain'): 0.63,\n",
              " ('want', 'choose'): 4.78,\n",
              " ('lose', 'get'): 0.77,\n",
              " ('try', 'think'): 2.62,\n",
              " ('become', 'appear'): 4.77,\n",
              " ('leave', 'ignore'): 4.42,\n",
              " ('accept', 'recommend'): 2.75,\n",
              " ('leave', 'wander'): 3.57,\n",
              " ('keep', 'give'): 1.05,\n",
              " ('give', 'allow'): 5.15,\n",
              " ('bring', 'send'): 2.97,\n",
              " ('absorb', 'learn'): 5.48,\n",
              " ('acquire', 'find'): 6.38,\n",
              " ('leave', 'appear'): 0.97,\n",
              " ('create', 'destroy'): 0.63,\n",
              " ('begin', 'go'): 7.42,\n",
              " ('get', 'buy'): 5.08,\n",
              " ('collect', 'save'): 6.67,\n",
              " ('replace', 'restore'): 5.73,\n",
              " ('join', 'add'): 8.1,\n",
              " ('join', 'marry'): 5.35,\n",
              " ('accept', 'deliver'): 1.58,\n",
              " ('attach', 'join'): 7.75,\n",
              " ('put', 'hang'): 3.0,\n",
              " ('go', 'sell'): 0.97,\n",
              " ('communicate', 'pray'): 3.55,\n",
              " ('give', 'steal'): 0.5,\n",
              " ('add', 'build'): 4.92,\n",
              " ('bring', 'restore'): 2.62,\n",
              " ('comprehend', 'satisfy'): 2.55,\n",
              " ('portray', 'decide'): 1.18,\n",
              " ('organize', 'become'): 1.77,\n",
              " ('give', 'know'): 0.88,\n",
              " ('say', 'verify'): 4.9,\n",
              " ('cooperate', 'join'): 5.18,\n",
              " ('arrange', 'require'): 0.98,\n",
              " ('borrow', 'want'): 1.77,\n",
              " ('investigate', 'pursue'): 7.15,\n",
              " ('ignore', 'explore'): 0.4,\n",
              " ('bring', 'complain'): 0.98,\n",
              " ('enter', 'owe'): 0.68,\n",
              " ('portray', 'notify'): 0.78,\n",
              " ('remind', 'sell'): 0.4,\n",
              " ('absorb', 'possess'): 5.0,\n",
              " ('join', 'acquire'): 2.85,\n",
              " ('send', 'attend'): 1.67,\n",
              " ('gather', 'attend'): 4.8,\n",
              " ('absorb', 'withdraw'): 2.97,\n",
              " ('attend', 'arrive'): 6.08}"
            ]
          },
          "metadata": {},
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute correlation between human scores and word2vec similarities\n",
        "\n",
        "In natural language processing and computational linguistics, it is often essential to evaluate the performance of word embeddings, such as Word2Vec, by comparing their semantic similarity scores with human-generated similarity scores. This evaluation helps us understand how well the model captures the relationships between words as perceived by humans.\n",
        "\n",
        "To perform this evaluation, we calculate the correlation between the human-assigned similarity scores and the similarity scores generated by the Word2Vec model. This correlation analysis provides insights into how closely the model's output aligns with human judgments."
      ],
      "metadata": {
        "id": "OIBcdUPryoIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_correlation_score(model, word_pair2score, print_warning=True):\n",
        "  human_scores = []\n",
        "  system_scores = []\n",
        "  for (w1, w2), score in word_pair2score.items():\n",
        "    if (w1 not in model) or (w2 not in model):\n",
        "      system_scores.append(-1)\n",
        "      human_scores.append(score)\n",
        "      if print_warning:\n",
        "        print(f\"WARNING ({w1} and {w2}) are not present in the embedding model!!\" )\n",
        "      continue\n",
        "    system_similarity = model.similarity(w1, w2)\n",
        "    human_scores.append(score)\n",
        "    system_scores.append(system_similarity)\n",
        "  human_scores = np.array(human_scores)\n",
        "  system_scores = np.array(system_scores)\n",
        "  pearson_r, _ = scipy.stats.pearsonr(human_scores, system_scores)    # Pearson's r\n",
        "  spearman_rho = scipy.stats.spearmanr(human_scores, system_scores).statistic   # Spearman's rho\n",
        "  return pearson_r, spearman_rho\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pxeRWj3uGzO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Correlation score of our model versus pretrained model!"
      ],
      "metadata": {
        "id": "0ADrmnfz1tCh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-20T00:10:01.154138Z",
          "start_time": "2019-01-20T00:10:00.865386Z"
        },
        "id": "iRgJC7v6Fez8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16a48a20-b516-4385-c57c-d5fb2e1e69d4"
      },
      "source": [
        "compute_correlation_score(model.wv, simplex_pairs, print_warning=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.014310638953798107, 0.03950377578426888)"
            ]
          },
          "metadata": {},
          "execution_count": 226
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compute_correlation_score(model_pretrained, simplex_pairs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWFE1_I1HtQI",
        "outputId": "ece81404-e6be-4359-a3e8-26baaecd5bba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.2941386830730656, 0.2645792192990813)"
            ]
          },
          "metadata": {},
          "execution_count": 223
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Semantic similarity\n",
        "\n",
        "We provide an adapted version of semantic correlation score that can be used to asses the performances of your senses embedding models."
      ],
      "metadata": {
        "id": "NssrXbFQ8t-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "##IMPLEMENT THIS TO LOAD sense2score\n",
        "# the returned dictionary should be similar to previous word_pair2score but instead of words we consider the senses from the dataset associated with this words\n",
        "def load_semantic_simplex(path):\n",
        "  senses2score = dict()\n",
        "  with open(path) as fr:\n",
        "    next(fr)\n",
        "    for line in fr:\n",
        "      chunks = line.strip().split()\n",
        "      w1 = chunks[0]\n",
        "      w2 = chunks[1]\n",
        "      sim_lex_score = chunks[3]\n",
        "      senses_w1 = chunks[10].split(\",\")\n",
        "      senses_w2 = chunks[11].split(\",\")\n",
        "      senses2score[tuple(tuple(senses_w1), tuple(senses_w2))] = sim_lex_score\n",
        "\n",
        "  return senses2score"
      ],
      "metadata": {
        "id": "1krTEdXK80NC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adapted the \"compute_correlation_score\"\n",
        "\n",
        "In order to tailor the \"compute_correlation_score\" function to our specific needs, we have produced an adapted version called \"compute_semantic_correlation_score.\" This modified function is designed to evaluate semantic similarity between pairs of senses and corresponding human-assigned scores.\n",
        "\n",
        "\n",
        "The \"compute_semantic_correlation_score\" function begins by collecting pairs of senses and their associated human scores.\n",
        "For those pairs the function computes the semantic similarity between each possible pair of senses. It calculates the system similarity score by averaging these individual sense-to-sense similarities. The human scores and system scores are then collected for subsequent correlation analysis.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7DyseLXk9Rot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def compute_semantic_correlation_score(model, senses2score,  print_warning=True):\n",
        "  human_scores = []\n",
        "  system_scores = []\n",
        "  for (senses_1, senses_2), score in senses2score.items():\n",
        "    senses_1_in_model = [s for s in senses_1 if s in model]\n",
        "    senses_2_in_model = [s for s in senses_2 if s in model]\n",
        "\n",
        "    if len(senses_1_in_model) == 0 or len(senses_1_in_model) == 0:\n",
        "      # sense is not present in the model\n",
        "      s1_str = \" \".join(senses_1)\n",
        "      s2_str = \" \".join(senses_2)\n",
        "      if print_warning:\n",
        "        print(f\"WARNING ({s1_str} and {s2_str}) are not present in the embedding model!!\" )\n",
        "      system_scores.append(-1)\n",
        "    # Calculate semantic similarities between all pairs of senses\n",
        "    all_similarities = []\n",
        "    for s1 in senses_1_in_model:\n",
        "      for s2 in senses_2_in_model:\n",
        "        all_similarities.append(model.similarity(s1, s2))\n",
        "\n",
        "    system_similarity = sum(all_similarities) / len(all_similarities)\n",
        "    human_scores.append(score)\n",
        "    system_scores.append(system_similarity)\n",
        "  human_scores = np.array(human_scores)\n",
        "  system_scores = np.array(system_scores)\n",
        "  # Calculate Pearson's r (Pearson correlation coefficient) and Spearman's rho (Spearman rank correlation coefficient)\n",
        "  pearson_r, _ = scipy.stats.pearsonr(human_scores, system_scores)    # Pearson's r\n",
        "  spearman_rho = scipy.stats.spearmanr(human_scores, system_scores).statistic   # Spearman's rho\n",
        "  return pearson_r, spearman_rho"
      ],
      "metadata": {
        "id": "eH02L-0X8wqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IiJfzfCN9QWo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}