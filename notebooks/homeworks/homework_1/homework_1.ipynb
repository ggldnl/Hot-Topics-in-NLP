{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget # to download data\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install gensim\n",
    "!pip install nltk\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import wget\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import scipy.stats\n",
    "import tqdm\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import jsonlines\n",
    "\n",
    "import zipfile\n",
    "\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implicit representations\n",
    "\n",
    "1. Create an embedding for the words using the (cleaned) Mosaico dataset by fitting a model.\n",
    "2. Replace WordNet senses for each word and then create embeddings as before.\n",
    "\n",
    "We can use different models to create word/sense embeddings and log which one performs better."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take all the lines in the file\n",
    "lines = []\n",
    "with jsonlines.open('data/sample_annotated_sentences/500000.jsonl') as reader:\n",
    "\n",
    "    limit = 10\n",
    "    for i, line in enumerate(reader):\n",
    "\n",
    "        if i < limit:\n",
    "            print(f'{i:3d} | {line}')\n",
    "\n",
    "        lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_digits(number):\n",
    "    return len(str(abs(number)))\n",
    "\n",
    "def get_token_and_index_strings(text):\n",
    "    line_tokens = text.split()\n",
    "    line_tokens_print = []\n",
    "    line_tokens_ids_print = []\n",
    "    for i, token in enumerate(line_tokens):\n",
    "        if get_num_digits(i) > len(token):\n",
    "            line_tokens_print.append((' ' * (get_num_digits(i) - len(token))) + token)\n",
    "        else:\n",
    "            line_tokens_print.append(token)\n",
    "        line_tokens_ids_print.append((' '*(len(line_tokens[i]) - len(str(i)))) + f'{i}')\n",
    "    return ' '.join(line_tokens_print), ' '.join(line_tokens_ids_print)\n",
    "\n",
    "def pretty_print(line, print_indexes=False):\n",
    "    \n",
    "    # Print the text\n",
    "    text = line['text']\n",
    "    \n",
    "    # Print the indexes\n",
    "    tokens_string, indexes_string = get_token_and_index_strings(text)\n",
    "    print(f'text: {tokens_string}')\n",
    "    if print_indexes:\n",
    "        print(f'      {\"\".join(indexes_string)}')\n",
    "\n",
    "    # Print the annotations\n",
    "    annotations = line['annotations']\n",
    "    if len(annotations) > 0:\n",
    "        print(f'annotations: {annotations[0]}')\n",
    "        for i in range(1, len(annotations)):\n",
    "            print(f\"{' '*(len('annotations: '))}{annotations[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_line_1 = lines[0].copy()\n",
    "sample_line_1 = {\n",
    "    'text': \"In Hinduism , the 60th birthday of a man is called Sashti poorthi .\",\n",
    "    'annotations': [\n",
    "        {'token_span': [5, 6], 'label': 'birthday%1:28:00::'},\n",
    "        {'token_span': [8, 9], 'label': 'man%1:18:00::'}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# sample_line_2 = lines[5].copy()\n",
    "sample_line_2 = {\n",
    "    'text': \"The new world of English words came out in 1658 and a dictionary of 40,000 words had been prepared in 1721 by Nathan Bailey , though none was as comprehensive in breadth or style as Johnson's .\",\n",
    "    'annotations': [\n",
    "        {'token_span': [1, 2], 'label': 'world%1:14:01::'},\n",
    "        {'token_span': [4, 5], 'label': 'word%1:10:00::'},\n",
    "        {'token_span': [11, 12], 'label': 'dictionary%1:10:00::'},\n",
    "        {'token_span': [14, 15], 'label': 'word%1:10:00::'},\n",
    "        {'token_span': [32, 33], 'label': 'style%1:10:00::'}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# sample_line_3 = lines[35].copy()\n",
    "sample_line_3 = {\n",
    "    'text': \"Even without the availability of either co-receptor ( even CCR5 ) , the virus can still invade cells if gp41 were to go through an alteration ( including its cytoplasmic tail ) that resulted in the independence of CD4 without the need of CCR5 and / or CXCR4 as a doorway .\",\n",
    "    'annotations': [\n",
    "        {'token_span': [17, 18], 'label': 'cell%1:03:00::'},\n",
    "        {'token_span': [30, 31], 'label': 'tail%1:05:00::'},\n",
    "        {'token_span': [50, 51], 'label': 'doorway%1:06:00::'}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# sample_line_4 = lines[15].copy()\n",
    "sample_line_4 = {\n",
    "    'text': \"Typically , NATO inert munitions are painted entirely in light blue and / or have the word \\\" INERT \\\" stenciled on them in prominent locations .[ citation needed ] IED ( barrel bomb , nail bomb , pipe bomb , pressure cooker bomb , fertilizer bomb , molotov cocktail )\", \n",
    "    'annotations': [\n",
    "        {\"token_span\": [16, 17], \"label\": \"word%1:10:00::\"}, \n",
    "        {\"token_span\": [35, 36], \"label\": \"nail%1:06:00::\"}, \n",
    "        {\"token_span\": [48, 49], \"label\": \"cocktail%1:13:00::\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# sample_line_5 = lines[51].copy()\n",
    "sample_line_5 = {\n",
    "    'text': \"Later on 9 January , Samuel scored a last - minute winner in the 4\\u20133 win versus Siena by beating goalkeeper Gianluca Curci with a left - footed shot .\", \n",
    "    'annotations': [\n",
    "        {\"token_span\": [10, 11], \"label\": \"minute%1:28:00::\"}, \n",
    "        {\"token_span\": [11, 12], \"label\": \"winner%1:18:00::\"}, \n",
    "        {\"token_span\": [15, 16], \"label\": \"win%1:11:00::\"}, \n",
    "        {\"token_span\": [27, 28], \"label\": \"foot%2:38:00::\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# sample_line_6 = lines[502].copy()\n",
    "sample_line_6 = {\n",
    "    'text': \"In a 1960 piano performance in Cologne , he played Chopin , threw himself on the piano and rushed into the audience , attacking Cage and pianist David Tudor by cutting their clothes with scissors and dumping shampoo on their heads .\", \n",
    "    'annotations': [\n",
    "        {\"token_span\": [24, 25], \"label\": \"cage%1:18:00::\"}, \n",
    "        {\"token_span\": [32, 33], \"label\": \"clothes%1:06:00::\"}, \n",
    "        {\"token_span\": [40, 41], \"label\": \"head%1:08:00::\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "sample_lines = [\n",
    "    # sample_line_1,\n",
    "    sample_line_2,\n",
    "    # sample_line_3,\n",
    "    sample_line_4,\n",
    "    sample_line_5,\n",
    "    sample_line_6\n",
    "]\n",
    "\n",
    "for sample_line in sample_lines:\n",
    "    pretty_print(sample_line, print_indexes=True)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load tje pretrained spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Regex to discard numbers, special characters and punctuation\n",
    "regexp_alphbetic = re.compile('[^a-zA-Z-]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how to spaCy model works \n",
    "\n",
    "from spacy.lang.en.examples import sentences \n",
    "doc = nlp(sentences[0])\n",
    "print(doc.text)\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.dep_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are sentences that are tokenized differently with respect to a simple split. We can't use token_span if we tokenize with the spaCy model. In some cases this is due to hyphens (`co-authored`), in other cases this is due to apostrophes (`Johnson's`) or to something like `.[` that is split into `.` and `[` from the spacy tokenizer and into a single token from a simple string.split(). We need to split the text in the same way both with the spaCy tokenizer and with the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_different_tokenization_indexes(lines, model, print_lines=False):\n",
    "\n",
    "    count = 0\n",
    "    for line in lines:\n",
    "\n",
    "        text = line['text']\n",
    "        annotations = line['annotations']\n",
    "\n",
    "        tokens = text.split()\n",
    "\n",
    "        doc = model(text)\n",
    "        tokens_doc = [token.text for token in doc]\n",
    "\n",
    "        if len(tokens) != len(tokens_doc):\n",
    "            count += 1\n",
    "\n",
    "            if print_lines:\n",
    "                print(text)\n",
    "                print(f'{tokens}')\n",
    "                print(f'{tokens_doc}')\n",
    "                for annotation in annotations:\n",
    "                    print(annotation)\n",
    "                print()\n",
    "\n",
    "    return count\n",
    "\n",
    "test_limit = 1000\n",
    "count = count_different_tokenization_indexes(lines[:test_limit], nlp, print_lines=True)\n",
    "print(f'{count}/{test_limit} sentences are tokenized differently with the standard tokenizer')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyphens (-) are used to join two or more words that act as a single term, to form some compound word. Looping through the tokens produced by the model as it is we will end up with two separate tokens for intra-hyphen words; we can consider both of them, but the position of the senses in the annotations will differ from the original one. Looping through the tokens produced by a split we will end up with a single token for intra-hyphen words; this will ensure that the annotations indexes are respected but the single token won't be correctly lemmatized by the spacy model unless we implement some complex logic to account for hyphens. We could split the text with respect to spaces and dashes but even in that case the index of the annotated words will differ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dashed_text = \"laptop-cover co-authored well-known stay-at-home 40.000 L'hymnaire\"\n",
    "\n",
    "# Define the splitting pattern to match spaces and dashes\n",
    "split_pattern = re.compile(r'[ -]')\n",
    "\n",
    "# Split the string using the pattern\n",
    "result = re.split(split_pattern, dashed_text)\n",
    "\n",
    "# Filter out empty strings resulting from consecutive spaces or dashes\n",
    "result = [s for s in result if s]\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a [custom tokenizer](https://stackoverflow.com/questions/55241927/spacy-intra-word-hyphens-how-to-treat-them-one-word) for the model that treats hyphenated words this as a single token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(nlp):\n",
    "    infix_re = re.compile(r'(\\w+)-(\\w+)')  # Define an infix regex pattern for hyphens between words\n",
    "\n",
    "    return Tokenizer(nlp.vocab, infix_finditer=infix_re.finditer)\n",
    "\n",
    "custom_nlp = spacy.load(\"en_core_web_sm\")\n",
    "custom_nlp.tokenizer = custom_tokenizer(custom_nlp)\n",
    "\n",
    "for line in sample_lines:\n",
    "    \n",
    "    custom_doc = custom_nlp(line['text'])\n",
    "    doc = nlp(line['text'])\n",
    "\n",
    "    pretty_print(line, print_indexes=True)\n",
    "    print(f'Original tokenizer: {[t.lemma_ for t in doc]}')\n",
    "    print(f'Split             : {[t for t in line[\"text\"].split()]}')\n",
    "    print(f'Custom tokenizer  : {[t.lemma_ for t in custom_doc]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_custom_nlp = count_different_tokenization_indexes(lines[:test_limit], custom_nlp, print_lines=True)\n",
    "print(f'{count_custom_nlp}/{test_limit} sentences are tokenized differently')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have another problem: sometimes the index in the annotations is off. We need to account for that as in some cases (e.g. in the second sample line) the word we need to replace will remain along with its explicit senses, that replaced an unrelated token instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    similarity = intersection / union if union != 0 else 0  # Avoid division by zero\n",
    "    return similarity\n",
    "\n",
    "def find_word_in_window(lemmatized_tokens, window_span, sense_key, use_jaccard_similarity=False):\n",
    "    \"\"\"\n",
    "    We have a few tokens and a sense_key, that contains the lemma of a word inside the text.\n",
    "    We want to find the index of that word.\n",
    "    \"\"\"\n",
    "\n",
    "    start_idx = window_span[0]\n",
    "    end_idx = window_span[1]\n",
    "\n",
    "    lemma_from_sense_key = sense_key[:sense_key.index('%')]\n",
    "\n",
    "    target_idx = -1\n",
    "\n",
    "    for index in range(start_idx, end_idx + 1):\n",
    "        lemma = lemmatized_tokens[index]\n",
    "        # print(f'Token [{token}] vs lemma [{lemma}]')\n",
    "\n",
    "        if use_jaccard_similarity:\n",
    "            if jaccard_similarity(set(lemma), set(lemma_from_sense_key)) >= 0.6:\n",
    "                target_idx = index\n",
    "                break\n",
    "        else:\n",
    "            if lemma == lemma_from_sense_key:\n",
    "                target_idx = index\n",
    "                break\n",
    "\n",
    "    if target_idx == -1:\n",
    "        raise ValueError(f'No match in window for token {sense_key} in window {lemmatized_tokens[start_idx:end_idx + 1]}')\n",
    "    return target_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample_line in sample_lines:\n",
    "    pretty_print(sample_line, print_indexes=True)\n",
    "    lemmatized_tokens = [token.lemma_ for token in custom_nlp(sample_line['text'].lower())]\n",
    "    for annotation in sample_line['annotations']:\n",
    "        window_start = annotation['token_span'][0]\n",
    "        window_end = annotation['token_span'][1]\n",
    "        idx = find_word_in_window(lemmatized_tokens, (window_start, window_end), annotation['label'], use_jaccard_similarity=True)\n",
    "        print(f'{annotation[\"label\"]} in range ({window_start} - {window_end}) -> true index = {idx}')\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build the final preprocessing routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_line(line, \n",
    "                    model,\n",
    "                    stopwords=[], \n",
    "                    punctuation=[],\n",
    "                    lemmatize=True,\n",
    "                    filtering_regex=None, \n",
    "                    replace_with_sense_key=True,\n",
    "                    use_jaccard_similarity=False,\n",
    "                    produce_both=False\n",
    "                    ):\n",
    "\n",
    "\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    "\n",
    "    if punctuation is None:\n",
    "        punctuation = []\n",
    "\n",
    "    # Take text and annotations from the line\n",
    "    text = line['text'].lower()\n",
    "    annotations = line['annotations']\n",
    "\n",
    "    # Indexes of the words we may want to replace with a sense key\n",
    "    # {token1_index: token1, ...}\n",
    "    tokens = model(text)\n",
    "    annotations_data = {(annotation['token_span'][0], annotation['token_span'][1]): annotation['label'] for annotation in annotations}\n",
    "    annotation_indexes = {find_word_in_window(\n",
    "        [token.lemma_ for token in tokens], \n",
    "        (k[0], k[1]), \n",
    "        v, \n",
    "        use_jaccard_similarity=use_jaccard_similarity\n",
    "    ): v for k, v in annotations_data.items()}\n",
    "    \n",
    "    # This will be then joined to create a new preprocessed sentence\n",
    "    new_tokens = []\n",
    "    new_semantic_tokens = []\n",
    "    \n",
    "    # For each token, check if it is a stopword, punctuation, if it matches the filter regex \n",
    "    # and if it needs to be retained or replaced with its sense key. Doc now tokenizes as the\n",
    "    # standard string split, so there will be no differences in the token indexes\n",
    "    for i, token in enumerate(tokens):\n",
    "\n",
    "        token_text = token.lemma_ if lemmatize else token.text\n",
    "        \n",
    "        # The token is a word we can replace with the lemma key\n",
    "        if i in annotation_indexes.keys():\n",
    "\n",
    "            # We may want to produce both sentences, one with lemma key and one without\n",
    "            # in order to train a model for word embeddings and another model with\n",
    "            # sense embeddings\n",
    "            if produce_both:\n",
    "                new_tokens.append(token_text)\n",
    "                new_semantic_tokens.append(annotation_indexes[i]) # [i][1])\n",
    "            else:\n",
    "                if replace_with_sense_key:\n",
    "                    # print(f'Token {token_text:20s} is good, we keep it as {annotation_indexes[i][1]}')\n",
    "                    new_tokens.append(annotation_indexes[i]) # [i][1])\n",
    "                else:\n",
    "                    # print(f'Token {token_text:20s} is good, we keep it as {token_text}')\n",
    "                    new_tokens.append(token_text)\n",
    "                \n",
    "        # The token is a stopword\n",
    "        elif token_text in stopwords:\n",
    "            # print(f'Token {token_text:20s} is a stopword')\n",
    "            continue\n",
    "\n",
    "        # The token is punctuation\n",
    "        elif token_text in punctuation:\n",
    "            # print(f'Token {token_text:20s} is punctuation')\n",
    "            continue\n",
    "\n",
    "        # The token matches the regex, it is not valid\n",
    "        elif filtering_regex is not None and filtering_regex.search(token_text):\n",
    "            # print(f'Token {token_text:20s} is filtered out from regex')\n",
    "            continue\n",
    "\n",
    "        # The token is valid, we can retain it\n",
    "        else:\n",
    "            # print(f'Token {token_text:20s} is good, we keep it')\n",
    "            new_tokens.append(token_text)\n",
    "            if produce_both:\n",
    "                new_semantic_tokens.append(token_text)\n",
    "\n",
    "    return new_tokens, new_semantic_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample_line in sample_lines:\n",
    "    \n",
    "    print([token for token in custom_nlp(sample_line['text'])])\n",
    "\n",
    "    print('Data before: ')\n",
    "    print(sample_line['text'])\n",
    "\n",
    "    try:\n",
    "        print('Preprocessed data:')\n",
    "        print(\" \".join(preprocess_line(sample_line, custom_nlp, stopwords=stop_words, filtering_regex=regexp_alphbetic)[0]))\n",
    "    except:\n",
    "        print('Exception')\n",
    "\n",
    "    print('Preprocessed data with jaccard similarity:')\n",
    "    print(\" \".join(preprocess_line(sample_line, custom_nlp, stopwords=stop_words, filtering_regex=regexp_alphbetic, use_jaccard_similarity=True)[0]))\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing with STANZA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the actual datasets \n",
    "original_sentences = []\n",
    "non_semantic_tokens = []\n",
    "semantic_tokens = []\n",
    "discarded_count = 0\n",
    "for line in tqdm(lines):\n",
    "\n",
    "    # Without jaccard similarity, ~5% of the sentences get discarded\n",
    "    # With jaccard similarity, ~2.4% of the sentences get discarded\n",
    "    try:\n",
    "        \n",
    "        non_semantic, semantic = preprocess_line(\n",
    "            line, \n",
    "            custom_nlp, \n",
    "            stopwords=stop_words, \n",
    "            filtering_regex=regexp_alphbetic, \n",
    "            produce_both=True, \n",
    "            use_jaccard_similarity=True\n",
    "        )\n",
    "\n",
    "        original_sentences.append(line['text'])\n",
    "        non_semantic_tokens.append(non_semantic)\n",
    "        semantic_tokens.append(semantic)\n",
    "    except:\n",
    "        print('Discarded: ')\n",
    "        pretty_print(line, print_indexes=True)\n",
    "        print()\n",
    "        discarded_count += 1\n",
    "\n",
    "print(f'Original sentence                : {original_sentences[0]}')\n",
    "print(f'Preprocessed sentence            : {\" \".join(non_semantic_tokens[0])}')\n",
    "print(f'Preprocessed sentence with senses: {\" \".join(semantic_tokens[0])}')\n",
    "print(f'Discarded {discarded_count} out of 500000')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now despite discarding some sentences at least we can rest assured that only senses for the specified tokens appear in the dataset (before we could've had \"word\", \"word%1:10:00\" and so on and this would've affected the similarity score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could filter out uncommon words that have a low frequency but we can do the same specifying the `min_count` parameter of the Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_frequency(sentences, frequency):\n",
    "\n",
    "    ## remove words that appear only once\n",
    "    frequency_dict = defaultdict(int)\n",
    "    for sentence in tqdm(sentences):\n",
    "        # tokens = sentence.split()\n",
    "        # for token in tokens:\n",
    "        for token in sentence:\n",
    "            frequency_dict[token] += 1\n",
    "\n",
    "    texts = [[token for token in sentence.split() if frequency_dict[token] > frequency]\n",
    "            for sentence in sentences]\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ff_word_sentences = filter_frequency(word_sentences, 1)\n",
    "# ff_sense_sentences = filter_frequency(sense_sentences, 1)\n",
    "\n",
    "# for i in range(10):\n",
    "#   print(ff_word_sentences[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the preprocessed dictionary to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the output JSONLines file. Set it to None to skip saving the dataset\n",
    "# output_file = 'preprocessed_dataset_new.jsonl'\n",
    "output_file = None\n",
    "\n",
    "if output_file is not None:\n",
    "\n",
    "    # Open the JSONLines file for writing\n",
    "    with jsonlines.open(output_file, mode='w') as writer:\n",
    "\n",
    "        for original_sentence, non_semantic_tokens_sentence, semantic_tokens_sentence in tqdm(zip(original_sentences, non_semantic_tokens, semantic_tokens)):\n",
    "\n",
    "            # Create a dictionary for each set of strings\n",
    "            data = {\n",
    "                'original': original_sentence,\n",
    "                'non_semantic': non_semantic_tokens_sentence,\n",
    "                'semantic': semantic_tokens_sentence\n",
    "            }\n",
    "            \n",
    "            # Write the dictionary to the JSONLines file\n",
    "            writer.write(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the preprocessed dictionary from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the preprocessed dictionary from file\n",
    "input_file = 'preprocessed_dataset_new.jsonl'\n",
    "# input_file = None\n",
    "\n",
    "read_non_semantic_tokens = []\n",
    "read_semantic_tokens = []\n",
    "\n",
    "if input_file is not None:\n",
    "\n",
    "    \"\"\"\n",
    "    with jsonlines.open(input_file) as reader:\n",
    "        for line in tqdm(reader.iter()):\n",
    "            read_non_semantic_sentences.append(line['non_semantic'])\n",
    "            read_semantic_sentences.append(line['semantic'])\n",
    "\n",
    "    non_semantic_sentences = [[token for token in sentence.split()] for sentence in read_non_semantic_sentences]\n",
    "    semantic_sentences = [[token for token in sentence.split()] for sentence in read_semantic_sentences]\n",
    "    \"\"\"\n",
    "\n",
    "    with jsonlines.open(input_file, 'r') as reader:\n",
    "        for line in tqdm(reader):\n",
    "            non_semantic_tokens = line.get(\"non_semantic\", [])\n",
    "            semantic_tokens = line.get(\"semantic\", [])\n",
    "            read_non_semantic_tokens.append(non_semantic_tokens)\n",
    "            read_semantic_tokens.append(semantic_tokens)\n",
    "        \n",
    "        non_semantic_tokens = read_non_semantic_tokens\n",
    "        semantic_tokens = read_semantic_tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simlex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_simlex(simlex_zip_path, simlex_dataset_path):\n",
    "  simlex_pairs = dict()\n",
    "  with zipfile.ZipFile(simlex_zip_path, 'r') as zipf, open(simlex_dataset_path, \"wb\") as fw:\n",
    "    with zipf.open('SimLex-999/SimLex-999.txt') as myfile:\n",
    "      next(myfile)\n",
    "      for line in myfile:\n",
    "        w1, w2, pos, score, *_ = line.strip().split()\n",
    "        w1 = w1.decode('utf-8')\n",
    "        w2 = w2.decode('utf-8')\n",
    "        score = float(score)\n",
    "        simlex_pairs[(w1, w2)] = score\n",
    "        fw.write(f'{w1}\\t{w2}\\t{score}\\n'.encode('utf-8'))\n",
    "        # print(f'{w1}\\t{w2}\\t{score}')\n",
    "  return simlex_pairs\n",
    "\n",
    "\n",
    "simlex_data = 'SimLex-999.zip'\n",
    "simple_simlex_path = \"data/simlex999/simlex999.tsv\"\n",
    "\n",
    "if not os.path.exists(simlex_data):\n",
    "    simlex_data = wget.download(\"https://fh295.github.io/SimLex-999.zip\")\n",
    "\n",
    "simlex_pairs = load_simlex(simlex_data, simple_simlex_path)\n",
    "simlex_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The returned dictionary should be similar to previous word_pair2score \n",
    "# but instead of words we consider the senses from the dataset \n",
    "# associated with this words\n",
    "def load_semantic_simplex(path):\n",
    "  senses2score = dict()\n",
    "  with open(path) as fr:\n",
    "    next(fr)\n",
    "    for line in fr:\n",
    "      chunks = line.strip().split()\n",
    "      w1 = chunks[0]\n",
    "      w2 = chunks[1]\n",
    "      sim_lex_score = float(chunks[3])\n",
    "      senses_w1 = chunks[10].split(\",\")\n",
    "      senses_w2 = chunks[11].split(\",\")\n",
    "      senses2score[(tuple(senses_w1), tuple(senses_w2))] = sim_lex_score\n",
    "\n",
    "  return senses2score\n",
    "\n",
    "senses2score = load_semantic_simplex(r'data/simlex999/semantic_simlex_v0.1.tsv')\n",
    "senses2score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the two are equals for the non semantic case\n",
    "def check_equals_non_semantic(semantic_pairs, non_semantic_path):\n",
    "    with open(non_semantic_path) as fr:\n",
    "        next(fr)\n",
    "        for line in fr:\n",
    "            chunks = line.strip().split()\n",
    "            w1 = chunks[0]\n",
    "            w2 = chunks[1]\n",
    "            sim_lex_score = float(chunks[3])\n",
    "\n",
    "            # print(f'Evaluating ({w1}, {w2})')\n",
    "\n",
    "            if (w1, w2) not in semantic_pairs:\n",
    "                print(f'({w1}, {w2}) not in simlex999')\n",
    "            else:\n",
    "                semantic_score = semantic_pairs[(w1, w2)]\n",
    "                if sim_lex_score != semantic_score:\n",
    "                    print(f'({w1}, {w2}) have different scores ({sim_lex_score} vs {semantic_score})')\n",
    "\n",
    "# check_equals_non_semantic(simlex_pairs, r'data/simlex999/semantic_simlex_v0.1.tsv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correlation_score(model, word_pair2score, print_warning=True):\n",
    "    human_scores = []\n",
    "    system_scores = []\n",
    "    count_print_warnings = 0\n",
    "    for (w1, w2), score in word_pair2score.items():\n",
    "        if (w1 not in model) or (w2 not in model):\n",
    "            system_scores.append(-1)\n",
    "            human_scores.append(score)\n",
    "            if print_warning:\n",
    "                print(f\"({count_print_warnings:6d}) | WARNING ({w1} and {w2}) are not present in the embedding model!!\" )\n",
    "                count_print_warnings += 1\n",
    "            continue\n",
    "        system_similarity = model.similarity(w1, w2)\n",
    "        human_scores.append(score)\n",
    "        system_scores.append(system_similarity)\n",
    "\n",
    "    human_scores = np.array(human_scores)\n",
    "    system_scores = np.array(system_scores)\n",
    "    pearson_r, _ = scipy.stats.pearsonr(human_scores, system_scores)    # Pearson's r\n",
    "    spearman_rho = scipy.stats.spearmanr(human_scores, system_scores).statistic   # Spearman's rho\n",
    "    \n",
    "    return pearson_r, spearman_rho\n",
    "\n",
    "\n",
    "\n",
    "def compute_semantic_correlation_score(model, senses2score,  print_warning=True):\n",
    "    human_scores = []\n",
    "    system_scores = []\n",
    "    for (senses_1, senses_2), score in senses2score.items():\n",
    "        senses_1_in_model = [s for s in senses_1 if s in model]\n",
    "        senses_2_in_model = [s for s in senses_2 if s in model]\n",
    "\n",
    "        if len(senses_1_in_model) == 0 or len(senses_2_in_model) == 0:\n",
    "            # sense is not present in the model\n",
    "            s1_str = \" \".join(senses_1)\n",
    "            s2_str = \" \".join(senses_2)\n",
    "            if print_warning:\n",
    "                print(f\"WARNING ({s1_str} and {s2_str}) are not present in the embedding model!!\" )\n",
    "            system_scores.append(-1)\n",
    "            continue\n",
    "        # Calculate semantic similarities between all pairs of senses\n",
    "        all_similarities = []\n",
    "        for s1 in senses_1_in_model:\n",
    "            for s2 in senses_2_in_model:\n",
    "                all_similarities.append(model.similarity(s1, s2))\n",
    "\n",
    "        system_similarity = sum(all_similarities) / len(all_similarities)\n",
    "        human_scores.append(score)\n",
    "        system_scores.append(system_similarity)\n",
    "    human_scores = np.array(human_scores)\n",
    "    system_scores = np.array(system_scores)\n",
    "    # Calculate Pearson's r (Pearson correlation coefficient) and Spearman's rho (Spearman rank correlation coefficient)\n",
    "    pearson_r, _ = scipy.stats.pearsonr(human_scores, system_scores)    # Pearson's r\n",
    "    spearman_rho = scipy.stats.spearmanr(human_scores, system_scores).statistic   # Spearman's rho\n",
    "    return pearson_r, spearman_rho"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings\n",
    "\n",
    "1. `Word2Vec` is a word embedding technique that learns vector representations of words, capturing semantic relationships through context.\n",
    "2. `GloVe` combines global and local word context through matrix factorization to create word embeddings.\n",
    "3. `FastText` represents words as character n-grams, handling morphological details and out-of-vocabulary words.\n",
    "4. `Doc2Vec` extends Word2Vec to learn document-level embeddings by treating each document as a unique word.\n",
    "5. `BERT`, a transformer-based model, learns contextual word embeddings by considering both left and right sentence context.\n",
    "6. `ELMo` uses bi-directional LSTM to create word embeddings based on entire sentence context, enhancing syntactic and semantic understanding.\n",
    "7. `USE` is a universal sentence encoder for generating fixed-length embeddings, applicable to various NLP tasks.\n",
    "\n",
    "More [here](https://medium.com/@vaibhav1403/embedding-techniques-in-natural-language-processing-nlp-29e424ab0cd9)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_semantic_tokens[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "vector_size = 10\n",
    "window = 2\n",
    "min_count = 1 \n",
    "\n",
    "non_semantic_embeddings_model = gensim.models.Word2Vec(non_semantic_tokens, vector_size=vector_size, window=window, min_count=min_count)\n",
    "non_semantic_embeddings_model_score = compute_correlation_score(non_semantic_embeddings_model.wv, simlex_pairs, print_warning=True)\n",
    "print(f'Pearson and Spearman scores: {non_semantic_embeddings_model_score}')\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can search for the best hyperparameters.\n",
    "- The `vector_size` determines the dimensionality of the word vectors or embeddings.\n",
    "- The `window_size` defines the context window for the Word2Vec model.\n",
    "- The `min_count` parameter is used to control the minimum count of a word in the corpus for it to be considered during the training of the Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_params(model, word_vector_retrieval_function, correlation_score_function, train_dataset, evaluation_dataset, param_grid):\n",
    "\n",
    "    best_score = 0\n",
    "    best_params = None\n",
    "\n",
    "    # Perform grid search\n",
    "    score_dict = dict()\n",
    "    pbar = tqdm(ParameterGrid(param_grid))\n",
    "    for params in pbar:\n",
    "\n",
    "        # Train Word2Vec model\n",
    "        nlp = model(train_dataset, **params)\n",
    "        \n",
    "        pearson_score, spearman_score = correlation_score_function(word_vector_retrieval_function(nlp), evaluation_dataset, print_warning=False)\n",
    "\n",
    "        # Normalize the scores [-1, 1] -> [0, 1]\n",
    "        pearson_score_norm = (pearson_score + 1) / 2\n",
    "        spearman_score_norm = (spearman_score + 1) / 2\n",
    "\n",
    "        # Compute the combined score using a weighted average\n",
    "        alpha = 0.5\n",
    "        combined_score = alpha * pearson_score_norm + (1 - alpha) * spearman_score_norm\n",
    "\n",
    "        score_dict[(params['vector_size'], params['window'], params['min_count'])] = combined_score\n",
    "        # pbar.set_description(f'Params {params} gives pearson {pearson_score} ({pearson_score_norm}) and spearman {spearman_score} ({spearman_score_norm}) scores ({combined_score})')\n",
    "        pbar.set_postfix(pearson=pearson_score_norm, spearman=spearman_score_norm, refresh=False)\n",
    "\n",
    "        # Update best parameters if current score is better\n",
    "        if combined_score > best_score:\n",
    "            best_score = combined_score\n",
    "            best_params = params\n",
    "\n",
    "    return best_params, best_score, score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Set up your grid of hyperparameters\n",
    "param_grid = {\n",
    "    'vector_size': [16, 64, 128, 256, 512],\n",
    "    'window': [2, 4, 6],\n",
    "    'min_count': [1, 3, 6]\n",
    "}\n",
    "# This gives the best params of window_size = 2, min_count = 6, vector_size = 512 \n",
    "\"\"\"\n",
    "\n",
    "param_grid = {\n",
    "    'vector_size': [128, 256, 512, 1024],\n",
    "    'window': [2],\n",
    "    'min_count': [6]\n",
    "}\n",
    "\n",
    "# Print and use the best parameters\n",
    "best_params, best_score, score_dict = find_best_params(Word2Vec, lambda model: model.wv, compute_correlation_score, non_semantic_tokens, simlex_pairs, param_grid)\n",
    "print(f'Best parameters: {best_params}')\n",
    "print(f'Best score: {best_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_semantic_embeddings_model = gensim.models.Word2Vec(non_semantic_tokens, **best_params)\n",
    "non_semantic_embeddings_model_score = compute_correlation_score(non_semantic_embeddings_model.wv, simlex_pairs, print_warning=True)\n",
    "print(f'Pearson and Spearman scores: {non_semantic_embeddings_model_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Extract hyperparameter values and loss\n",
    "min_count_values = [key[0] for key in score_dict.keys()]\n",
    "vector_size_values = [key[1] for key in score_dict.keys()]\n",
    "window_size_values = [key[2] for key in score_dict.keys()]\n",
    "score_values = list(score_dict.values())\n",
    "\n",
    "# Create a 3D scatter plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "scatter = ax.scatter(window_size_values, min_count_values, vector_size_values, c=score_values, cmap='viridis', marker='o')\n",
    "\n",
    "# Add a color bar\n",
    "cbar = fig.colorbar(scatter, ax=ax, pad=0.1, fraction=0.05)\n",
    "\n",
    "# Label axes\n",
    "ax.set_xlabel('Min Count')\n",
    "ax.set_ylabel('Vector Size')\n",
    "ax.set_zlabel('Window Size')\n",
    "ax.set_title('Score')\n",
    "\n",
    "plt.show()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv_file_path = 'data/student_predictions/non_semantic_word2vec.tsv'\n",
    "\n",
    "with open(tsv_file_path, 'w') as tsv_file:\n",
    "\n",
    "    # Write the header\n",
    "    # tsv_file.write('w1\\tw2\\tscore\\n')\n",
    "\n",
    "    for (w1, w2), score in simlex_pairs.items():\n",
    "        if (w1 not in non_semantic_embeddings_model.wv) or (w2 not in non_semantic_embeddings_model.wv):\n",
    "            system_similarity = np.nan\n",
    "        else:\n",
    "            # The cosine similarity ranges from -1 (completely dissimilar) to 1 (completely similar). \n",
    "            # A value of 0 means that the vectors are orthogonal or uncorrelated.\n",
    "            system_similarity = non_semantic_embeddings_model.wv.similarity(w1, w2)\n",
    "\n",
    "        tsv_file.write(f'{w1}\\t{w2}\\t{system_similarity}\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_comparison = False\n",
    "if glove_comparison:\n",
    "    glove_pretrained = api.load(\"glove-wiki-gigaword-50\")\n",
    "    glove_word_vectors = {word: glove_pretrained[word] for word in glove_pretrained.index_to_key}    \n",
    "    non_semantic_embeddings_glove_score = compute_correlation_score(glove_pretrained, simlex_pairs, print_warning=True)\n",
    "    print(f'Pearson and Spearman scores for GloVe: {non_semantic_embeddings_glove_score}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sense embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_tokens[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "vector_size = 10\n",
    "window = 2\n",
    "min_count = 1 \n",
    "\n",
    "semantic_embeddings_model = gensim.models.Word2Vec(semantic_tokens, vector_size=vector_size, window=window, min_count=min_count)\n",
    "semantic_embeddings_model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Set up your grid of hyperparameters\n",
    "param_grid = {\n",
    "    'vector_size': [16, 64, 128, 256, 512],\n",
    "    'window': [2, 4, 6],\n",
    "    'min_count': [1, 3, 6]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "param_grid = {\n",
    "    'vector_size': [128, 256, 512, 1024],\n",
    "    'window': [2],\n",
    "    'min_count': [6]\n",
    "}\n",
    "\n",
    "# Print and use the best parameters\n",
    "best_semantic_params, best_semantic_score, _ = find_best_params(Word2Vec, lambda model: model.wv, compute_semantic_correlation_score, semantic_tokens, senses2score, param_grid)\n",
    "print(f'Best parameters: {best_semantic_params}')\n",
    "print(f'Best score: {best_semantic_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_embeddings_model = Word2Vec(semantic_tokens, **best_semantic_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv_file_path = 'data/student_predictions/semantic_word2vec.tsv'\n",
    "\n",
    "with open(tsv_file_path, 'w') as tsv_file:\n",
    "\n",
    "    for (senses_1, senses_2), score in senses2score.items():\n",
    "        senses_1_in_model = [s for s in senses_1 if s in semantic_embeddings_model.wv]\n",
    "        senses_2_in_model = [s for s in senses_2 if s in semantic_embeddings_model.wv]\n",
    "        \n",
    "        all_similarities = []\n",
    "        if len(senses_1_in_model) == 0 or len(senses_2_in_model) == 0:\n",
    "            # Sense is not present in the model\n",
    "            s1_str = \" \".join(senses_1)\n",
    "            s2_str = \" \".join(senses_2)\n",
    "            print(f\"WARNING ({s1_str} and {s2_str}) are not present in the embedding model!!\" )\n",
    "            all_similarities.append(-1)\n",
    "        # Calculate semantic similarities between all pairs of senses\n",
    "        else:\n",
    "            for s1 in senses_1_in_model:\n",
    "                for s2 in senses_2_in_model:\n",
    "                    all_similarities.append(semantic_embeddings_model.wv.similarity(s1, s2))\n",
    "        w1 = senses_1[0].split('%')[0]\n",
    "        w2 = senses_2[0].split('%')[0]\n",
    "        tsv_file.write(f'{w1}\\t{w2}\\t{max(all_similarities)}\\n')  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
