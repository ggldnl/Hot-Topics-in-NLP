{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wget in /home/daniel/.local/lib/python3.10/site-packages (3.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy in /home/daniel/.local/lib/python3.10/site-packages (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (0.3.3)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/lib/python3/dist-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (2.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (1.24.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/daniel/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /home/daniel/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/daniel/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.8.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/daniel/.local/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/daniel/.local/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/daniel/.local/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/daniel/.local/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/daniel/.local/lib/python3.10/site-packages (from jinja2->spacy) (2.1.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "/bin/bash: line 1: python: command not found\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gensim in /home/daniel/.local/lib/python3.10/site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/daniel/.local/lib/python3.10/site-packages (from gensim) (1.24.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /home/daniel/.local/lib/python3.10/site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/daniel/.local/lib/python3.10/site-packages (from gensim) (6.4.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/daniel/.local/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /home/daniel/.local/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/daniel/.local/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/daniel/.local/lib/python3.10/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /home/daniel/.local/lib/python3.10/site-packages (from nltk) (4.65.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /home/daniel/.local/lib/python3.10/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /home/daniel/.local/lib/python3.10/site-packages (from scikit-learn) (1.24.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/daniel/.local/lib/python3.10/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/daniel/.local/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/daniel/.local/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install wget # to download data\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install gensim\n",
    "!pip install nltk\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import wget\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import scipy.stats\n",
    "import tqdm\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import jsonlines\n",
    "\n",
    "import zipfile"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implicit representations\n",
    "\n",
    "1. Create an embedding for the words using the (cleaned) Mosaico dataset by fitting a model.\n",
    "2. Replace WordNet senses for each word and then create embeddings as before.\n",
    "\n",
    "We can use different models to create word/sense embeddings and log which one performs better."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0 | {'text': 'In Hinduism , the 60th birthday of a man is called Sashti poorthi .', 'annotations': [{'token_span': [5, 6], 'label': 'birthday%1:28:00::'}, {'token_span': [8, 9], 'label': 'man%1:18:00::'}]}\n",
      "  1 | {'text': 'In New Zealand , both Metal Box and Second Edition briefly entered the Top 50 Albums Chart .', 'annotations': [{'token_span': [6, 7], 'label': 'box%1:06:00::'}, {'token_span': [8, 9], 'label': 'second%5:00:00:ordinal:00'}, {'token_span': [11, 12], 'label': 'enter%2:33:00::'}, {'token_span': [13, 14], 'label': 'top%3:00:00::'}]}\n",
      "  2 | {'text': 'Article with extensive bibliography Article about the book \" L\\'hymnaire d\\'Adonis \" of Jacques d\\'Adelswärd - Fersen ( in Spanish ) Pictures of Villa Lysis today [ 1 ] [ 2 ]', 'annotations': [{'token_span': [0, 1], 'label': 'article%1:10:00::'}, {'token_span': [4, 5], 'label': 'article%1:10:00::'}, {'token_span': [7, 8], 'label': 'book%1:10:00::'}]}\n",
      "  3 | {'text': 'The root cluster attached to the basal plate of the bulb is the only part not typically considered palatable in any form .', 'annotations': [{'token_span': [1, 2], 'label': 'root%1:20:00::'}, {'token_span': [3, 4], 'label': 'attach%2:35:02::'}, {'token_span': [7, 8], 'label': 'plate%1:06:02::'}, {'token_span': [10, 11], 'label': 'bulb%1:20:00::'}]}\n",
      "  4 | {'text': \"The new world of English words came out in 1658 and a dictionary of 40,000 words had been prepared in 1721 by Nathan Bailey , though none was as comprehensive in breadth or style as Johnson's .\", 'annotations': [{'token_span': [1, 2], 'label': 'world%1:14:01::'}, {'token_span': [4, 5], 'label': 'word%1:10:00::'}, {'token_span': [11, 12], 'label': 'dictionary%1:10:00::'}, {'token_span': [14, 15], 'label': 'word%1:10:00::'}, {'token_span': [32, 33], 'label': 'style%1:10:00::'}]}\n",
      "  5 | {'text': 'Garlic softens and can be extracted from the cloves by squeezing the ( root ) end of the bulb , or individually by squeezing one end of the clove .', 'annotations': [{'token_span': [13, 14], 'label': 'root%1:20:00::'}, {'token_span': [18, 19], 'label': 'bulb%1:20:00::'}]}\n",
      "  6 | {'text': 'Under normal circumstances , this deflagration occurs too slowly to produce a significant pressure wave ; low explosives , therefore , must generally be used in large quantities or confined in a container with a high burst pressure to be useful as a bomb .', 'annotations': [{'token_span': [1, 2], 'label': 'normal%3:00:01::'}, {'token_span': [2, 3], 'label': 'circumstance%1:26:01::'}, {'token_span': [6, 7], 'label': 'occur%2:30:00::'}, {'token_span': [26, 27], 'label': 'large%3:00:00::'}, {'token_span': [32, 33], 'label': 'container%1:06:00::'}]}\n",
      "  7 | {'text': 'JOBD is a version of OBD - II for vehicles sold in Japan .', 'annotations': [{'token_span': [9, 10], 'label': 'vehicle%1:06:00::'}, {'token_span': [10, 11], 'label': 'sell%2:40:00::'}]}\n",
      "  8 | {'text': 'The indigenous peoples made an alcoholic beverage from fruits of the palm Attalea maripa found at the lower elevations .', 'annotations': [{'token_span': [3, 4], 'label': 'make%2:36:11::'}, {'token_span': [11, 12], 'label': 'palm%1:20:00::'}, {'token_span': [14, 15], 'label': 'find%2:40:02::'}]}\n",
      "  9 | {'text': 'It is a sanctuary as well as a tourist attraction , because it offers different climate , terrain , flora and fauna environments , ranging from beaches to snowy mountain peaks .', 'annotations': [{'token_span': [14, 15], 'label': 'different%3:00:00::'}, {'token_span': [26, 27], 'label': 'beach%1:17:00::'}]}\n"
     ]
    }
   ],
   "source": [
    "# Take all the lines in the file\n",
    "lines = []\n",
    "with jsonlines.open('data/sample_annotated_sentences/500000.jsonl') as reader:\n",
    "\n",
    "    limit = 10\n",
    "    for i, line in enumerate(reader):\n",
    "\n",
    "        if i < limit:\n",
    "            print(f'{i:3d} | {line}')\n",
    "\n",
    "        lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_digits(number):\n",
    "    return len(str(abs(number)))\n",
    "\n",
    "def get_token_and_index_strings(text):\n",
    "    line_tokens = text.split()\n",
    "    line_tokens_print = []\n",
    "    line_tokens_ids_print = []\n",
    "    for i, token in enumerate(line_tokens):\n",
    "        if get_num_digits(i) > len(token):\n",
    "            line_tokens_print.append((' ' * (get_num_digits(i) - len(token))) + token)\n",
    "        else:\n",
    "            line_tokens_print.append(token)\n",
    "        line_tokens_ids_print.append((' '*(len(line_tokens[i]) - len(str(i)))) + f'{i}')\n",
    "    return ' '.join(line_tokens_print), ' '.join(line_tokens_ids_print)\n",
    "\n",
    "def pretty_print(line, print_indexes=False):\n",
    "    \n",
    "    # Print the text\n",
    "    text = line['text']\n",
    "    \n",
    "    # Print the indexes\n",
    "    tokens_string, indexes_string = get_token_and_index_strings(text)\n",
    "    print(f'text: {tokens_string}')\n",
    "    if print_indexes:\n",
    "        print(f'      {\"\".join(indexes_string)}')\n",
    "\n",
    "    # Print the annotations\n",
    "    annotations = line['annotations']\n",
    "    if len(annotations) > 0:\n",
    "        print(f'annotations: {annotations[0]}')\n",
    "        for i in range(1, len(annotations)):\n",
    "            print(f\"{' '*(len('annotations: '))}{annotations[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: In Hinduism , the 60th birthday of a man is called Sashti poorthi  .\n",
      "       0        1 2   3    4        5  6 7   8  9     10     11      12 13\n",
      "annotations: {'token_span': [5, 6], 'label': 'birthday%1:28:00::'}\n",
      "             {'token_span': [8, 9], 'label': 'man%1:18:00::'}\n",
      "\n",
      "text: The new world of English words came out in 1658 and  a dictionary of 40,000 words had been prepared in 1721 by Nathan Bailey  , though none was as comprehensive in breadth or style as Johnson's  .\n",
      "        0   1     2  3       4     5    6   7  8    9  10 11         12 13     14    15  16   17       18 19   20 21     22     23 24     25   26  27 28            29 30      31 32    33 34        35 36\n",
      "annotations: {'token_span': [1, 2], 'label': 'world%1:14:01::'}\n",
      "             {'token_span': [4, 5], 'label': 'word%1:10:00::'}\n",
      "             {'token_span': [11, 12], 'label': 'dictionary%1:10:00::'}\n",
      "             {'token_span': [14, 15], 'label': 'word%1:10:00::'}\n",
      "             {'token_span': [32, 33], 'label': 'style%1:10:00::'}\n",
      "\n",
      "text: Even without the availability of either co-receptor ( even CCR5  )  , the virus can still invade cells if gp41 were to go through an alteration  ( including its cytoplasmic tail  ) that resulted in the independence of CD4 without the need of CCR5 and  / or CXCR4 as  a doorway  .\n",
      "         0       1   2            3  4      5           6 7    8    9 10 11  12    13  14    15     16    17 18   19   20 21 22      23 24         25 26        27  28          29   30 31   32       33 34  35           36 37  38      39  40   41 42   43  44 45 46    47 48 49      50 51\n",
      "annotations: {'token_span': [17, 18], 'label': 'cell%1:03:00::'}\n",
      "             {'token_span': [30, 31], 'label': 'tail%1:05:00::'}\n",
      "             {'token_span': [50, 51], 'label': 'doorway%1:06:00::'}\n",
      "\n",
      "text: Typically , NATO inert munitions are painted entirely in light blue and  / or have the word  \" INERT  \" stenciled on them in prominent locations .[ citation needed  ] IED  ( barrel bomb  , nail bomb  , pipe bomb  , pressure cooker bomb  , fertilizer bomb  , molotov cocktail  )\n",
      "              0 1    2     3         4   5       6        7  8     9   10  11 12 13   14  15   16 17    18 19        20 21   22 23        24        25 26       27     28 29  30 31     32   33 34   35   36 37   38   39 40       41     42   43 44         45   46 47      48       49 50\n",
      "annotations: {'token_span': [16, 17], 'label': 'word%1:10:00::'}\n",
      "             {'token_span': [35, 36], 'label': 'nail%1:06:00::'}\n",
      "             {'token_span': [48, 49], 'label': 'cocktail%1:13:00::'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sample_line_1 = lines[0].copy()\n",
    "sample_line_1 = {\n",
    "    'text': \"In Hinduism , the 60th birthday of a man is called Sashti poorthi .\",\n",
    "    'annotations': [\n",
    "        {'token_span': [5, 6], 'label': 'birthday%1:28:00::'},\n",
    "        {'token_span': [8, 9], 'label': 'man%1:18:00::'}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# sample_line_2 = lines[5].copy()\n",
    "sample_line_2 = {\n",
    "    'text': \"The new world of English words came out in 1658 and a dictionary of 40,000 words had been prepared in 1721 by Nathan Bailey , though none was as comprehensive in breadth or style as Johnson's .\",\n",
    "    'annotations': [\n",
    "        {'token_span': [1, 2], 'label': 'world%1:14:01::'},\n",
    "        {'token_span': [4, 5], 'label': 'word%1:10:00::'},\n",
    "        {'token_span': [11, 12], 'label': 'dictionary%1:10:00::'},\n",
    "        {'token_span': [14, 15], 'label': 'word%1:10:00::'},\n",
    "        {'token_span': [32, 33], 'label': 'style%1:10:00::'}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# sample_line_3 = lines[35].copy()\n",
    "sample_line_3 = {\n",
    "    'text': \"Even without the availability of either co-receptor ( even CCR5 ) , the virus can still invade cells if gp41 were to go through an alteration ( including its cytoplasmic tail ) that resulted in the independence of CD4 without the need of CCR5 and / or CXCR4 as a doorway .\",\n",
    "    'annotations': [\n",
    "        {'token_span': [17, 18], 'label': 'cell%1:03:00::'},\n",
    "        {'token_span': [30, 31], 'label': 'tail%1:05:00::'},\n",
    "        {'token_span': [50, 51], 'label': 'doorway%1:06:00::'}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# sample_line_4 = lines[15].copy()\n",
    "sample_line_4 = {\n",
    "    'text': \"Typically , NATO inert munitions are painted entirely in light blue and / or have the word \\\" INERT \\\" stenciled on them in prominent locations .[ citation needed ] IED ( barrel bomb , nail bomb , pipe bomb , pressure cooker bomb , fertilizer bomb , molotov cocktail )\", \n",
    "    'annotations': [\n",
    "        {\"token_span\": [16, 17], \"label\": \"word%1:10:00::\"}, \n",
    "        {\"token_span\": [35, 36], \"label\": \"nail%1:06:00::\"}, \n",
    "        {\"token_span\": [48, 49], \"label\": \"cocktail%1:13:00::\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "sample_lines = [\n",
    "    sample_line_1,\n",
    "    sample_line_2,\n",
    "    sample_line_3,\n",
    "    sample_line_4\n",
    "]\n",
    "\n",
    "for sample_line in sample_lines:\n",
    "    pretty_print(sample_line, print_indexes=True)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno -2] Name or\n",
      "[nltk_data]     service not known>\n"
     ]
    }
   ],
   "source": [
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load tje pretrained spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Regex to discard numbers, special characters and punctuation\n",
    "regexp_alphbetic = re.compile('[^a-zA-Z-]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple is looking at buying U.K. startup for $1 billion\n",
      "Apple Apple PROPN nsubj\n",
      "is be AUX aux\n",
      "looking look VERB ROOT\n",
      "at at ADP prep\n",
      "buying buy VERB pcomp\n",
      "U.K. U.K. PROPN dobj\n",
      "startup startup NOUN dep\n",
      "for for ADP prep\n",
      "$ $ SYM quantmod\n",
      "1 1 NUM compound\n",
      "billion billion NUM pobj\n"
     ]
    }
   ],
   "source": [
    "# Check how to spaCy model works \n",
    "\n",
    "from spacy.lang.en.examples import sentences \n",
    "doc = nlp(sentences[0])\n",
    "print(doc.text)\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.dep_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are sentences that are tokenized differently with respect to a simple split. We can't use token_span if we tokenize with the spaCy model. In some cases this is due to hyphens (`co-authored`), in other cases this is due to apostrophes (`Johnson's`) or to something like `.[` that is split into `.` and `[` from the spacy tokenizer and into a single token from a simple string.split(). We need to split the text in the same way both with the spaCy tokenizer and with the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new world of English words came out in 1658 and a dictionary of 40,000 words had been prepared in 1721 by Nathan Bailey , though none was as comprehensive in breadth or style as Johnson's .\n",
      "['The', 'new', 'world', 'of', 'English', 'words', 'came', 'out', 'in', '1658', 'and', 'a', 'dictionary', 'of', '40,000', 'words', 'had', 'been', 'prepared', 'in', '1721', 'by', 'Nathan', 'Bailey', ',', 'though', 'none', 'was', 'as', 'comprehensive', 'in', 'breadth', 'or', 'style', 'as', \"Johnson's\", '.']\n",
      "['The', 'new', 'world', 'of', 'English', 'words', 'came', 'out', 'in', '1658', 'and', 'a', 'dictionary', 'of', '40,000', 'words', 'had', 'been', 'prepared', 'in', '1721', 'by', 'Nathan', 'Bailey', ',', 'though', 'none', 'was', 'as', 'comprehensive', 'in', 'breadth', 'or', 'style', 'as', 'Johnson', \"'s\", '.']\n",
      "{'token_span': [1, 2], 'label': 'world%1:14:01::'}\n",
      "{'token_span': [4, 5], 'label': 'word%1:10:00::'}\n",
      "{'token_span': [11, 12], 'label': 'dictionary%1:10:00::'}\n",
      "{'token_span': [14, 15], 'label': 'word%1:10:00::'}\n",
      "{'token_span': [32, 33], 'label': 'style%1:10:00::'}\n",
      "\n",
      "Ubald of Gubbio ( Italian : Ubaldo ; Latin : Ubaldus ; French : Ubalde ; ca. 1084 –1160 ) was a medieval bishop of Gubbio , in Umbria , today venerated as a saint by the Catholic Church .\n",
      "['Ubald', 'of', 'Gubbio', '(', 'Italian', ':', 'Ubaldo', ';', 'Latin', ':', 'Ubaldus', ';', 'French', ':', 'Ubalde', ';', 'ca.', '1084', '–1160', ')', 'was', 'a', 'medieval', 'bishop', 'of', 'Gubbio', ',', 'in', 'Umbria', ',', 'today', 'venerated', 'as', 'a', 'saint', 'by', 'the', 'Catholic', 'Church', '.']\n",
      "['Ubald', 'of', 'Gubbio', '(', 'Italian', ':', 'Ubaldo', ';', 'Latin', ':', 'Ubaldus', ';', 'French', ':', 'Ubalde', ';', 'ca', '.', '1084', '–', '1160', ')', 'was', 'a', 'medieval', 'bishop', 'of', 'Gubbio', ',', 'in', 'Umbria', ',', 'today', 'venerated', 'as', 'a', 'saint', 'by', 'the', 'Catholic', 'Church', '.']\n",
      "{'token_span': [23, 24], 'label': 'bishop%1:18:00::'}\n",
      "{'token_span': [34, 35], 'label': 'saint%1:18:03::'}\n",
      "\n",
      "Typically , NATO inert munitions are painted entirely in light blue and / or have the word \" INERT \" stenciled on them in prominent locations .[ citation needed ] IED ( barrel bomb , nail bomb , pipe bomb , pressure cooker bomb , fertilizer bomb , molotov cocktail )\n",
      "['Typically', ',', 'NATO', 'inert', 'munitions', 'are', 'painted', 'entirely', 'in', 'light', 'blue', 'and', '/', 'or', 'have', 'the', 'word', '\"', 'INERT', '\"', 'stenciled', 'on', 'them', 'in', 'prominent', 'locations', '.[', 'citation', 'needed', ']', 'IED', '(', 'barrel', 'bomb', ',', 'nail', 'bomb', ',', 'pipe', 'bomb', ',', 'pressure', 'cooker', 'bomb', ',', 'fertilizer', 'bomb', ',', 'molotov', 'cocktail', ')']\n",
      "['Typically', ',', 'NATO', 'inert', 'munitions', 'are', 'painted', 'entirely', 'in', 'light', 'blue', 'and', '/', 'or', 'have', 'the', 'word', '\"', 'INERT', '\"', 'stenciled', 'on', 'them', 'in', 'prominent', 'locations', '.', '[', 'citation', 'needed', ']', 'IED', '(', 'barrel', 'bomb', ',', 'nail', 'bomb', ',', 'pipe', 'bomb', ',', 'pressure', 'cooker', 'bomb', ',', 'fertilizer', 'bomb', ',', 'molotov', 'cocktail', ')']\n",
      "{'token_span': [16, 17], 'label': 'word%1:10:00::'}\n",
      "{'token_span': [35, 36], 'label': 'nail%1:06:00::'}\n",
      "{'token_span': [48, 49], 'label': 'cocktail%1:13:00::'}\n",
      "\n",
      "Even without the availability of either co-receptor ( even CCR5 ) , the virus can still invade cells if gp41 were to go through an alteration ( including its cytoplasmic tail ) that resulted in the independence of CD4 without the need of CCR5 and / or CXCR4 as a doorway .\n",
      "['Even', 'without', 'the', 'availability', 'of', 'either', 'co-receptor', '(', 'even', 'CCR5', ')', ',', 'the', 'virus', 'can', 'still', 'invade', 'cells', 'if', 'gp41', 'were', 'to', 'go', 'through', 'an', 'alteration', '(', 'including', 'its', 'cytoplasmic', 'tail', ')', 'that', 'resulted', 'in', 'the', 'independence', 'of', 'CD4', 'without', 'the', 'need', 'of', 'CCR5', 'and', '/', 'or', 'CXCR4', 'as', 'a', 'doorway', '.']\n",
      "['Even', 'without', 'the', 'availability', 'of', 'either', 'co', '-', 'receptor', '(', 'even', 'CCR5', ')', ',', 'the', 'virus', 'can', 'still', 'invade', 'cells', 'if', 'gp41', 'were', 'to', 'go', 'through', 'an', 'alteration', '(', 'including', 'its', 'cytoplasmic', 'tail', ')', 'that', 'resulted', 'in', 'the', 'independence', 'of', 'CD4', 'without', 'the', 'need', 'of', 'CCR5', 'and', '/', 'or', 'CXCR4', 'as', 'a', 'doorway', '.']\n",
      "{'token_span': [17, 18], 'label': 'cell%1:03:00::'}\n",
      "{'token_span': [30, 31], 'label': 'tail%1:05:00::'}\n",
      "{'token_span': [50, 51], 'label': 'doorway%1:06:00::'}\n",
      "\n",
      "However , it was never used for the soundtrack album at the last minute , and the song with added vocals from Slim Thug , was then added on Beyoncé 's former group Destiny 's Child 's greatest hits , # 1's ( 2005 ) .\n",
      "['However', ',', 'it', 'was', 'never', 'used', 'for', 'the', 'soundtrack', 'album', 'at', 'the', 'last', 'minute', ',', 'and', 'the', 'song', 'with', 'added', 'vocals', 'from', 'Slim', 'Thug', ',', 'was', 'then', 'added', 'on', 'Beyoncé', \"'s\", 'former', 'group', 'Destiny', \"'s\", 'Child', \"'s\", 'greatest', 'hits', ',', '#', \"1's\", '(', '2005', ')', '.']\n",
      "['However', ',', 'it', 'was', 'never', 'used', 'for', 'the', 'soundtrack', 'album', 'at', 'the', 'last', 'minute', ',', 'and', 'the', 'song', 'with', 'added', 'vocals', 'from', 'Slim', 'Thug', ',', 'was', 'then', 'added', 'on', 'Beyoncé', \"'s\", 'former', 'group', 'Destiny', \"'s\", 'Child', \"'s\", 'greatest', 'hits', ',', '#', '1', \"'s\", '(', '2005', ')', '.']\n",
      "{'token_span': [17, 18], 'label': 'song%1:10:00::'}\n",
      "{'token_span': [19, 20], 'label': 'add%2:30:00::'}\n",
      "{'token_span': [27, 28], 'label': 'add%2:30:00::'}\n",
      "{'token_span': [37, 38], 'label': 'great%5:00:01:important:00'}\n",
      "\n",
      "A 2016 meta-analysis of case - control and cohort studies found a moderate inverse association between garlic intake and some cancers of the upper digestive tract .\n",
      "['A', '2016', 'meta-analysis', 'of', 'case', '-', 'control', 'and', 'cohort', 'studies', 'found', 'a', 'moderate', 'inverse', 'association', 'between', 'garlic', 'intake', 'and', 'some', 'cancers', 'of', 'the', 'upper', 'digestive', 'tract', '.']\n",
      "['A', '2016', 'meta', '-', 'analysis', 'of', 'case', '-', 'control', 'and', 'cohort', 'studies', 'found', 'a', 'moderate', 'inverse', 'association', 'between', 'garlic', 'intake', 'and', 'some', 'cancers', 'of', 'the', 'upper', 'digestive', 'tract', '.']\n",
      "{'token_span': [10, 11], 'label': 'find%2:39:02::'}\n",
      "{'token_span': [20, 21], 'label': 'cancer%1:26:00::'}\n",
      "\n",
      "The total Swiss population change in 2008 ( from all sources ) was an increase of 10 and the non-Swiss population change was an increase of 4 people .\n",
      "['The', 'total', 'Swiss', 'population', 'change', 'in', '2008', '(', 'from', 'all', 'sources', ')', 'was', 'an', 'increase', 'of', '10', 'and', 'the', 'non-Swiss', 'population', 'change', 'was', 'an', 'increase', 'of', '4', 'people', '.']\n",
      "['The', 'total', 'Swiss', 'population', 'change', 'in', '2008', '(', 'from', 'all', 'sources', ')', 'was', 'an', 'increase', 'of', '10', 'and', 'the', 'non', '-', 'Swiss', 'population', 'change', 'was', 'an', 'increase', 'of', '4', 'people', '.']\n",
      "{'token_span': [3, 4], 'label': 'population%1:14:00::'}\n",
      "{'token_span': [20, 21], 'label': 'population%1:14:00::'}\n",
      "{'token_span': [27, 28], 'label': 'people%1:14:00::'}\n",
      "\n",
      "A 2014 meta-analysis of observational epidemiological studies found that garlic consumption was associated with a lower risk of stomach cancer in Korean people .\n",
      "['A', '2014', 'meta-analysis', 'of', 'observational', 'epidemiological', 'studies', 'found', 'that', 'garlic', 'consumption', 'was', 'associated', 'with', 'a', 'lower', 'risk', 'of', 'stomach', 'cancer', 'in', 'Korean', 'people', '.']\n",
      "['A', '2014', 'meta', '-', 'analysis', 'of', 'observational', 'epidemiological', 'studies', 'found', 'that', 'garlic', 'consumption', 'was', 'associated', 'with', 'a', 'lower', 'risk', 'of', 'stomach', 'cancer', 'in', 'Korean', 'people', '.']\n",
      "{'token_span': [7, 8], 'label': 'find%2:32:00::'}\n",
      "{'token_span': [18, 19], 'label': 'stomach%1:08:00::'}\n",
      "{'token_span': [19, 20], 'label': 'cancer%1:26:00::'}\n",
      "{'token_span': [22, 23], 'label': 'people%1:14:00::'}\n",
      "\n",
      "Individuals homozygous ( denoted Δ32/Δ32 ) for CCR5 Δ32 do not express functional CCR5 receptors on their cell surfaces and are resistant to HIV - 1 infection , despite multiple high - risk exposures .\n",
      "['Individuals', 'homozygous', '(', 'denoted', 'Δ32/Δ32', ')', 'for', 'CCR5', 'Δ32', 'do', 'not', 'express', 'functional', 'CCR5', 'receptors', 'on', 'their', 'cell', 'surfaces', 'and', 'are', 'resistant', 'to', 'HIV', '-', '1', 'infection', ',', 'despite', 'multiple', 'high', '-', 'risk', 'exposures', '.']\n",
      "['Individuals', 'homozygous', '(', 'denoted', 'Δ32', '/', 'Δ32', ')', 'for', 'CCR5', 'Δ32', 'do', 'not', 'express', 'functional', 'CCR5', 'receptors', 'on', 'their', 'cell', 'surfaces', 'and', 'are', 'resistant', 'to', 'HIV', '-', '1', 'infection', ',', 'despite', 'multiple', 'high', '-', 'risk', 'exposures', '.']\n",
      "{'token_span': [17, 18], 'label': 'cell%1:03:00::'}\n",
      "{'token_span': [26, 27], 'label': 'infection%1:26:00::'}\n",
      "\n",
      "Another group , Libert et al. ( 1998 ) , used microsatellite mutations to estimate the age of the CCR5 Δ32 mutation to be 2100 years ( 700 - 4800 , 95 % confidence interval ) .\n",
      "['Another', 'group', ',', 'Libert', 'et', 'al.', '(', '1998', ')', ',', 'used', 'microsatellite', 'mutations', 'to', 'estimate', 'the', 'age', 'of', 'the', 'CCR5', 'Δ32', 'mutation', 'to', 'be', '2100', 'years', '(', '700', '-', '4800', ',', '95', '%', 'confidence', 'interval', ')', '.']\n",
      "['Another', 'group', ',', 'Libert', 'et', 'al', '.', '(', '1998', ')', ',', 'used', 'microsatellite', 'mutations', 'to', 'estimate', 'the', 'age', 'of', 'the', 'CCR5', 'Δ32', 'mutation', 'to', 'be', '2100', 'years', '(', '700', '-', '4800', ',', '95', '%', 'confidence', 'interval', ')', '.']\n",
      "{'token_span': [16, 17], 'label': 'age%1:07:00::'}\n",
      "{'token_span': [25, 26], 'label': 'year%1:28:01::'}\n",
      "{'token_span': [33, 34], 'label': 'confidence%1:12:00::'}\n",
      "\n",
      "With quarterback Peyton Manning , wide receiver Marvin Harrison , and running back Edgerrin James , the Colts had scored 79 points in their 2 playoff victories against the Denver Broncos and the Kansas City Chiefs , including a 38 –31 victory over the Chiefs in the first punt - less game in NFL playoff history .\n",
      "['With', 'quarterback', 'Peyton', 'Manning', ',', 'wide', 'receiver', 'Marvin', 'Harrison', ',', 'and', 'running', 'back', 'Edgerrin', 'James', ',', 'the', 'Colts', 'had', 'scored', '79', 'points', 'in', 'their', '2', 'playoff', 'victories', 'against', 'the', 'Denver', 'Broncos', 'and', 'the', 'Kansas', 'City', 'Chiefs', ',', 'including', 'a', '38', '–31', 'victory', 'over', 'the', 'Chiefs', 'in', 'the', 'first', 'punt', '-', 'less', 'game', 'in', 'NFL', 'playoff', 'history', '.']\n",
      "['With', 'quarterback', 'Peyton', 'Manning', ',', 'wide', 'receiver', 'Marvin', 'Harrison', ',', 'and', 'running', 'back', 'Edgerrin', 'James', ',', 'the', 'Colts', 'had', 'scored', '79', 'points', 'in', 'their', '2', 'playoff', 'victories', 'against', 'the', 'Denver', 'Broncos', 'and', 'the', 'Kansas', 'City', 'Chiefs', ',', 'including', 'a', '38', '–', '31', 'victory', 'over', 'the', 'Chiefs', 'in', 'the', 'first', 'punt', '-', 'less', 'game', 'in', 'NFL', 'playoff', 'history', '.']\n",
      "{'token_span': [17, 18], 'label': 'colt%1:05:02::'}\n",
      "{'token_span': [26, 27], 'label': 'victory%1:11:00::'}\n",
      "{'token_span': [41, 42], 'label': 'victory%1:11:00::'}\n",
      "{'token_span': [51, 52], 'label': 'game%1:04:03::'}\n",
      "\n",
      "The side aisles are vaulted with five cross-ribbed vaults on an almost square floor plan , which is joined by one cross rib vault in the tower .\n",
      "['The', 'side', 'aisles', 'are', 'vaulted', 'with', 'five', 'cross-ribbed', 'vaults', 'on', 'an', 'almost', 'square', 'floor', 'plan', ',', 'which', 'is', 'joined', 'by', 'one', 'cross', 'rib', 'vault', 'in', 'the', 'tower', '.']\n",
      "['The', 'side', 'aisles', 'are', 'vaulted', 'with', 'five', 'cross', '-', 'ribbed', 'vaults', 'on', 'an', 'almost', 'square', 'floor', 'plan', ',', 'which', 'is', 'joined', 'by', 'one', 'cross', 'rib', 'vault', 'in', 'the', 'tower', '.']\n",
      "{'token_span': [1, 2], 'label': 'side%1:15:02::'}\n",
      "{'token_span': [2, 3], 'label': 'aisle%1:06:00::'}\n",
      "{'token_span': [18, 19], 'label': 'join%2:35:01::'}\n",
      "{'token_span': [26, 27], 'label': 'tower%1:06:00::'}\n",
      "\n",
      "Janaki Manaki in 1899 Janaki Manaki photographed vomiting due to sickness on a ship while traveling from Paris to London in order to buy a film camera ( Paris , 1905 ) Janaki Manaki filmed in front of the audience with King Carol I ( Bucharest , 1906 ) Milton Manaki on a carriage , photographed on the road between Grevena and Sorovikj ( 1913 ) Janaki Manaki in Plovdiv ( 1916 –1919 ) Milton Manaki ( first from right ) with fiancée Vasilikija Dauka and other relatives ( Grevena , 1928 ) The construction of the Manaki cinema ( Bitola , 1923 ) Ruins of the Manaki Brothers cinema after the fire ( Bitola , 1939 ) Biography Early life Lumina , № 10 festival , Octombrie 1905 , p. 304 , # 84 .\n",
      "['Janaki', 'Manaki', 'in', '1899', 'Janaki', 'Manaki', 'photographed', 'vomiting', 'due', 'to', 'sickness', 'on', 'a', 'ship', 'while', 'traveling', 'from', 'Paris', 'to', 'London', 'in', 'order', 'to', 'buy', 'a', 'film', 'camera', '(', 'Paris', ',', '1905', ')', 'Janaki', 'Manaki', 'filmed', 'in', 'front', 'of', 'the', 'audience', 'with', 'King', 'Carol', 'I', '(', 'Bucharest', ',', '1906', ')', 'Milton', 'Manaki', 'on', 'a', 'carriage', ',', 'photographed', 'on', 'the', 'road', 'between', 'Grevena', 'and', 'Sorovikj', '(', '1913', ')', 'Janaki', 'Manaki', 'in', 'Plovdiv', '(', '1916', '–1919', ')', 'Milton', 'Manaki', '(', 'first', 'from', 'right', ')', 'with', 'fiancée', 'Vasilikija', 'Dauka', 'and', 'other', 'relatives', '(', 'Grevena', ',', '1928', ')', 'The', 'construction', 'of', 'the', 'Manaki', 'cinema', '(', 'Bitola', ',', '1923', ')', 'Ruins', 'of', 'the', 'Manaki', 'Brothers', 'cinema', 'after', 'the', 'fire', '(', 'Bitola', ',', '1939', ')', 'Biography', 'Early', 'life', 'Lumina', ',', '№', '10', 'festival', ',', 'Octombrie', '1905', ',', 'p.', '304', ',', '#', '84', '.']\n",
      "['Janaki', 'Manaki', 'in', '1899', 'Janaki', 'Manaki', 'photographed', 'vomiting', 'due', 'to', 'sickness', 'on', 'a', 'ship', 'while', 'traveling', 'from', 'Paris', 'to', 'London', 'in', 'order', 'to', 'buy', 'a', 'film', 'camera', '(', 'Paris', ',', '1905', ')', 'Janaki', 'Manaki', 'filmed', 'in', 'front', 'of', 'the', 'audience', 'with', 'King', 'Carol', 'I', '(', 'Bucharest', ',', '1906', ')', 'Milton', 'Manaki', 'on', 'a', 'carriage', ',', 'photographed', 'on', 'the', 'road', 'between', 'Grevena', 'and', 'Sorovikj', '(', '1913', ')', 'Janaki', 'Manaki', 'in', 'Plovdiv', '(', '1916', '–', '1919', ')', 'Milton', 'Manaki', '(', 'first', 'from', 'right', ')', 'with', 'fiancée', 'Vasilikija', 'Dauka', 'and', 'other', 'relatives', '(', 'Grevena', ',', '1928', ')', 'The', 'construction', 'of', 'the', 'Manaki', 'cinema', '(', 'Bitola', ',', '1923', ')', 'Ruins', 'of', 'the', 'Manaki', 'Brothers', 'cinema', 'after', 'the', 'fire', '(', 'Bitola', ',', '1939', ')', 'Biography', 'Early', 'life', 'Lumina', ',', '№', '10', 'festival', ',', 'Octombrie', '1905', ',', 'p.', '304', ',', '#', '84', '.']\n",
      "{'token_span': [10, 11], 'label': 'sickness%1:26:01::'}\n",
      "{'token_span': [21, 22], 'label': 'order%1:10:04::'}\n",
      "{'token_span': [23, 24], 'label': 'buy%2:40:00::'}\n",
      "{'token_span': [25, 26], 'label': 'film%1:10:01::'}\n",
      "{'token_span': [26, 27], 'label': 'camera%1:06:00::'}\n",
      "{'token_span': [34, 35], 'label': 'film%2:32:00::'}\n",
      "{'token_span': [41, 42], 'label': 'king%1:18:00::'}\n",
      "{'token_span': [53, 54], 'label': 'carriage%1:06:00::'}\n",
      "{'token_span': [79, 80], 'label': 'right%1:15:00::'}\n",
      "{'token_span': [94, 95], 'label': 'construction%1:04:00::'}\n",
      "{'token_span': [108, 109], 'label': 'brother%1:18:00::'}\n",
      "{'token_span': [118, 119], 'label': 'biography%1:10:00::'}\n",
      "\n",
      "Negative reviews included Mark Olsen of Boxoffice Pro's claim that , \" this supposedly honest behind - the - scenes look at Perry 's 2011 world tour often feels more like sharp brand management \" .\n",
      "['Negative', 'reviews', 'included', 'Mark', 'Olsen', 'of', 'Boxoffice', \"Pro's\", 'claim', 'that', ',', '\"', 'this', 'supposedly', 'honest', 'behind', '-', 'the', '-', 'scenes', 'look', 'at', 'Perry', \"'s\", '2011', 'world', 'tour', 'often', 'feels', 'more', 'like', 'sharp', 'brand', 'management', '\"', '.']\n",
      "['Negative', 'reviews', 'included', 'Mark', 'Olsen', 'of', 'Boxoffice', 'Pro', \"'s\", 'claim', 'that', ',', '\"', 'this', 'supposedly', 'honest', 'behind', '-', 'the', '-', 'scenes', 'look', 'at', 'Perry', \"'s\", '2011', 'world', 'tour', 'often', 'feels', 'more', 'like', 'sharp', 'brand', 'management', '\"', '.']\n",
      "{'token_span': [14, 15], 'label': 'honest%3:00:00::'}\n",
      "{'token_span': [25, 26], 'label': 'world%1:17:00::'}\n",
      "{'token_span': [31, 32], 'label': 'sharp%5:00:00:smart:00'}\n",
      "{'token_span': [33, 34], 'label': 'management%1:04:00::'}\n",
      "\n",
      "If at the end of extra-time , the aggregate score is still equal , then the team scoring the most goals in the second leg will be declared winner . ”\n",
      "['If', 'at', 'the', 'end', 'of', 'extra-time', ',', 'the', 'aggregate', 'score', 'is', 'still', 'equal', ',', 'then', 'the', 'team', 'scoring', 'the', 'most', 'goals', 'in', 'the', 'second', 'leg', 'will', 'be', 'declared', 'winner', '.', '”']\n",
      "['If', 'at', 'the', 'end', 'of', 'extra', '-', 'time', ',', 'the', 'aggregate', 'score', 'is', 'still', 'equal', ',', 'then', 'the', 'team', 'scoring', 'the', 'most', 'goals', 'in', 'the', 'second', 'leg', 'will', 'be', 'declared', 'winner', '.', '”']\n",
      "{'token_span': [20, 21], 'label': 'goal%1:04:00::'}\n",
      "{'token_span': [23, 24], 'label': 'second%5:00:00:ordinal:00'}\n",
      "{'token_span': [24, 25], 'label': 'leg%1:04:00::'}\n",
      "{'token_span': [27, 28], 'label': 'declare%2:32:04::'}\n",
      "{'token_span': [28, 29], 'label': 'winner%1:18:00::'}\n",
      "\n",
      "Eli Biham , cryptanalyst and cryptographer Yaakov Dori , President Avram Hershko and Aaron Ciechanover , recipients of the 2004 Nobel Prize in chemistry for the discovery of ubiquitin - mediated protein degradation Amos Horev , former president , former Chairman of Rafael ; member of the Israeli Turkel Commission of Inquiry into the Gaza flotilla raid Abraham Lempel and Jacob Ziv , developers of the Lempel - Ziv ( LZW ) compression algorithm Liviu Librescu , hero during the Virginia Tech shooting Marcelle Machluf , biotechnology and food engineering Shlomo Moran , computer scientist Yehudit Naot , scientist and politician , Israeli Minister of the Environment Eliahu Nissim ( born 1933 ) , Professor of Aeronautical Engineering ; President of the Open University of Israel Asher Peres , co-discoverer of quantum teleportation , awarded the 2004 Rothschild Prize in Physics Anat Rafaeli , organisational behaviour researcher Nathan Rosen , co-author with Albert Einstein and Boris Podolsky of physics paper about the EPR paradox in quantum mechanics Rachel Shalon , first woman engineer in Israel Shlomo Shamai , electrical information theorist , winner of the 2011 Shannon Award .\n",
      "['Eli', 'Biham', ',', 'cryptanalyst', 'and', 'cryptographer', 'Yaakov', 'Dori', ',', 'President', 'Avram', 'Hershko', 'and', 'Aaron', 'Ciechanover', ',', 'recipients', 'of', 'the', '2004', 'Nobel', 'Prize', 'in', 'chemistry', 'for', 'the', 'discovery', 'of', 'ubiquitin', '-', 'mediated', 'protein', 'degradation', 'Amos', 'Horev', ',', 'former', 'president', ',', 'former', 'Chairman', 'of', 'Rafael', ';', 'member', 'of', 'the', 'Israeli', 'Turkel', 'Commission', 'of', 'Inquiry', 'into', 'the', 'Gaza', 'flotilla', 'raid', 'Abraham', 'Lempel', 'and', 'Jacob', 'Ziv', ',', 'developers', 'of', 'the', 'Lempel', '-', 'Ziv', '(', 'LZW', ')', 'compression', 'algorithm', 'Liviu', 'Librescu', ',', 'hero', 'during', 'the', 'Virginia', 'Tech', 'shooting', 'Marcelle', 'Machluf', ',', 'biotechnology', 'and', 'food', 'engineering', 'Shlomo', 'Moran', ',', 'computer', 'scientist', 'Yehudit', 'Naot', ',', 'scientist', 'and', 'politician', ',', 'Israeli', 'Minister', 'of', 'the', 'Environment', 'Eliahu', 'Nissim', '(', 'born', '1933', ')', ',', 'Professor', 'of', 'Aeronautical', 'Engineering', ';', 'President', 'of', 'the', 'Open', 'University', 'of', 'Israel', 'Asher', 'Peres', ',', 'co-discoverer', 'of', 'quantum', 'teleportation', ',', 'awarded', 'the', '2004', 'Rothschild', 'Prize', 'in', 'Physics', 'Anat', 'Rafaeli', ',', 'organisational', 'behaviour', 'researcher', 'Nathan', 'Rosen', ',', 'co-author', 'with', 'Albert', 'Einstein', 'and', 'Boris', 'Podolsky', 'of', 'physics', 'paper', 'about', 'the', 'EPR', 'paradox', 'in', 'quantum', 'mechanics', 'Rachel', 'Shalon', ',', 'first', 'woman', 'engineer', 'in', 'Israel', 'Shlomo', 'Shamai', ',', 'electrical', 'information', 'theorist', ',', 'winner', 'of', 'the', '2011', 'Shannon', 'Award', '.']\n",
      "['Eli', 'Biham', ',', 'cryptanalyst', 'and', 'cryptographer', 'Yaakov', 'Dori', ',', 'President', 'Avram', 'Hershko', 'and', 'Aaron', 'Ciechanover', ',', 'recipients', 'of', 'the', '2004', 'Nobel', 'Prize', 'in', 'chemistry', 'for', 'the', 'discovery', 'of', 'ubiquitin', '-', 'mediated', 'protein', 'degradation', 'Amos', 'Horev', ',', 'former', 'president', ',', 'former', 'Chairman', 'of', 'Rafael', ';', 'member', 'of', 'the', 'Israeli', 'Turkel', 'Commission', 'of', 'Inquiry', 'into', 'the', 'Gaza', 'flotilla', 'raid', 'Abraham', 'Lempel', 'and', 'Jacob', 'Ziv', ',', 'developers', 'of', 'the', 'Lempel', '-', 'Ziv', '(', 'LZW', ')', 'compression', 'algorithm', 'Liviu', 'Librescu', ',', 'hero', 'during', 'the', 'Virginia', 'Tech', 'shooting', 'Marcelle', 'Machluf', ',', 'biotechnology', 'and', 'food', 'engineering', 'Shlomo', 'Moran', ',', 'computer', 'scientist', 'Yehudit', 'Naot', ',', 'scientist', 'and', 'politician', ',', 'Israeli', 'Minister', 'of', 'the', 'Environment', 'Eliahu', 'Nissim', '(', 'born', '1933', ')', ',', 'Professor', 'of', 'Aeronautical', 'Engineering', ';', 'President', 'of', 'the', 'Open', 'University', 'of', 'Israel', 'Asher', 'Peres', ',', 'co', '-', 'discoverer', 'of', 'quantum', 'teleportation', ',', 'awarded', 'the', '2004', 'Rothschild', 'Prize', 'in', 'Physics', 'Anat', 'Rafaeli', ',', 'organisational', 'behaviour', 'researcher', 'Nathan', 'Rosen', ',', 'co', '-', 'author', 'with', 'Albert', 'Einstein', 'and', 'Boris', 'Podolsky', 'of', 'physics', 'paper', 'about', 'the', 'EPR', 'paradox', 'in', 'quantum', 'mechanics', 'Rachel', 'Shalon', ',', 'first', 'woman', 'engineer', 'in', 'Israel', 'Shlomo', 'Shamai', ',', 'electrical', 'information', 'theorist', ',', 'winner', 'of', 'the', '2011', 'Shannon', 'Award', '.']\n",
      "{'token_span': [9, 10], 'label': 'president%1:18:01::'}\n",
      "{'token_span': [23, 24], 'label': 'chemistry%1:09:00::'}\n",
      "{'token_span': [37, 38], 'label': 'president%1:18:01::'}\n",
      "{'token_span': [77, 78], 'label': 'hero%1:18:00::'}\n",
      "{'token_span': [98, 99], 'label': 'scientist%1:18:00::'}\n",
      "{'token_span': [100, 101], 'label': 'politician%1:18:01::'}\n",
      "{'token_span': [103, 104], 'label': 'minister%1:18:02::'}\n",
      "{'token_span': [118, 119], 'label': 'president%1:18:02::'}\n",
      "{'token_span': [157, 158], 'label': 'paper%1:10:02::'}\n",
      "{'token_span': [164, 165], 'label': 'mechanic%1:18:00::'}\n",
      "{'token_span': [169, 170], 'label': 'woman%1:18:00::'}\n",
      "{'token_span': [177, 178], 'label': 'information%1:10:00::'}\n",
      "{'token_span': [180, 181], 'label': 'winner%1:18:00::'}\n",
      "\n",
      "This warrior had multi-colored weapons and armor .\n",
      "['This', 'warrior', 'had', 'multi-colored', 'weapons', 'and', 'armor', '.']\n",
      "['This', 'warrior', 'had', 'multi', '-', 'colored', 'weapons', 'and', 'armor', '.']\n",
      "{'token_span': [1, 2], 'label': 'warrior%1:18:00::'}\n",
      "\n",
      "Rose , feeling guilty over her treatment of Charlie , returns to the hospital and discovers that Charlie is in a coma in the room next to her mother's .\n",
      "['Rose', ',', 'feeling', 'guilty', 'over', 'her', 'treatment', 'of', 'Charlie', ',', 'returns', 'to', 'the', 'hospital', 'and', 'discovers', 'that', 'Charlie', 'is', 'in', 'a', 'coma', 'in', 'the', 'room', 'next', 'to', 'her', \"mother's\", '.']\n",
      "['Rose', ',', 'feeling', 'guilty', 'over', 'her', 'treatment', 'of', 'Charlie', ',', 'returns', 'to', 'the', 'hospital', 'and', 'discovers', 'that', 'Charlie', 'is', 'in', 'a', 'coma', 'in', 'the', 'room', 'next', 'to', 'her', 'mother', \"'s\", '.']\n",
      "{'token_span': [3, 4], 'label': 'guilty%3:00:00::'}\n",
      "{'token_span': [13, 14], 'label': 'hospital%1:14:00::'}\n",
      "{'token_span': [15, 16], 'label': 'discover%2:31:01::'}\n",
      "{'token_span': [24, 25], 'label': 'room%1:06:00::'}\n",
      "\n",
      "The idea is to start with an initial guess , then to approximate the function by its tangent line , and finally to compute the x-intercept of this tangent line .\n",
      "['The', 'idea', 'is', 'to', 'start', 'with', 'an', 'initial', 'guess', ',', 'then', 'to', 'approximate', 'the', 'function', 'by', 'its', 'tangent', 'line', ',', 'and', 'finally', 'to', 'compute', 'the', 'x-intercept', 'of', 'this', 'tangent', 'line', '.']\n",
      "['The', 'idea', 'is', 'to', 'start', 'with', 'an', 'initial', 'guess', ',', 'then', 'to', 'approximate', 'the', 'function', 'by', 'its', 'tangent', 'line', ',', 'and', 'finally', 'to', 'compute', 'the', 'x', '-', 'intercept', 'of', 'this', 'tangent', 'line', '.']\n",
      "{'token_span': [1, 2], 'label': 'idea%1:09:00::'}\n",
      "\n",
      "The county has several other elected offices , including sheriff , coroner , auditor , treasurer , recorder , surveyor and circuit court clerk ; they are elected to four –year terms .\n",
      "['The', 'county', 'has', 'several', 'other', 'elected', 'offices', ',', 'including', 'sheriff', ',', 'coroner', ',', 'auditor', ',', 'treasurer', ',', 'recorder', ',', 'surveyor', 'and', 'circuit', 'court', 'clerk', ';', 'they', 'are', 'elected', 'to', 'four', '–year', 'terms', '.']\n",
      "['The', 'county', 'has', 'several', 'other', 'elected', 'offices', ',', 'including', 'sheriff', ',', 'coroner', ',', 'auditor', ',', 'treasurer', ',', 'recorder', ',', 'surveyor', 'and', 'circuit', 'court', 'clerk', ';', 'they', 'are', 'elected', 'to', 'four', '–', 'year', 'terms', '.']\n",
      "{'token_span': [5, 6], 'label': 'elect%2:41:00::'}\n",
      "{'token_span': [9, 10], 'label': 'sheriff%1:18:00::'}\n",
      "{'token_span': [26, 27], 'label': 'elect%2:41:00::'}\n",
      "\n",
      "To prevent the pilot from exceeding a 17° angle of attack , the control column incorporated a \" knuckle rapper \" which would strike the pilot 's knuckles as the limit was approached .\n",
      "['To', 'prevent', 'the', 'pilot', 'from', 'exceeding', 'a', '17°', 'angle', 'of', 'attack', ',', 'the', 'control', 'column', 'incorporated', 'a', '\"', 'knuckle', 'rapper', '\"', 'which', 'would', 'strike', 'the', 'pilot', \"'s\", 'knuckles', 'as', 'the', 'limit', 'was', 'approached', '.']\n",
      "['To', 'prevent', 'the', 'pilot', 'from', 'exceeding', 'a', '17', '°', 'angle', 'of', 'attack', ',', 'the', 'control', 'column', 'incorporated', 'a', '\"', 'knuckle', 'rapper', '\"', 'which', 'would', 'strike', 'the', 'pilot', \"'s\", 'knuckles', 'as', 'the', 'limit', 'was', 'approached', '.']\n",
      "{'token_span': [3, 4], 'label': 'pilot%1:18:00::'}\n",
      "{'token_span': [25, 26], 'label': 'pilot%1:18:00::'}\n",
      "\n",
      "Notes : Gżira United were awarded a 3 –0 win after the Malta Football Association 's Protests Board upheld their protest .\n",
      "['Notes', ':', 'Gżira', 'United', 'were', 'awarded', 'a', '3', '–0', 'win', 'after', 'the', 'Malta', 'Football', 'Association', \"'s\", 'Protests', 'Board', 'upheld', 'their', 'protest', '.']\n",
      "['Notes', ':', 'Gżira', 'United', 'were', 'awarded', 'a', '3', '–', '0', 'win', 'after', 'the', 'Malta', 'Football', 'Association', \"'s\", 'Protests', 'Board', 'upheld', 'their', 'protest', '.']\n",
      "{'token_span': [9, 10], 'label': 'win%1:11:00::'}\n",
      "{'token_span': [13, 14], 'label': 'football%1:04:00::'}\n",
      "\n",
      "This fuel capacity gave the MiG - 23 better endurance than a \" clean \" F - 4 ( carrying no drop tanks ) ; if traveling at the MiG - 23's endurance speed of 230 knots an individual sortie could be stretched out to an hour , though if afterburner was involved that could fall down to around 45 minutes or less .\n",
      "['This', 'fuel', 'capacity', 'gave', 'the', 'MiG', '-', '23', 'better', 'endurance', 'than', 'a', '\"', 'clean', '\"', 'F', '-', '4', '(', 'carrying', 'no', 'drop', 'tanks', ')', ';', 'if', 'traveling', 'at', 'the', 'MiG', '-', \"23's\", 'endurance', 'speed', 'of', '230', 'knots', 'an', 'individual', 'sortie', 'could', 'be', 'stretched', 'out', 'to', 'an', 'hour', ',', 'though', 'if', 'afterburner', 'was', 'involved', 'that', 'could', 'fall', 'down', 'to', 'around', '45', 'minutes', 'or', 'less', '.']\n",
      "['This', 'fuel', 'capacity', 'gave', 'the', 'MiG', '-', '23', 'better', 'endurance', 'than', 'a', '\"', 'clean', '\"', 'F', '-', '4', '(', 'carrying', 'no', 'drop', 'tanks', ')', ';', 'if', 'traveling', 'at', 'the', 'MiG', '-', '23', \"'s\", 'endurance', 'speed', 'of', '230', 'knots', 'an', 'individual', 'sortie', 'could', 'be', 'stretched', 'out', 'to', 'an', 'hour', ',', 'though', 'if', 'afterburner', 'was', 'involved', 'that', 'could', 'fall', 'down', 'to', 'around', '45', 'minutes', 'or', 'less', '.']\n",
      "{'token_span': [3, 4], 'label': 'give%2:40:05::'}\n",
      "{'token_span': [9, 10], 'label': 'endurance%1:07:00::'}\n",
      "{'token_span': [19, 20], 'label': 'carry%2:35:02::'}\n",
      "{'token_span': [32, 33], 'label': 'endurance%1:07:00::'}\n",
      "{'token_span': [60, 61], 'label': 'minute%1:28:00::'}\n",
      "\n",
      "His grandfather Richard Evelyn Byrd Sr. served as the Speaker of the Virginia House of Delegates , and his father had served as a Virginia state senator , Governor of Virginia and United States senator .\n",
      "['His', 'grandfather', 'Richard', 'Evelyn', 'Byrd', 'Sr.', 'served', 'as', 'the', 'Speaker', 'of', 'the', 'Virginia', 'House', 'of', 'Delegates', ',', 'and', 'his', 'father', 'had', 'served', 'as', 'a', 'Virginia', 'state', 'senator', ',', 'Governor', 'of', 'Virginia', 'and', 'United', 'States', 'senator', '.']\n",
      "['His', 'grandfather', 'Richard', 'Evelyn', 'Byrd', 'Sr', '.', 'served', 'as', 'the', 'Speaker', 'of', 'the', 'Virginia', 'House', 'of', 'Delegates', ',', 'and', 'his', 'father', 'had', 'served', 'as', 'a', 'Virginia', 'state', 'senator', ',', 'Governor', 'of', 'Virginia', 'and', 'United', 'States', 'senator', '.']\n",
      "{'token_span': [19, 20], 'label': 'father%1:18:00::'}\n",
      "\n",
      "The MiG - 23's deficits and qualities were also recognized by allied air forces who received the fighter from the Soviet Union , including the East German Air Force : I spent a lot of time in Berlin watching GCI tapes to verify we were flying the right tactics , and it became clear to me that the East Germans knew exactly what the MiG - 23 's limitations were .\n",
      "['The', 'MiG', '-', \"23's\", 'deficits', 'and', 'qualities', 'were', 'also', 'recognized', 'by', 'allied', 'air', 'forces', 'who', 'received', 'the', 'fighter', 'from', 'the', 'Soviet', 'Union', ',', 'including', 'the', 'East', 'German', 'Air', 'Force', ':', 'I', 'spent', 'a', 'lot', 'of', 'time', 'in', 'Berlin', 'watching', 'GCI', 'tapes', 'to', 'verify', 'we', 'were', 'flying', 'the', 'right', 'tactics', ',', 'and', 'it', 'became', 'clear', 'to', 'me', 'that', 'the', 'East', 'Germans', 'knew', 'exactly', 'what', 'the', 'MiG', '-', '23', \"'s\", 'limitations', 'were', '.']\n",
      "['The', 'MiG', '-', '23', \"'s\", 'deficits', 'and', 'qualities', 'were', 'also', 'recognized', 'by', 'allied', 'air', 'forces', 'who', 'received', 'the', 'fighter', 'from', 'the', 'Soviet', 'Union', ',', 'including', 'the', 'East', 'German', 'Air', 'Force', ':', 'I', 'spent', 'a', 'lot', 'of', 'time', 'in', 'Berlin', 'watching', 'GCI', 'tapes', 'to', 'verify', 'we', 'were', 'flying', 'the', 'right', 'tactics', ',', 'and', 'it', 'became', 'clear', 'to', 'me', 'that', 'the', 'East', 'Germans', 'knew', 'exactly', 'what', 'the', 'MiG', '-', '23', \"'s\", 'limitations', 'were', '.']\n",
      "{'token_span': [15, 16], 'label': 'receive%2:40:00::'}\n",
      "{'token_span': [31, 32], 'label': 'spend%2:42:00::'}\n",
      "{'token_span': [42, 43], 'label': 'verify%2:31:01::'}\n",
      "{'token_span': [47, 48], 'label': 'right%5:00:01:proper:00'}\n",
      "{'token_span': [52, 53], 'label': 'become%2:30:00::'}\n",
      "{'token_span': [60, 61], 'label': 'know%2:31:01::'}\n",
      "\n",
      "Public Opinion explains who she is – the guardian of morality ( \" Qui suis - je? du Théâtre Antique \" ) .\n",
      "['Public', 'Opinion', 'explains', 'who', 'she', 'is', '–', 'the', 'guardian', 'of', 'morality', '(', '\"', 'Qui', 'suis', '-', 'je?', 'du', 'Théâtre', 'Antique', '\"', ')', '.']\n",
      "['Public', 'Opinion', 'explains', 'who', 'she', 'is', '–', 'the', 'guardian', 'of', 'morality', '(', '\"', 'Qui', 'suis', '-', 'je', '?', 'du', 'Théâtre', 'Antique', '\"', ')', '.']\n",
      "{'token_span': [0, 1], 'label': 'opinion%1:10:01::'}\n",
      "{'token_span': [1, 2], 'label': 'explain%2:32:00::'}\n",
      "{'token_span': [7, 8], 'label': 'guardian%1:18:00::'}\n",
      "\n",
      "Based on Dutch Totaal Voetbal , Zdeněk Zeman has described the concept of his 4 –3 –3 system as \" geometry \" , and his teams also known for approaching a football philosophy clearly based on lively , quick , attacking play , the offside , and zonal marking .\n",
      "['Based', 'on', 'Dutch', 'Totaal', 'Voetbal', ',', 'Zdeněk', 'Zeman', 'has', 'described', 'the', 'concept', 'of', 'his', '4', '–3', '–3', 'system', 'as', '\"', 'geometry', '\"', ',', 'and', 'his', 'teams', 'also', 'known', 'for', 'approaching', 'a', 'football', 'philosophy', 'clearly', 'based', 'on', 'lively', ',', 'quick', ',', 'attacking', 'play', ',', 'the', 'offside', ',', 'and', 'zonal', 'marking', '.']\n",
      "['Based', 'on', 'Dutch', 'Totaal', 'Voetbal', ',', 'Zdeněk', 'Zeman', 'has', 'described', 'the', 'concept', 'of', 'his', '4', '–', '3', '–', '3', 'system', 'as', '\"', 'geometry', '\"', ',', 'and', 'his', 'teams', 'also', 'known', 'for', 'approaching', 'a', 'football', 'philosophy', 'clearly', 'based', 'on', 'lively', ',', 'quick', ',', 'attacking', 'play', ',', 'the', 'offside', ',', 'and', 'zonal', 'marking', '.']\n",
      "{'token_span': [11, 12], 'label': 'concept%1:09:00::'}\n",
      "{'token_span': [27, 28], 'label': 'know%2:31:03::'}\n",
      "{'token_span': [31, 32], 'label': 'football%1:04:00::'}\n",
      "{'token_span': [38, 39], 'label': 'quick%5:00:00:fast:01'}\n",
      "\n",
      "During the Second World War years , when long cross-county trips were impossible due to the war , Wrexham played in the Regional League West against local teams from Merseyside and Manchester , amongst others in the north west region .\n",
      "['During', 'the', 'Second', 'World', 'War', 'years', ',', 'when', 'long', 'cross-county', 'trips', 'were', 'impossible', 'due', 'to', 'the', 'war', ',', 'Wrexham', 'played', 'in', 'the', 'Regional', 'League', 'West', 'against', 'local', 'teams', 'from', 'Merseyside', 'and', 'Manchester', ',', 'amongst', 'others', 'in', 'the', 'north', 'west', 'region', '.']\n",
      "['During', 'the', 'Second', 'World', 'War', 'years', ',', 'when', 'long', 'cross', '-', 'county', 'trips', 'were', 'impossible', 'due', 'to', 'the', 'war', ',', 'Wrexham', 'played', 'in', 'the', 'Regional', 'League', 'West', 'against', 'local', 'teams', 'from', 'Merseyside', 'and', 'Manchester', ',', 'amongst', 'others', 'in', 'the', 'north', 'west', 'region', '.']\n",
      "{'token_span': [2, 3], 'label': 'world%1:17:00::'}\n",
      "{'token_span': [4, 5], 'label': 'year%1:28:01::'}\n",
      "{'token_span': [7, 8], 'label': 'long%3:00:02::'}\n",
      "{'token_span': [9, 10], 'label': 'trip%1:04:00::'}\n",
      "{'token_span': [23, 24], 'label': 'west%1:15:01::'}\n",
      "{'token_span': [36, 37], 'label': 'north%3:00:00::'}\n",
      "{'token_span': [37, 38], 'label': 'west%3:00:00::'}\n",
      "{'token_span': [38, 39], 'label': 'region%1:15:00::'}\n",
      "\n",
      "Written and recorded in the 11th hour of The Sport of Kings sessions , in an attempt to deliver a hit ' single ' to satisfy the demands of the record company , \" Somebody 's Out There \" made it to No. 27 on Billboard Hot 100 during October and November 1986 .\n",
      "['Written', 'and', 'recorded', 'in', 'the', '11th', 'hour', 'of', 'The', 'Sport', 'of', 'Kings', 'sessions', ',', 'in', 'an', 'attempt', 'to', 'deliver', 'a', 'hit', \"'\", 'single', \"'\", 'to', 'satisfy', 'the', 'demands', 'of', 'the', 'record', 'company', ',', '\"', 'Somebody', \"'s\", 'Out', 'There', '\"', 'made', 'it', 'to', 'No.', '27', 'on', 'Billboard', 'Hot', '100', 'during', 'October', 'and', 'November', '1986', '.']\n",
      "['Written', 'and', 'recorded', 'in', 'the', '11th', 'hour', 'of', 'The', 'Sport', 'of', 'Kings', 'sessions', ',', 'in', 'an', 'attempt', 'to', 'deliver', 'a', 'hit', \"'\", 'single', \"'\", 'to', 'satisfy', 'the', 'demands', 'of', 'the', 'record', 'company', ',', '\"', 'Somebody', \"'s\", 'Out', 'There', '\"', 'made', 'it', 'to', 'No', '.', '27', 'on', 'Billboard', 'Hot', '100', 'during', 'October', 'and', 'November', '1986', '.']\n",
      "{'token_span': [18, 19], 'label': 'deliver%2:35:00::'}\n",
      "{'token_span': [25, 26], 'label': 'satisfy%2:42:00::'}\n",
      "{'token_span': [39, 40], 'label': 'make%2:38:00::'}\n",
      "\n",
      "Meanwhile , Triumph released 1989's Classics as their obligatory fifth album owed to MCA Records .\n",
      "['Meanwhile', ',', 'Triumph', 'released', \"1989's\", 'Classics', 'as', 'their', 'obligatory', 'fifth', 'album', 'owed', 'to', 'MCA', 'Records', '.']\n",
      "['Meanwhile', ',', 'Triumph', 'released', '1989', \"'s\", 'Classics', 'as', 'their', 'obligatory', 'fifth', 'album', 'owed', 'to', 'MCA', 'Records', '.']\n",
      "{'token_span': [11, 12], 'label': 'owe%2:40:01::'}\n",
      "\n",
      "With his wife Catherine , Charles had : Margaret Karlsdotter ( Bonde ) ( 1442 –1462 ) Magdalena of Sweden ( 1445 –1495 ) , married to noble Ivar Axelsson ( Tott ) in 1466 Richeza Karlsdotter ( Bonde ) ( born c. 1445 ) , nun at Vadstena Abbey Bridget Karlsdotter ( Bonde ) ( 1446–1469 ) , nun at Vadstena Abbey four sons died early With his mistress Christina Abrahamsdotter , Charles had : Anna Karlsdotter ( Bonde ) , married to the noble Håkan Svensson ( Bölja ) , governor of Västerås castle .\n",
      "['With', 'his', 'wife', 'Catherine', ',', 'Charles', 'had', ':', 'Margaret', 'Karlsdotter', '(', 'Bonde', ')', '(', '1442', '–1462', ')', 'Magdalena', 'of', 'Sweden', '(', '1445', '–1495', ')', ',', 'married', 'to', 'noble', 'Ivar', 'Axelsson', '(', 'Tott', ')', 'in', '1466', 'Richeza', 'Karlsdotter', '(', 'Bonde', ')', '(', 'born', 'c.', '1445', ')', ',', 'nun', 'at', 'Vadstena', 'Abbey', 'Bridget', 'Karlsdotter', '(', 'Bonde', ')', '(', '1446–1469', ')', ',', 'nun', 'at', 'Vadstena', 'Abbey', 'four', 'sons', 'died', 'early', 'With', 'his', 'mistress', 'Christina', 'Abrahamsdotter', ',', 'Charles', 'had', ':', 'Anna', 'Karlsdotter', '(', 'Bonde', ')', ',', 'married', 'to', 'the', 'noble', 'Håkan', 'Svensson', '(', 'Bölja', ')', ',', 'governor', 'of', 'Västerås', 'castle', '.']\n",
      "['With', 'his', 'wife', 'Catherine', ',', 'Charles', 'had', ':', 'Margaret', 'Karlsdotter', '(', 'Bonde', ')', '(', '1442', '–', '1462', ')', 'Magdalena', 'of', 'Sweden', '(', '1445', '–', '1495', ')', ',', 'married', 'to', 'noble', 'Ivar', 'Axelsson', '(', 'Tott', ')', 'in', '1466', 'Richeza', 'Karlsdotter', '(', 'Bonde', ')', '(', 'born', 'c.', '1445', ')', ',', 'nun', 'at', 'Vadstena', 'Abbey', 'Bridget', 'Karlsdotter', '(', 'Bonde', ')', '(', '1446–1469', ')', ',', 'nun', 'at', 'Vadstena', 'Abbey', 'four', 'sons', 'died', 'early', 'With', 'his', 'mistress', 'Christina', 'Abrahamsdotter', ',', 'Charles', 'had', ':', 'Anna', 'Karlsdotter', '(', 'Bonde', ')', ',', 'married', 'to', 'the', 'noble', 'Håkan', 'Svensson', '(', 'Bölja', ')', ',', 'governor', 'of', 'Västerås', 'castle', '.']\n",
      "{'token_span': [2, 3], 'label': 'wife%1:18:00::'}\n",
      "{'token_span': [25, 26], 'label': 'marry%2:41:00::'}\n",
      "{'token_span': [64, 65], 'label': 'son%1:18:00::'}\n",
      "{'token_span': [82, 83], 'label': 'marry%2:41:00::'}\n",
      "\n",
      "In some cases , the death cap has been introduced to new regions with the cultivation of non-native species of oak , chestnut , and pine .\n",
      "['In', 'some', 'cases', ',', 'the', 'death', 'cap', 'has', 'been', 'introduced', 'to', 'new', 'regions', 'with', 'the', 'cultivation', 'of', 'non-native', 'species', 'of', 'oak', ',', 'chestnut', ',', 'and', 'pine', '.']\n",
      "['In', 'some', 'cases', ',', 'the', 'death', 'cap', 'has', 'been', 'introduced', 'to', 'new', 'regions', 'with', 'the', 'cultivation', 'of', 'non', '-', 'native', 'species', 'of', 'oak', ',', 'chestnut', ',', 'and', 'pine', '.']\n",
      "{'token_span': [11, 12], 'label': 'new%3:00:00::'}\n",
      "{'token_span': [12, 13], 'label': 'region%1:15:01::'}\n",
      "{'token_span': [20, 21], 'label': 'oak%1:20:00::'}\n",
      "\n",
      "Vatican Officials Gregory Peck as Monsignor Hugh O'Flaherty John Gielgud as Pope Pius XII Raf Vallone as Father Vittorio Angelo Infanti as Father Morosini Marne Maitland as Papal Secretary Stelio Candelli as O'Flaherty 's Secretary Gabriella D'Olive as Mother Superior SS Personnel Christopher Plummer as SS - Obersturmbannführer Herbert Kappler Kenneth Colley as SS - Hauptsturmführer Hirsch ( representing Erich Priebke ) ( as Ken Colley ) Walter Gotell as SS - Obergruppenführer Max Helm ( representing Karl Wolff ) Michael Byrne as Reinhard Beck T. P. McKenna as Reichsführer Heinrich Himmler David Brandon as SS officer Allied Personnel John Terry as Lt. Jack Manning Phillip Hatton as Lt. Harry Barnett Mark Lewis as Cpl. Les Tate William Berger as U.S. Intelligence Officer ( as Bill Berger ) Edmund Purdom as British Intelligence Officer / Epilogue Narrator ( as Edmond Purdom ) Civilians Barbara Bouchet as Minna Kappler Julian Holloway as Alfred West ( John May ) Olga Karlatos as Francesca Lombardo ( representing Chetta Chevalier ) Vernon Dobtcheff as Count Langenthal ( representing Count Demetris Sarsfield Salazar ) Peter Burton as Sir D'Arcy Osborne Fabiana Udenio as Guila Lombardo Remo Remotti as Rabbi Leoni Giovanni Crippa as Simon Weiss Billy Boyle as Paddy Doyle Itaco Nardulli as Franz Kappler Cariddi Nardulli as Liesel Kappler ( as Carridi Nardulli ) Alessandra Cozzo as Emilia Lombardo Cesarina Tacconi as Pregnant Woman Sergio Nicolai as Firing Squad Officer Bruno Corazzari as Coalman Francesco Carnelutti as Cameriere Segreto Monsignor Hugh O'Flaherty was a real Irish - born priest and Vatican official , credited with saving 6,500 Jews and Allied war prisoners .\n",
      "['Vatican', 'Officials', 'Gregory', 'Peck', 'as', 'Monsignor', 'Hugh', \"O'Flaherty\", 'John', 'Gielgud', 'as', 'Pope', 'Pius', 'XII', 'Raf', 'Vallone', 'as', 'Father', 'Vittorio', 'Angelo', 'Infanti', 'as', 'Father', 'Morosini', 'Marne', 'Maitland', 'as', 'Papal', 'Secretary', 'Stelio', 'Candelli', 'as', \"O'Flaherty\", \"'s\", 'Secretary', 'Gabriella', \"D'Olive\", 'as', 'Mother', 'Superior', 'SS', 'Personnel', 'Christopher', 'Plummer', 'as', 'SS', '-', 'Obersturmbannführer', 'Herbert', 'Kappler', 'Kenneth', 'Colley', 'as', 'SS', '-', 'Hauptsturmführer', 'Hirsch', '(', 'representing', 'Erich', 'Priebke', ')', '(', 'as', 'Ken', 'Colley', ')', 'Walter', 'Gotell', 'as', 'SS', '-', 'Obergruppenführer', 'Max', 'Helm', '(', 'representing', 'Karl', 'Wolff', ')', 'Michael', 'Byrne', 'as', 'Reinhard', 'Beck', 'T.', 'P.', 'McKenna', 'as', 'Reichsführer', 'Heinrich', 'Himmler', 'David', 'Brandon', 'as', 'SS', 'officer', 'Allied', 'Personnel', 'John', 'Terry', 'as', 'Lt.', 'Jack', 'Manning', 'Phillip', 'Hatton', 'as', 'Lt.', 'Harry', 'Barnett', 'Mark', 'Lewis', 'as', 'Cpl.', 'Les', 'Tate', 'William', 'Berger', 'as', 'U.S.', 'Intelligence', 'Officer', '(', 'as', 'Bill', 'Berger', ')', 'Edmund', 'Purdom', 'as', 'British', 'Intelligence', 'Officer', '/', 'Epilogue', 'Narrator', '(', 'as', 'Edmond', 'Purdom', ')', 'Civilians', 'Barbara', 'Bouchet', 'as', 'Minna', 'Kappler', 'Julian', 'Holloway', 'as', 'Alfred', 'West', '(', 'John', 'May', ')', 'Olga', 'Karlatos', 'as', 'Francesca', 'Lombardo', '(', 'representing', 'Chetta', 'Chevalier', ')', 'Vernon', 'Dobtcheff', 'as', 'Count', 'Langenthal', '(', 'representing', 'Count', 'Demetris', 'Sarsfield', 'Salazar', ')', 'Peter', 'Burton', 'as', 'Sir', \"D'Arcy\", 'Osborne', 'Fabiana', 'Udenio', 'as', 'Guila', 'Lombardo', 'Remo', 'Remotti', 'as', 'Rabbi', 'Leoni', 'Giovanni', 'Crippa', 'as', 'Simon', 'Weiss', 'Billy', 'Boyle', 'as', 'Paddy', 'Doyle', 'Itaco', 'Nardulli', 'as', 'Franz', 'Kappler', 'Cariddi', 'Nardulli', 'as', 'Liesel', 'Kappler', '(', 'as', 'Carridi', 'Nardulli', ')', 'Alessandra', 'Cozzo', 'as', 'Emilia', 'Lombardo', 'Cesarina', 'Tacconi', 'as', 'Pregnant', 'Woman', 'Sergio', 'Nicolai', 'as', 'Firing', 'Squad', 'Officer', 'Bruno', 'Corazzari', 'as', 'Coalman', 'Francesco', 'Carnelutti', 'as', 'Cameriere', 'Segreto', 'Monsignor', 'Hugh', \"O'Flaherty\", 'was', 'a', 'real', 'Irish', '-', 'born', 'priest', 'and', 'Vatican', 'official', ',', 'credited', 'with', 'saving', '6,500', 'Jews', 'and', 'Allied', 'war', 'prisoners', '.']\n",
      "['Vatican', 'Officials', 'Gregory', 'Peck', 'as', 'Monsignor', 'Hugh', \"O'Flaherty\", 'John', 'Gielgud', 'as', 'Pope', 'Pius', 'XII', 'Raf', 'Vallone', 'as', 'Father', 'Vittorio', 'Angelo', 'Infanti', 'as', 'Father', 'Morosini', 'Marne', 'Maitland', 'as', 'Papal', 'Secretary', 'Stelio', 'Candelli', 'as', \"O'Flaherty\", \"'s\", 'Secretary', 'Gabriella', \"D'Olive\", 'as', 'Mother', 'Superior', 'SS', 'Personnel', 'Christopher', 'Plummer', 'as', 'SS', '-', 'Obersturmbannführer', 'Herbert', 'Kappler', 'Kenneth', 'Colley', 'as', 'SS', '-', 'Hauptsturmführer', 'Hirsch', '(', 'representing', 'Erich', 'Priebke', ')', '(', 'as', 'Ken', 'Colley', ')', 'Walter', 'Gotell', 'as', 'SS', '-', 'Obergruppenführer', 'Max', 'Helm', '(', 'representing', 'Karl', 'Wolff', ')', 'Michael', 'Byrne', 'as', 'Reinhard', 'Beck', 'T.', 'P.', 'McKenna', 'as', 'Reichsführer', 'Heinrich', 'Himmler', 'David', 'Brandon', 'as', 'SS', 'officer', 'Allied', 'Personnel', 'John', 'Terry', 'as', 'Lt', '.', 'Jack', 'Manning', 'Phillip', 'Hatton', 'as', 'Lt', '.', 'Harry', 'Barnett', 'Mark', 'Lewis', 'as', 'Cpl', '.', 'Les', 'Tate', 'William', 'Berger', 'as', 'U.S.', 'Intelligence', 'Officer', '(', 'as', 'Bill', 'Berger', ')', 'Edmund', 'Purdom', 'as', 'British', 'Intelligence', 'Officer', '/', 'Epilogue', 'Narrator', '(', 'as', 'Edmond', 'Purdom', ')', 'Civilians', 'Barbara', 'Bouchet', 'as', 'Minna', 'Kappler', 'Julian', 'Holloway', 'as', 'Alfred', 'West', '(', 'John', 'May', ')', 'Olga', 'Karlatos', 'as', 'Francesca', 'Lombardo', '(', 'representing', 'Chetta', 'Chevalier', ')', 'Vernon', 'Dobtcheff', 'as', 'Count', 'Langenthal', '(', 'representing', 'Count', 'Demetris', 'Sarsfield', 'Salazar', ')', 'Peter', 'Burton', 'as', 'Sir', \"D'Arcy\", 'Osborne', 'Fabiana', 'Udenio', 'as', 'Guila', 'Lombardo', 'Remo', 'Remotti', 'as', 'Rabbi', 'Leoni', 'Giovanni', 'Crippa', 'as', 'Simon', 'Weiss', 'Billy', 'Boyle', 'as', 'Paddy', 'Doyle', 'Itaco', 'Nardulli', 'as', 'Franz', 'Kappler', 'Cariddi', 'Nardulli', 'as', 'Liesel', 'Kappler', '(', 'as', 'Carridi', 'Nardulli', ')', 'Alessandra', 'Cozzo', 'as', 'Emilia', 'Lombardo', 'Cesarina', 'Tacconi', 'as', 'Pregnant', 'Woman', 'Sergio', 'Nicolai', 'as', 'Firing', 'Squad', 'Officer', 'Bruno', 'Corazzari', 'as', 'Coalman', 'Francesco', 'Carnelutti', 'as', 'Cameriere', 'Segreto', 'Monsignor', 'Hugh', \"O'Flaherty\", 'was', 'a', 'real', 'Irish', '-', 'born', 'priest', 'and', 'Vatican', 'official', ',', 'credited', 'with', 'saving', '6,500', 'Jews', 'and', 'Allied', 'war', 'prisoners', '.']\n",
      "{'token_span': [17, 18], 'label': 'father%1:18:05::'}\n",
      "{'token_span': [22, 23], 'label': 'father%1:18:05::'}\n",
      "{'token_span': [149, 150], 'label': 'west%1:15:00::'}\n",
      "{'token_span': [190, 191], 'label': 'rabbi%1:18:00::'}\n",
      "{'token_span': [250, 251], 'label': 'priest%1:18:00::'}\n",
      "{'token_span': [257, 258], 'label': 'save%2:41:01::'}\n",
      "\n",
      "The trans isomer holds proton { H} far from the cone of the benzene ring thus the magnetic anisotropy is not present .\n",
      "['The', 'trans', 'isomer', 'holds', 'proton', '{', 'H}', 'far', 'from', 'the', 'cone', 'of', 'the', 'benzene', 'ring', 'thus', 'the', 'magnetic', 'anisotropy', 'is', 'not', 'present', '.']\n",
      "['The', 'trans', 'isomer', 'holds', 'proton', '{', 'H', '}', 'far', 'from', 'the', 'cone', 'of', 'the', 'benzene', 'ring', 'thus', 'the', 'magnetic', 'anisotropy', 'is', 'not', 'present', '.']\n",
      "{'token_span': [10, 11], 'label': 'cone%1:25:00::'}\n",
      "\n",
      "It 's difficult to compare if you 've not played in each one but there 's something special about any derby at any level .\n",
      "['It', \"'s\", 'difficult', 'to', 'compare', 'if', 'you', \"'ve\", 'not', 'played', 'in', 'each', 'one', 'but', 'there', \"'s\", 'something', 'special', 'about', 'any', 'derby', 'at', 'any', 'level', '.']\n",
      "['It', \"'s\", 'difficult', 'to', 'compare', 'if', 'you', \"'\", 've', 'not', 'played', 'in', 'each', 'one', 'but', 'there', \"'s\", 'something', 'special', 'about', 'any', 'derby', 'at', 'any', 'level', '.']\n",
      "{'token_span': [2, 3], 'label': 'difficult%3:00:00::'}\n",
      "{'token_span': [4, 5], 'label': 'compare%2:31:00::'}\n",
      "\n",
      "The neck of the instrument is long and narrow , long enough to support a 62 – 70 cm-long string ( minus the 25 cm where the string passes over the bowl after leaving the neck ) .\n",
      "['The', 'neck', 'of', 'the', 'instrument', 'is', 'long', 'and', 'narrow', ',', 'long', 'enough', 'to', 'support', 'a', '62', '–', '70', 'cm-long', 'string', '(', 'minus', 'the', '25', 'cm', 'where', 'the', 'string', 'passes', 'over', 'the', 'bowl', 'after', 'leaving', 'the', 'neck', ')', '.']\n",
      "['The', 'neck', 'of', 'the', 'instrument', 'is', 'long', 'and', 'narrow', ',', 'long', 'enough', 'to', 'support', 'a', '62', '–', '70', 'cm', '-', 'long', 'string', '(', 'minus', 'the', '25', 'cm', 'where', 'the', 'string', 'passes', 'over', 'the', 'bowl', 'after', 'leaving', 'the', 'neck', ')', '.']\n",
      "{'token_span': [1, 2], 'label': 'neck%1:06:01::'}\n",
      "{'token_span': [4, 5], 'label': 'instrument%1:06:01::'}\n",
      "{'token_span': [6, 7], 'label': 'long%3:00:01::'}\n",
      "{'token_span': [8, 9], 'label': 'narrow%3:00:00::'}\n",
      "{'token_span': [10, 11], 'label': 'long%4:02:03::'}\n",
      "{'token_span': [31, 32], 'label': 'bowl%1:25:00::'}\n",
      "{'token_span': [33, 34], 'label': 'leave%2:38:00::'}\n",
      "{'token_span': [35, 36], 'label': 'neck%1:06:01::'}\n",
      "\n",
      "M-TEC lost the supply contract for the 2006 season , with the rules changing to allow Toyota associate TOM'S to join Mugen as engine supplier .\n",
      "['M-TEC', 'lost', 'the', 'supply', 'contract', 'for', 'the', '2006', 'season', ',', 'with', 'the', 'rules', 'changing', 'to', 'allow', 'Toyota', 'associate', \"TOM'S\", 'to', 'join', 'Mugen', 'as', 'engine', 'supplier', '.']\n",
      "['M', '-', 'TEC', 'lost', 'the', 'supply', 'contract', 'for', 'the', '2006', 'season', ',', 'with', 'the', 'rules', 'changing', 'to', 'allow', 'Toyota', 'associate', 'TOM', \"'S\", 'to', 'join', 'Mugen', 'as', 'engine', 'supplier', '.']\n",
      "{'token_span': [1, 2], 'label': 'lose%2:40:02::'}\n",
      "{'token_span': [8, 9], 'label': 'season%1:28:02::'}\n",
      "{'token_span': [15, 16], 'label': 'allow%2:32:00::'}\n",
      "{'token_span': [20, 21], 'label': 'join%2:41:00::'}\n",
      "{'token_span': [23, 24], 'label': 'engine%1:06:00::'}\n",
      "\n",
      "Computer : I 'M NOT SURE WHAT YOU MEAN BY \" ON TOP OF \" IN THE PHRASE \" ON TOP OF GREEN CUBES \" .\n",
      "['Computer', ':', 'I', \"'M\", 'NOT', 'SURE', 'WHAT', 'YOU', 'MEAN', 'BY', '\"', 'ON', 'TOP', 'OF', '\"', 'IN', 'THE', 'PHRASE', '\"', 'ON', 'TOP', 'OF', 'GREEN', 'CUBES', '\"', '.']\n",
      "['Computer', ':', 'I', \"'\", 'M', 'NOT', 'SURE', 'WHAT', 'YOU', 'MEAN', 'BY', '\"', 'ON', 'TOP', 'OF', '\"', 'IN', 'THE', 'PHRASE', '\"', 'ON', 'TOP', 'OF', 'GREEN', 'CUBES', '\"', '.']\n",
      "{'token_span': [5, 6], 'label': 'sure%3:00:00::'}\n",
      "{'token_span': [12, 13], 'label': 'top%1:15:00::'}\n",
      "{'token_span': [17, 18], 'label': 'phrase%1:10:00::'}\n",
      "{'token_span': [20, 21], 'label': 'top%1:15:00::'}\n",
      "\n",
      "Engine Name : MF408S Engine : 90° V8 , naturally aspirated Displacement : 4,000 cm³ Bore x Stroke : 97 mm x 67 mm Max Power : Over 456 kW ( 612 hp ; 620 PS ) @ 9,500 rpm Max Torque : Over 520 N⋅m ( 380 lb⋅ft ) @ 7,500 rpm Restrictor Size : 33.4 mm x2 or 46.8 mm x1 Ignition Type : Direct Injection ECU System : EFI Technology Inc CDI System : EFI Technology Inc Clutch Type / Size Carbon / 5.5 inch 4 - plate Maintenance Interval : >3,000 km ( > 5,000 km at Le Mans 24h ) Length : 559 mm ( not including flywheel )\n",
      "['Engine', 'Name', ':', 'MF408S', 'Engine', ':', '90°', 'V8', ',', 'naturally', 'aspirated', 'Displacement', ':', '4,000', 'cm³', 'Bore', 'x', 'Stroke', ':', '97', 'mm', 'x', '67', 'mm', 'Max', 'Power', ':', 'Over', '456', 'kW', '(', '612', 'hp', ';', '620', 'PS', ')', '@', '9,500', 'rpm', 'Max', 'Torque', ':', 'Over', '520', 'N⋅m', '(', '380', 'lb⋅ft', ')', '@', '7,500', 'rpm', 'Restrictor', 'Size', ':', '33.4', 'mm', 'x2', 'or', '46.8', 'mm', 'x1', 'Ignition', 'Type', ':', 'Direct', 'Injection', 'ECU', 'System', ':', 'EFI', 'Technology', 'Inc', 'CDI', 'System', ':', 'EFI', 'Technology', 'Inc', 'Clutch', 'Type', '/', 'Size', 'Carbon', '/', '5.5', 'inch', '4', '-', 'plate', 'Maintenance', 'Interval', ':', '>3,000', 'km', '(', '>', '5,000', 'km', 'at', 'Le', 'Mans', '24h', ')', 'Length', ':', '559', 'mm', '(', 'not', 'including', 'flywheel', ')']\n",
      "['Engine', 'Name', ':', 'MF408S', 'Engine', ':', '90', '°', 'V8', ',', 'naturally', 'aspirated', 'Displacement', ':', '4,000', 'cm³', 'Bore', 'x', 'Stroke', ':', '97', 'mm', 'x', '67', 'mm', 'Max', 'Power', ':', 'Over', '456', 'kW', '(', '612', 'hp', ';', '620', 'PS', ')', '@', '9,500', 'rpm', 'Max', 'Torque', ':', 'Over', '520', 'N⋅m', '(', '380', 'lb⋅ft', ')', '@', '7,500', 'rpm', 'Restrictor', 'Size', ':', '33.4', 'mm', 'x2', 'or', '46.8', 'mm', 'x1', 'Ignition', 'Type', ':', 'Direct', 'Injection', 'ECU', 'System', ':', 'EFI', 'Technology', 'Inc', 'CDI', 'System', ':', 'EFI', 'Technology', 'Inc', 'Clutch', 'Type', '/', 'Size', 'Carbon', '/', '5.5', 'inch', '4', '-', 'plate', 'Maintenance', 'Interval', ':', '>', '3,000', 'km', '(', '>', '5,000', 'km', 'at', 'Le', 'Mans', '24h', ')', 'Length', ':', '559', 'mm', '(', 'not', 'including', 'flywheel', ')']\n",
      "{'token_span': [0, 1], 'label': 'engine%1:06:00::'}\n",
      "{'token_span': [4, 5], 'label': 'engine%1:06:00::'}\n",
      "{'token_span': [54, 55], 'label': 'size%1:07:00::'}\n",
      "{'token_span': [83, 84], 'label': 'size%1:07:00::'}\n",
      "{'token_span': [84, 85], 'label': 'carbon%1:27:00::'}\n",
      "{'token_span': [87, 88], 'label': 'inch%1:23:00::'}\n",
      "{'token_span': [90, 91], 'label': 'plate%1:06:02::'}\n",
      "{'token_span': [102, 103], 'label': 'man%1:18:00::'}\n",
      "\n",
      "He was also an \" ardent devotee \" of the Bengali Hindu saint , Sri Anandamayi Ma. Shankar used to visit Anandamayi Ma frequently and performed for her on various occasions .\n",
      "['He', 'was', 'also', 'an', '\"', 'ardent', 'devotee', '\"', 'of', 'the', 'Bengali', 'Hindu', 'saint', ',', 'Sri', 'Anandamayi', 'Ma.', 'Shankar', 'used', 'to', 'visit', 'Anandamayi', 'Ma', 'frequently', 'and', 'performed', 'for', 'her', 'on', 'various', 'occasions', '.']\n",
      "['He', 'was', 'also', 'an', '\"', 'ardent', 'devotee', '\"', 'of', 'the', 'Bengali', 'Hindu', 'saint', ',', 'Sri', 'Anandamayi', 'Ma', '.', 'Shankar', 'used', 'to', 'visit', 'Anandamayi', 'Ma', 'frequently', 'and', 'performed', 'for', 'her', 'on', 'various', 'occasions', '.']\n",
      "{'token_span': [12, 13], 'label': 'saint%1:18:03::'}\n",
      "\n",
      "January 4 – The tallest man-made structure to date , the Burj Khalifa in Dubai , United Arab Emirates , is officially opened .\n",
      "['January', '4', '–', 'The', 'tallest', 'man-made', 'structure', 'to', 'date', ',', 'the', 'Burj', 'Khalifa', 'in', 'Dubai', ',', 'United', 'Arab', 'Emirates', ',', 'is', 'officially', 'opened', '.']\n",
      "['January', '4', '–', 'The', 'tallest', 'man', '-', 'made', 'structure', 'to', 'date', ',', 'the', 'Burj', 'Khalifa', 'in', 'Dubai', ',', 'United', 'Arab', 'Emirates', ',', 'is', 'officially', 'opened', '.']\n",
      "{'token_span': [8, 9], 'label': 'date%1:28:02::'}\n",
      "\n",
      "And you turn it over and you put the gun down , but I 'm not pretending what it 's taken to get me here .\n",
      "['And', 'you', 'turn', 'it', 'over', 'and', 'you', 'put', 'the', 'gun', 'down', ',', 'but', 'I', \"'m\", 'not', 'pretending', 'what', 'it', \"'s\", 'taken', 'to', 'get', 'me', 'here', '.']\n",
      "['And', 'you', 'turn', 'it', 'over', 'and', 'you', 'put', 'the', 'gun', 'down', ',', 'but', 'I', \"'\", 'm', 'not', 'pretending', 'what', 'it', \"'s\", 'taken', 'to', 'get', 'me', 'here', '.']\n",
      "{'token_span': [7, 8], 'label': 'put%2:35:00::'}\n",
      "{'token_span': [9, 10], 'label': 'gun%1:06:00::'}\n",
      "{'token_span': [16, 17], 'label': 'pretend%2:32:00::'}\n",
      "{'token_span': [20, 21], 'label': 'take%2:42:00::'}\n",
      "{'token_span': [22, 23], 'label': 'get%2:35:03::'}\n",
      "\n",
      "The second of Malraux 's Asian novels was the semi-autobiographical La Voie Royale which relates the adventures of a Frenchman Claude Vannec who together with his Danish friend Perken head down the royal road of the title into the jungle of Cambodia with the intention of stealing bas - relief sculptures from the ruins of Hindu temples .\n",
      "['The', 'second', 'of', 'Malraux', \"'s\", 'Asian', 'novels', 'was', 'the', 'semi-autobiographical', 'La', 'Voie', 'Royale', 'which', 'relates', 'the', 'adventures', 'of', 'a', 'Frenchman', 'Claude', 'Vannec', 'who', 'together', 'with', 'his', 'Danish', 'friend', 'Perken', 'head', 'down', 'the', 'royal', 'road', 'of', 'the', 'title', 'into', 'the', 'jungle', 'of', 'Cambodia', 'with', 'the', 'intention', 'of', 'stealing', 'bas', '-', 'relief', 'sculptures', 'from', 'the', 'ruins', 'of', 'Hindu', 'temples', '.']\n",
      "['The', 'second', 'of', 'Malraux', \"'s\", 'Asian', 'novels', 'was', 'the', 'semi', '-', 'autobiographical', 'La', 'Voie', 'Royale', 'which', 'relates', 'the', 'adventures', 'of', 'a', 'Frenchman', 'Claude', 'Vannec', 'who', 'together', 'with', 'his', 'Danish', 'friend', 'Perken', 'head', 'down', 'the', 'royal', 'road', 'of', 'the', 'title', 'into', 'the', 'jungle', 'of', 'Cambodia', 'with', 'the', 'intention', 'of', 'stealing', 'bas', '-', 'relief', 'sculptures', 'from', 'the', 'ruins', 'of', 'Hindu', 'temples', '.']\n",
      "{'token_span': [1, 2], 'label': 'second%1:24:00::'}\n",
      "{'token_span': [27, 28], 'label': 'friend%1:18:00::'}\n",
      "{'token_span': [29, 30], 'label': 'head%2:38:00::'}\n",
      "{'token_span': [46, 47], 'label': 'steal%2:40:00::'}\n",
      "\n",
      "Other notable stories to appear in Superboy include the story of the first Bizarro and the first appearances of Legion of Super - Heroes members Mon-El and Ultra Boy .\n",
      "['Other', 'notable', 'stories', 'to', 'appear', 'in', 'Superboy', 'include', 'the', 'story', 'of', 'the', 'first', 'Bizarro', 'and', 'the', 'first', 'appearances', 'of', 'Legion', 'of', 'Super', '-', 'Heroes', 'members', 'Mon-El', 'and', 'Ultra', 'Boy', '.']\n",
      "['Other', 'notable', 'stories', 'to', 'appear', 'in', 'Superboy', 'include', 'the', 'story', 'of', 'the', 'first', 'Bizarro', 'and', 'the', 'first', 'appearances', 'of', 'Legion', 'of', 'Super', '-', 'Heroes', 'members', 'Mon', '-', 'El', 'and', 'Ultra', 'Boy', '.']\n",
      "{'token_span': [2, 3], 'label': 'story%1:10:00::'}\n",
      "{'token_span': [4, 5], 'label': 'appear%2:30:01::'}\n",
      "{'token_span': [9, 10], 'label': 'story%1:10:00::'}\n",
      "\n",
      "She had four children during this first marriage : George St John , 3rd Viscount Bolingbroke ( 5 March 1761 – 11 December 1824 ) Henriette St John ( 1 Aug 1762 - April 1834 ) - married in 1792 Henry Towcester Anne ( born ca. 1764 , and did not survive infancy ) Frederick St John ( British Army officer ) ( 20 December 1765 — 19 November 1844 ) Finding herself in a desperately unhappy marriage to the notoriously unfaithful Viscount Bolingbroke , Lady Di overturned convention .\n",
      "['She', 'had', 'four', 'children', 'during', 'this', 'first', 'marriage', ':', 'George', 'St', 'John', ',', '3rd', 'Viscount', 'Bolingbroke', '(', '5', 'March', '1761', '–', '11', 'December', '1824', ')', 'Henriette', 'St', 'John', '(', '1', 'Aug', '1762', '-', 'April', '1834', ')', '-', 'married', 'in', '1792', 'Henry', 'Towcester', 'Anne', '(', 'born', 'ca.', '1764', ',', 'and', 'did', 'not', 'survive', 'infancy', ')', 'Frederick', 'St', 'John', '(', 'British', 'Army', 'officer', ')', '(', '20', 'December', '1765', '—', '19', 'November', '1844', ')', 'Finding', 'herself', 'in', 'a', 'desperately', 'unhappy', 'marriage', 'to', 'the', 'notoriously', 'unfaithful', 'Viscount', 'Bolingbroke', ',', 'Lady', 'Di', 'overturned', 'convention', '.']\n",
      "['She', 'had', 'four', 'children', 'during', 'this', 'first', 'marriage', ':', 'George', 'St', 'John', ',', '3rd', 'Viscount', 'Bolingbroke', '(', '5', 'March', '1761', '–', '11', 'December', '1824', ')', 'Henriette', 'St', 'John', '(', '1', 'Aug', '1762', '-', 'April', '1834', ')', '-', 'married', 'in', '1792', 'Henry', 'Towcester', 'Anne', '(', 'born', 'ca', '.', '1764', ',', 'and', 'did', 'not', 'survive', 'infancy', ')', 'Frederick', 'St', 'John', '(', 'British', 'Army', 'officer', ')', '(', '20', 'December', '1765', '—', '19', 'November', '1844', ')', 'Finding', 'herself', 'in', 'a', 'desperately', 'unhappy', 'marriage', 'to', 'the', 'notoriously', 'unfaithful', 'Viscount', 'Bolingbroke', ',', 'Lady', 'Di', 'overturned', 'convention', '.']\n",
      "{'token_span': [3, 4], 'label': 'child%1:18:01::'}\n",
      "{'token_span': [37, 38], 'label': 'marry%2:41:00::'}\n",
      "{'token_span': [70, 71], 'label': 'find%2:39:01::'}\n",
      "{'token_span': [75, 76], 'label': 'unhappy%3:00:00::'}\n",
      "{'token_span': [84, 85], 'label': 'lady%1:18:00::'}\n",
      "\n",
      "The following are listed buildings or sites in Rhineland - Palatinate ’s Directory of Cultural Monuments : Am Kissel 3 – so - called “ Kloster ” ( “ Monastery ” ) ; house , window walling possibly from about 1600 , roof frame 1725 , commercial building less old Mühlenweg 1 – so - called Fettigsmühle ; Baroque Quereinhaus ( a combination residential and commercial house divided for these two purposes down the middle , perpendicularly to the street ) , about 1800 ; technical equipment At Hauptstraße 7 – elaborate portal , marked 1767 At Hochwaldstraße 1 – door lintel with inscription , 18th century Hochwaldstraße 8 – former Schneidersfranzenmühle ; former gristmill with three - floor mill tower , essentially possibly from the 18th century ( timber framing dendrochronologically dated to 1725 ) Running east of the municipality is Bundesstraße 41 , which towards the south leads to the Autobahn A 62 ( Kaiserslautern– Trier ) .\n",
      "['The', 'following', 'are', 'listed', 'buildings', 'or', 'sites', 'in', 'Rhineland', '-', 'Palatinate', '’s', 'Directory', 'of', 'Cultural', 'Monuments', ':', 'Am', 'Kissel', '3', '–', 'so', '-', 'called', '“', 'Kloster', '”', '(', '“', 'Monastery', '”', ')', ';', 'house', ',', 'window', 'walling', 'possibly', 'from', 'about', '1600', ',', 'roof', 'frame', '1725', ',', 'commercial', 'building', 'less', 'old', 'Mühlenweg', '1', '–', 'so', '-', 'called', 'Fettigsmühle', ';', 'Baroque', 'Quereinhaus', '(', 'a', 'combination', 'residential', 'and', 'commercial', 'house', 'divided', 'for', 'these', 'two', 'purposes', 'down', 'the', 'middle', ',', 'perpendicularly', 'to', 'the', 'street', ')', ',', 'about', '1800', ';', 'technical', 'equipment', 'At', 'Hauptstraße', '7', '–', 'elaborate', 'portal', ',', 'marked', '1767', 'At', 'Hochwaldstraße', '1', '–', 'door', 'lintel', 'with', 'inscription', ',', '18th', 'century', 'Hochwaldstraße', '8', '–', 'former', 'Schneidersfranzenmühle', ';', 'former', 'gristmill', 'with', 'three', '-', 'floor', 'mill', 'tower', ',', 'essentially', 'possibly', 'from', 'the', '18th', 'century', '(', 'timber', 'framing', 'dendrochronologically', 'dated', 'to', '1725', ')', 'Running', 'east', 'of', 'the', 'municipality', 'is', 'Bundesstraße', '41', ',', 'which', 'towards', 'the', 'south', 'leads', 'to', 'the', 'Autobahn', 'A', '62', '(', 'Kaiserslautern–', 'Trier', ')', '.']\n",
      "['The', 'following', 'are', 'listed', 'buildings', 'or', 'sites', 'in', 'Rhineland', '-', 'Palatinate', '’s', 'Directory', 'of', 'Cultural', 'Monuments', ':', 'Am', 'Kissel', '3', '–', 'so', '-', 'called', '“', 'Kloster', '”', '(', '“', 'Monastery', '”', ')', ';', 'house', ',', 'window', 'walling', 'possibly', 'from', 'about', '1600', ',', 'roof', 'frame', '1725', ',', 'commercial', 'building', 'less', 'old', 'Mühlenweg', '1', '–', 'so', '-', 'called', 'Fettigsmühle', ';', 'Baroque', 'Quereinhaus', '(', 'a', 'combination', 'residential', 'and', 'commercial', 'house', 'divided', 'for', 'these', 'two', 'purposes', 'down', 'the', 'middle', ',', 'perpendicularly', 'to', 'the', 'street', ')', ',', 'about', '1800', ';', 'technical', 'equipment', 'At', 'Hauptstraße', '7', '–', 'elaborate', 'portal', ',', 'marked', '1767', 'At', 'Hochwaldstraße', '1', '–', 'door', 'lintel', 'with', 'inscription', ',', '18th', 'century', 'Hochwaldstraße', '8', '–', 'former', 'Schneidersfranzenmühle', ';', 'former', 'gristmill', 'with', 'three', '-', 'floor', 'mill', 'tower', ',', 'essentially', 'possibly', 'from', 'the', '18th', 'century', '(', 'timber', 'framing', 'dendrochronologically', 'dated', 'to', '1725', ')', 'Running', 'east', 'of', 'the', 'municipality', 'is', 'Bundesstraße', '41', ',', 'which', 'towards', 'the', 'south', 'leads', 'to', 'the', 'Autobahn', 'A', '62', '(', 'Kaiserslautern', '–', 'Trier', ')', '.']\n",
      "{'token_span': [33, 34], 'label': 'house%1:06:00::'}\n",
      "{'token_span': [35, 36], 'label': 'window%1:06:00::'}\n",
      "{'token_span': [42, 43], 'label': 'roof%1:06:00::'}\n",
      "{'token_span': [49, 50], 'label': 'old%3:00:01::'}\n",
      "{'token_span': [66, 67], 'label': 'house%1:06:00::'}\n",
      "{'token_span': [67, 68], 'label': 'divide%2:41:00::'}\n",
      "{'token_span': [78, 79], 'label': 'street%1:06:00::'}\n",
      "{'token_span': [99, 100], 'label': 'door%1:06:00::'}\n",
      "{'token_span': [105, 106], 'label': 'century%1:28:00::'}\n",
      "{'token_span': [117, 118], 'label': 'floor%1:06:01::'}\n",
      "{'token_span': [119, 120], 'label': 'tower%1:06:00::'}\n",
      "{'token_span': [126, 127], 'label': 'century%1:28:00::'}\n",
      "{'token_span': [128, 129], 'label': 'timber%1:06:00::'}\n",
      "{'token_span': [131, 132], 'label': 'date%2:31:00::'}\n",
      "{'token_span': [147, 148], 'label': 'south%1:24:00::'}\n",
      "\n",
      "Though intended by Harvest to be a weapon against superpowered humans , Kon-El pursues a heroic career as a solo hero and a member of the Teen Titans .\n",
      "['Though', 'intended', 'by', 'Harvest', 'to', 'be', 'a', 'weapon', 'against', 'superpowered', 'humans', ',', 'Kon-El', 'pursues', 'a', 'heroic', 'career', 'as', 'a', 'solo', 'hero', 'and', 'a', 'member', 'of', 'the', 'Teen', 'Titans', '.']\n",
      "['Though', 'intended', 'by', 'Harvest', 'to', 'be', 'a', 'weapon', 'against', 'superpowered', 'humans', ',', 'Kon', '-', 'El', 'pursues', 'a', 'heroic', 'career', 'as', 'a', 'solo', 'hero', 'and', 'a', 'member', 'of', 'the', 'Teen', 'Titans', '.']\n",
      "{'token_span': [13, 14], 'label': 'pursue%2:41:00::'}\n",
      "{'token_span': [20, 21], 'label': 'hero%1:09:00::'}\n",
      "\n",
      "A contemporary review published by Variety called the film \" man-vs - beast nonsense \" , and lamented that \" fine special effects and underwater camera work are plowed under in dumb story - telling . \"\n",
      "['A', 'contemporary', 'review', 'published', 'by', 'Variety', 'called', 'the', 'film', '\"', 'man-vs', '-', 'beast', 'nonsense', '\"', ',', 'and', 'lamented', 'that', '\"', 'fine', 'special', 'effects', 'and', 'underwater', 'camera', 'work', 'are', 'plowed', 'under', 'in', 'dumb', 'story', '-', 'telling', '.', '\"']\n",
      "['A', 'contemporary', 'review', 'published', 'by', 'Variety', 'called', 'the', 'film', '\"', 'man', '-', 'vs', '-', 'beast', 'nonsense', '\"', ',', 'and', 'lamented', 'that', '\"', 'fine', 'special', 'effects', 'and', 'underwater', 'camera', 'work', 'are', 'plowed', 'under', 'in', 'dumb', 'story', '-', 'telling', '.', '\"']\n",
      "{'token_span': [8, 9], 'label': 'film%1:10:01::'}\n",
      "{'token_span': [12, 13], 'label': 'beast%1:03:00::'}\n",
      "{'token_span': [25, 26], 'label': 'camera%1:06:00::'}\n",
      "{'token_span': [31, 32], 'label': 'dumb%5:00:00:stupid:00'}\n",
      "{'token_span': [32, 33], 'label': 'story%1:10:03::'}\n",
      "\n",
      "On June 28 , 2008 , DC Comics Vice President and Executive Editor Dan DiDio said in reference to the Legion of Three Worlds comic at the Wizard World Chicago convention , \" We 've got Geoff ( Johns ) ( writer ) , we 've got George ( Pérez ) ( artist ) , we 've got SuperBOY Prime ( yes , we can say that again ) . \"\n",
      "['On', 'June', '28', ',', '2008', ',', 'DC', 'Comics', 'Vice', 'President', 'and', 'Executive', 'Editor', 'Dan', 'DiDio', 'said', 'in', 'reference', 'to', 'the', 'Legion', 'of', 'Three', 'Worlds', 'comic', 'at', 'the', 'Wizard', 'World', 'Chicago', 'convention', ',', '\"', 'We', \"'ve\", 'got', 'Geoff', '(', 'Johns', ')', '(', 'writer', ')', ',', 'we', \"'ve\", 'got', 'George', '(', 'Pérez', ')', '(', 'artist', ')', ',', 'we', \"'ve\", 'got', 'SuperBOY', 'Prime', '(', 'yes', ',', 'we', 'can', 'say', 'that', 'again', ')', '.', '\"']\n",
      "['On', 'June', '28', ',', '2008', ',', 'DC', 'Comics', 'Vice', 'President', 'and', 'Executive', 'Editor', 'Dan', 'DiDio', 'said', 'in', 'reference', 'to', 'the', 'Legion', 'of', 'Three', 'Worlds', 'comic', 'at', 'the', 'Wizard', 'World', 'Chicago', 'convention', ',', '\"', 'We', \"'\", 've', 'got', 'Geoff', '(', 'Johns', ')', '(', 'writer', ')', ',', 'we', \"'\", 've', 'got', 'George', '(', 'Pérez', ')', '(', 'artist', ')', ',', 'we', \"'\", 've', 'got', 'SuperBOY', 'Prime', '(', 'yes', ',', 'we', 'can', 'say', 'that', 'again', ')', '.', '\"']\n",
      "{'token_span': [8, 9], 'label': 'president%1:18:01::'}\n",
      "{'token_span': [14, 15], 'label': 'say%2:32:00::'}\n",
      "{'token_span': [27, 28], 'label': 'world%1:17:00::'}\n",
      "{'token_span': [34, 35], 'label': 'get%2:40:00::'}\n",
      "{'token_span': [40, 41], 'label': 'writer%1:18:00::'}\n",
      "{'token_span': [45, 46], 'label': 'get%2:40:00::'}\n",
      "{'token_span': [56, 57], 'label': 'get%2:40:00::'}\n",
      "{'token_span': [64, 65], 'label': 'say%2:32:00::'}\n",
      "\n",
      "According to the record in which Offenbach had its first documentary mention , Archbishop Heinrich of Mainz ( 1142-1153 ) acknowledged that the edelfrei nobleman Reinfried had donated a plot of land to Saint Vincent 's Benedictine Abbey in Metz .\n",
      "['According', 'to', 'the', 'record', 'in', 'which', 'Offenbach', 'had', 'its', 'first', 'documentary', 'mention', ',', 'Archbishop', 'Heinrich', 'of', 'Mainz', '(', '1142-1153', ')', 'acknowledged', 'that', 'the', 'edelfrei', 'nobleman', 'Reinfried', 'had', 'donated', 'a', 'plot', 'of', 'land', 'to', 'Saint', 'Vincent', \"'s\", 'Benedictine', 'Abbey', 'in', 'Metz', '.']\n",
      "['According', 'to', 'the', 'record', 'in', 'which', 'Offenbach', 'had', 'its', 'first', 'documentary', 'mention', ',', 'Archbishop', 'Heinrich', 'of', 'Mainz', '(', '1142', '-', '1153', ')', 'acknowledged', 'that', 'the', 'edelfrei', 'nobleman', 'Reinfried', 'had', 'donated', 'a', 'plot', 'of', 'land', 'to', 'Saint', 'Vincent', \"'s\", 'Benedictine', 'Abbey', 'in', 'Metz', '.']\n",
      "{'token_span': [13, 14], 'label': 'archbishop%1:18:00::'}\n",
      "{'token_span': [20, 21], 'label': 'acknowledge%2:32:00::'}\n",
      "\n",
      "With ' IT ' you win all men if you are a woman–and all women if you are a man .\n",
      "['With', \"'\", 'IT', \"'\", 'you', 'win', 'all', 'men', 'if', 'you', 'are', 'a', 'woman–and', 'all', 'women', 'if', 'you', 'are', 'a', 'man', '.']\n",
      "['With', \"'\", 'IT', \"'\", 'you', 'win', 'all', 'men', 'if', 'you', 'are', 'a', 'woman', '–', 'and', 'all', 'women', 'if', 'you', 'are', 'a', 'man', '.']\n",
      "{'token_span': [5, 6], 'label': 'win%2:40:00::'}\n",
      "{'token_span': [7, 8], 'label': 'man%1:18:00::'}\n",
      "{'token_span': [14, 15], 'label': 'woman%1:18:00::'}\n",
      "{'token_span': [19, 20], 'label': 'man%1:18:00::'}\n",
      "\n",
      "However , she is only credited as an author , adapter , and co-producer on the project .\n",
      "['However', ',', 'she', 'is', 'only', 'credited', 'as', 'an', 'author', ',', 'adapter', ',', 'and', 'co-producer', 'on', 'the', 'project', '.']\n",
      "['However', ',', 'she', 'is', 'only', 'credited', 'as', 'an', 'author', ',', 'adapter', ',', 'and', 'co', '-', 'producer', 'on', 'the', 'project', '.']\n",
      "{'token_span': [8, 9], 'label': 'author%1:18:00::'}\n",
      "\n",
      "Roosevelt proposed that : \" [t ] he number of Jews engaged in the practice of the professions ( law , medicine , etc. ) should be definitely limited to the percentage that the Jewish population in North Africa bears to the whole of the North African population .... [ T ] his plan would further eliminate the specific and understandable complaints which the Germans bore towards the Jews in Germany , namely , that while they represented a small part of the population , over 50 percent of the lawyers , doctors , schoolteachers , college professors , etc. , in Germany were Jews . \"\n",
      "['Roosevelt', 'proposed', 'that', ':', '\"', '[t', ']', 'he', 'number', 'of', 'Jews', 'engaged', 'in', 'the', 'practice', 'of', 'the', 'professions', '(', 'law', ',', 'medicine', ',', 'etc.', ')', 'should', 'be', 'definitely', 'limited', 'to', 'the', 'percentage', 'that', 'the', 'Jewish', 'population', 'in', 'North', 'Africa', 'bears', 'to', 'the', 'whole', 'of', 'the', 'North', 'African', 'population', '....', '[', 'T', ']', 'his', 'plan', 'would', 'further', 'eliminate', 'the', 'specific', 'and', 'understandable', 'complaints', 'which', 'the', 'Germans', 'bore', 'towards', 'the', 'Jews', 'in', 'Germany', ',', 'namely', ',', 'that', 'while', 'they', 'represented', 'a', 'small', 'part', 'of', 'the', 'population', ',', 'over', '50', 'percent', 'of', 'the', 'lawyers', ',', 'doctors', ',', 'schoolteachers', ',', 'college', 'professors', ',', 'etc.', ',', 'in', 'Germany', 'were', 'Jews', '.', '\"']\n",
      "['Roosevelt', 'proposed', 'that', ':', '\"', '[', 't', ']', 'he', 'number', 'of', 'Jews', 'engaged', 'in', 'the', 'practice', 'of', 'the', 'professions', '(', 'law', ',', 'medicine', ',', 'etc', '.', ')', 'should', 'be', 'definitely', 'limited', 'to', 'the', 'percentage', 'that', 'the', 'Jewish', 'population', 'in', 'North', 'Africa', 'bears', 'to', 'the', 'whole', 'of', 'the', 'North', 'African', 'population', '....', '[', 'T', ']', 'his', 'plan', 'would', 'further', 'eliminate', 'the', 'specific', 'and', 'understandable', 'complaints', 'which', 'the', 'Germans', 'bore', 'towards', 'the', 'Jews', 'in', 'Germany', ',', 'namely', ',', 'that', 'while', 'they', 'represented', 'a', 'small', 'part', 'of', 'the', 'population', ',', 'over', '50', 'percent', 'of', 'the', 'lawyers', ',', 'doctors', ',', 'schoolteachers', ',', 'college', 'professors', ',', 'etc', '.', ',', 'in', 'Germany', 'were', 'Jews', '.', '\"']\n",
      "{'token_span': [11, 12], 'label': 'engage%2:41:06::'}\n",
      "{'token_span': [17, 18], 'label': 'profession%1:04:00::'}\n",
      "{'token_span': [19, 20], 'label': 'law%1:04:00::'}\n",
      "{'token_span': [35, 36], 'label': 'population%1:14:00::'}\n",
      "{'token_span': [45, 46], 'label': 'north%1:15:02::'}\n",
      "{'token_span': [47, 48], 'label': 'population%1:14:00::'}\n",
      "{'token_span': [83, 84], 'label': 'population%1:14:00::'}\n",
      "{'token_span': [90, 91], 'label': 'lawyer%1:18:00::'}\n",
      "{'token_span': [92, 93], 'label': 'doctor%1:18:00::'}\n",
      "{'token_span': [96, 97], 'label': 'college%1:14:00::'}\n",
      "{'token_span': [97, 98], 'label': 'professor%1:18:00::'}\n",
      "\n",
      "Lukather describes 1997's Luke as a much different and more \" introspective \" album than his previous two solo efforts .\n",
      "['Lukather', 'describes', \"1997's\", 'Luke', 'as', 'a', 'much', 'different', 'and', 'more', '\"', 'introspective', '\"', 'album', 'than', 'his', 'previous', 'two', 'solo', 'efforts', '.']\n",
      "['Lukather', 'describes', '1997', \"'s\", 'Luke', 'as', 'a', 'much', 'different', 'and', 'more', '\"', 'introspective', '\"', 'album', 'than', 'his', 'previous', 'two', 'solo', 'efforts', '.']\n",
      "{'token_span': [7, 8], 'label': 'different%3:00:00::'}\n",
      "{'token_span': [19, 20], 'label': 'effort%1:04:00::'}\n",
      "\n",
      "As important aspects the group brought to the genre , Echard cites the Beatles ' rhythmic originality and unpredictability ; \" true \" tonal ambiguity ; leadership in incorporating elements from Indian music and studio techniques such as vari-speed , tape loops and reverse tape sounds ; and their embrace of the avant - garde .\n",
      "['As', 'important', 'aspects', 'the', 'group', 'brought', 'to', 'the', 'genre', ',', 'Echard', 'cites', 'the', 'Beatles', \"'\", 'rhythmic', 'originality', 'and', 'unpredictability', ';', '\"', 'true', '\"', 'tonal', 'ambiguity', ';', 'leadership', 'in', 'incorporating', 'elements', 'from', 'Indian', 'music', 'and', 'studio', 'techniques', 'such', 'as', 'vari-speed', ',', 'tape', 'loops', 'and', 'reverse', 'tape', 'sounds', ';', 'and', 'their', 'embrace', 'of', 'the', 'avant', '-', 'garde', '.']\n",
      "['As', 'important', 'aspects', 'the', 'group', 'brought', 'to', 'the', 'genre', ',', 'Echard', 'cites', 'the', 'Beatles', \"'\", 'rhythmic', 'originality', 'and', 'unpredictability', ';', '\"', 'true', '\"', 'tonal', 'ambiguity', ';', 'leadership', 'in', 'incorporating', 'elements', 'from', 'Indian', 'music', 'and', 'studio', 'techniques', 'such', 'as', 'vari', '-', 'speed', ',', 'tape', 'loops', 'and', 'reverse', 'tape', 'sounds', ';', 'and', 'their', 'embrace', 'of', 'the', 'avant', '-', 'garde', '.']\n",
      "{'token_span': [1, 2], 'label': 'important%3:00:00::'}\n",
      "{'token_span': [5, 6], 'label': 'bring%2:38:00::'}\n",
      "{'token_span': [31, 32], 'label': 'music%1:10:00::'}\n",
      "{'token_span': [40, 41], 'label': 'loop%1:22:00::'}\n",
      "\n",
      "( 1990 , No. 1 in Norway ) ; Chasing Shadows ( 1992 ) ; and Celebration ( 1994 ) , which contained old hits in new arrangements accompanied by an orchestra .\n",
      "['(', '1990', ',', 'No.', '1', 'in', 'Norway', ')', ';', 'Chasing', 'Shadows', '(', '1992', ')', ';', 'and', 'Celebration', '(', '1994', ')', ',', 'which', 'contained', 'old', 'hits', 'in', 'new', 'arrangements', 'accompanied', 'by', 'an', 'orchestra', '.']\n",
      "['(', '1990', ',', 'No', '.', '1', 'in', 'Norway', ')', ';', 'Chasing', 'Shadows', '(', '1992', ')', ';', 'and', 'Celebration', '(', '1994', ')', ',', 'which', 'contained', 'old', 'hits', 'in', 'new', 'arrangements', 'accompanied', 'by', 'an', 'orchestra', '.']\n",
      "{'token_span': [16, 17], 'label': 'celebration%1:11:00::'}\n",
      "{'token_span': [23, 24], 'label': 'old%3:00:01::'}\n",
      "{'token_span': [26, 27], 'label': 'new%3:00:00::'}\n",
      "{'token_span': [31, 32], 'label': 'orchestra%1:14:00::'}\n",
      "\n",
      "After the cancellation of Bronson , Parks did n't work in a major Hollywood production for several years , but he had regular small roles in independent or Canadian features throughout the 1970s , such as Between Friends ( 1973 ) , although director Donald Shebib had trouble dealing with Parks , describing him as a \" terrific actor in a lot of ways , but weird \" while also accusing him of anti-Semitism .\n",
      "['After', 'the', 'cancellation', 'of', 'Bronson', ',', 'Parks', 'did', \"n't\", 'work', 'in', 'a', 'major', 'Hollywood', 'production', 'for', 'several', 'years', ',', 'but', 'he', 'had', 'regular', 'small', 'roles', 'in', 'independent', 'or', 'Canadian', 'features', 'throughout', 'the', '1970s', ',', 'such', 'as', 'Between', 'Friends', '(', '1973', ')', ',', 'although', 'director', 'Donald', 'Shebib', 'had', 'trouble', 'dealing', 'with', 'Parks', ',', 'describing', 'him', 'as', 'a', '\"', 'terrific', 'actor', 'in', 'a', 'lot', 'of', 'ways', ',', 'but', 'weird', '\"', 'while', 'also', 'accusing', 'him', 'of', 'anti-Semitism', '.']\n",
      "['After', 'the', 'cancellation', 'of', 'Bronson', ',', 'Parks', 'did', \"n't\", 'work', 'in', 'a', 'major', 'Hollywood', 'production', 'for', 'several', 'years', ',', 'but', 'he', 'had', 'regular', 'small', 'roles', 'in', 'independent', 'or', 'Canadian', 'features', 'throughout', 'the', '1970s', ',', 'such', 'as', 'Between', 'Friends', '(', '1973', ')', ',', 'although', 'director', 'Donald', 'Shebib', 'had', 'trouble', 'dealing', 'with', 'Parks', ',', 'describing', 'him', 'as', 'a', '\"', 'terrific', 'actor', 'in', 'a', 'lot', 'of', 'ways', ',', 'but', 'weird', '\"', 'while', 'also', 'accusing', 'him', 'of', 'anti', '-', 'Semitism', '.']\n",
      "{'token_span': [17, 18], 'label': 'year%1:28:01::'}\n",
      "{'token_span': [37, 38], 'label': 'friend%1:18:00::'}\n",
      "{'token_span': [57, 58], 'label': 'terrific%5:00:00:extraordinary:00'}\n",
      "{'token_span': [58, 59], 'label': 'actor%1:18:00::'}\n",
      "{'token_span': [63, 64], 'label': 'way%1:07:01::'}\n",
      "{'token_span': [66, 67], 'label': 'weird%5:00:00:strange:00'}\n",
      "\n",
      "Branching off to the south , in the more southerly centre of Hundheim , is Landesstraße 273 ( Rothselberg–Offenbach - Hundheim ) , linking both centres with the Lauter valley and running onwards to the uplands to Altenglan , while Hundheim also serves as a gateway to the villages in the Glan valley .\n",
      "['Branching', 'off', 'to', 'the', 'south', ',', 'in', 'the', 'more', 'southerly', 'centre', 'of', 'Hundheim', ',', 'is', 'Landesstraße', '273', '(', 'Rothselberg–Offenbach', '-', 'Hundheim', ')', ',', 'linking', 'both', 'centres', 'with', 'the', 'Lauter', 'valley', 'and', 'running', 'onwards', 'to', 'the', 'uplands', 'to', 'Altenglan', ',', 'while', 'Hundheim', 'also', 'serves', 'as', 'a', 'gateway', 'to', 'the', 'villages', 'in', 'the', 'Glan', 'valley', '.']\n",
      "['Branching', 'off', 'to', 'the', 'south', ',', 'in', 'the', 'more', 'southerly', 'centre', 'of', 'Hundheim', ',', 'is', 'Landesstraße', '273', '(', 'Rothselberg', '–', 'Offenbach', '-', 'Hundheim', ')', ',', 'linking', 'both', 'centres', 'with', 'the', 'Lauter', 'valley', 'and', 'running', 'onwards', 'to', 'the', 'uplands', 'to', 'Altenglan', ',', 'while', 'Hundheim', 'also', 'serves', 'as', 'a', 'gateway', 'to', 'the', 'villages', 'in', 'the', 'Glan', 'valley', '.']\n",
      "{'token_span': [4, 5], 'label': 'south%1:24:00::'}\n",
      "{'token_span': [29, 30], 'label': 'valley%1:17:00::'}\n",
      "{'token_span': [52, 53], 'label': 'valley%1:17:00::'}\n",
      "\n",
      "When the smoker is finished , s/he either places the hose back on the table , signifying that it is available , or hands it from one user to the next , folded back on itself so that the mouthpiece is not pointing at the recipient .\n",
      "['When', 'the', 'smoker', 'is', 'finished', ',', 's/he', 'either', 'places', 'the', 'hose', 'back', 'on', 'the', 'table', ',', 'signifying', 'that', 'it', 'is', 'available', ',', 'or', 'hands', 'it', 'from', 'one', 'user', 'to', 'the', 'next', ',', 'folded', 'back', 'on', 'itself', 'so', 'that', 'the', 'mouthpiece', 'is', 'not', 'pointing', 'at', 'the', 'recipient', '.']\n",
      "['When', 'the', 'smoker', 'is', 'finished', ',', 's', '/', 'he', 'either', 'places', 'the', 'hose', 'back', 'on', 'the', 'table', ',', 'signifying', 'that', 'it', 'is', 'available', ',', 'or', 'hands', 'it', 'from', 'one', 'user', 'to', 'the', 'next', ',', 'folded', 'back', 'on', 'itself', 'so', 'that', 'the', 'mouthpiece', 'is', 'not', 'pointing', 'at', 'the', 'recipient', '.']\n",
      "{'token_span': [10, 11], 'label': 'hose%1:06:01::'}\n",
      "{'token_span': [23, 24], 'label': 'hand%2:40:00::'}\n",
      "\n",
      "Judaic interpreters as early as Philo and Yochanan ben Zakai ( 1st century AD ) interpreted \" a mighty hunter before the Lord \" ( Heb. : גבר ציד לפני יהוה , ḡibbōr - ṣayiḏ lip̄n ê\n",
      "['Judaic', 'interpreters', 'as', 'early', 'as', 'Philo', 'and', 'Yochanan', 'ben', 'Zakai', '(', '1st', 'century', 'AD', ')', 'interpreted', '\"', 'a', 'mighty', 'hunter', 'before', 'the', 'Lord', '\"', '(', 'Heb.', ':', 'גבר', 'ציד', 'לפני', 'יהוה', ',', 'ḡibbōr', '-', 'ṣayiḏ', 'lip̄n', 'ê']\n",
      "['Judaic', 'interpreters', 'as', 'early', 'as', 'Philo', 'and', 'Yochanan', 'ben', 'Zakai', '(', '1st', 'century', 'AD', ')', 'interpreted', '\"', 'a', 'mighty', 'hunter', 'before', 'the', 'Lord', '\"', '(', 'Heb', '.', ':', 'גבר', 'ציד', 'לפני', 'יהוה', ',', 'ḡibbōr', '-', 'ṣayiḏ', 'lip̄n', 'ê']\n",
      "{'token_span': [12, 13], 'label': 'century%1:28:00::'}\n",
      "\n",
      "In the following campaign he continued in that country and tier , joining Granada CF also in a temporary deal and being a solid contributor as the Andalusians returned to La Liga after a 35 - year absence , netting seven times – plus once in the play - offs– in 2,673 minutes of play ( he was also sent off three times ) .\n",
      "['In', 'the', 'following', 'campaign', 'he', 'continued', 'in', 'that', 'country', 'and', 'tier', ',', 'joining', 'Granada', 'CF', 'also', 'in', 'a', 'temporary', 'deal', 'and', 'being', 'a', 'solid', 'contributor', 'as', 'the', 'Andalusians', 'returned', 'to', 'La', 'Liga', 'after', 'a', '35', '-', 'year', 'absence', ',', 'netting', 'seven', 'times', '–', 'plus', 'once', 'in', 'the', 'play', '-', 'offs–', 'in', '2,673', 'minutes', 'of', 'play', '(', 'he', 'was', 'also', 'sent', 'off', 'three', 'times', ')', '.']\n",
      "['In', 'the', 'following', 'campaign', 'he', 'continued', 'in', 'that', 'country', 'and', 'tier', ',', 'joining', 'Granada', 'CF', 'also', 'in', 'a', 'temporary', 'deal', 'and', 'being', 'a', 'solid', 'contributor', 'as', 'the', 'Andalusians', 'returned', 'to', 'La', 'Liga', 'after', 'a', '35', '-', 'year', 'absence', ',', 'netting', 'seven', 'times', '–', 'plus', 'once', 'in', 'the', 'play', '-', 'offs', '–', 'in', '2,673', 'minutes', 'of', 'play', '(', 'he', 'was', 'also', 'sent', 'off', 'three', 'times', ')', '.']\n",
      "{'token_span': [12, 13], 'label': 'join%2:41:00::'}\n",
      "{'token_span': [36, 37], 'label': 'year%1:28:01::'}\n",
      "{'token_span': [37, 38], 'label': 'absence%1:28:00::'}\n",
      "{'token_span': [52, 53], 'label': 'minute%1:28:00::'}\n",
      "\n",
      "Houseman 's first assignment for the producer would be writing the script to Jane Eyre. : 475 , 478 Houseman later said \" I foolishly believed I would then produce the film because Selznick had just made Gone with the Wind and Rebecca and I believed he was going to take a rest from producing for a while and let me do the producing for his studio .\n",
      "['Houseman', \"'s\", 'first', 'assignment', 'for', 'the', 'producer', 'would', 'be', 'writing', 'the', 'script', 'to', 'Jane', 'Eyre.', ':', '475', ',', '478', 'Houseman', 'later', 'said', '\"', 'I', 'foolishly', 'believed', 'I', 'would', 'then', 'produce', 'the', 'film', 'because', 'Selznick', 'had', 'just', 'made', 'Gone', 'with', 'the', 'Wind', 'and', 'Rebecca', 'and', 'I', 'believed', 'he', 'was', 'going', 'to', 'take', 'a', 'rest', 'from', 'producing', 'for', 'a', 'while', 'and', 'let', 'me', 'do', 'the', 'producing', 'for', 'his', 'studio', '.']\n",
      "['Houseman', \"'s\", 'first', 'assignment', 'for', 'the', 'producer', 'would', 'be', 'writing', 'the', 'script', 'to', 'Jane', 'Eyre', '.', ':', '475', ',', '478', 'Houseman', 'later', 'said', '\"', 'I', 'foolishly', 'believed', 'I', 'would', 'then', 'produce', 'the', 'film', 'because', 'Selznick', 'had', 'just', 'made', 'Gone', 'with', 'the', 'Wind', 'and', 'Rebecca', 'and', 'I', 'believed', 'he', 'was', 'going', 'to', 'take', 'a', 'rest', 'from', 'producing', 'for', 'a', 'while', 'and', 'let', 'me', 'do', 'the', 'producing', 'for', 'his', 'studio', '.']\n",
      "{'token_span': [3, 4], 'label': 'assignment%1:04:03::'}\n",
      "{'token_span': [21, 22], 'label': 'say%2:32:00::'}\n",
      "{'token_span': [25, 26], 'label': 'believe%2:31:00::'}\n",
      "{'token_span': [31, 32], 'label': 'film%1:10:01::'}\n",
      "{'token_span': [36, 37], 'label': 'make%2:36:09::'}\n",
      "{'token_span': [37, 38], 'label': 'go%2:41:00::'}\n",
      "{'token_span': [45, 46], 'label': 'believe%2:31:03::'}\n",
      "{'token_span': [48, 49], 'label': 'go%2:41:00::'}\n",
      "{'token_span': [50, 51], 'label': 'take%2:40:06::'}\n",
      "{'token_span': [61, 62], 'label': 'do%2:41:01::'}\n",
      "\n",
      "Until then the champions of central - southern Italy had always suffered defeats by the great clubs of the north , but after the honorable defeat of the first leg by 3 –1 , in the return leg the Savoia entered the history of Italian football by drawing for 1 - 1 with the eight - time champions of Italy , becoming the first club in central - southern Italy to end a race .\n",
      "['Until', 'then', 'the', 'champions', 'of', 'central', '-', 'southern', 'Italy', 'had', 'always', 'suffered', 'defeats', 'by', 'the', 'great', 'clubs', 'of', 'the', 'north', ',', 'but', 'after', 'the', 'honorable', 'defeat', 'of', 'the', 'first', 'leg', 'by', '3', '–1', ',', 'in', 'the', 'return', 'leg', 'the', 'Savoia', 'entered', 'the', 'history', 'of', 'Italian', 'football', 'by', 'drawing', 'for', '1', '-', '1', 'with', 'the', 'eight', '-', 'time', 'champions', 'of', 'Italy', ',', 'becoming', 'the', 'first', 'club', 'in', 'central', '-', 'southern', 'Italy', 'to', 'end', 'a', 'race', '.']\n",
      "['Until', 'then', 'the', 'champions', 'of', 'central', '-', 'southern', 'Italy', 'had', 'always', 'suffered', 'defeats', 'by', 'the', 'great', 'clubs', 'of', 'the', 'north', ',', 'but', 'after', 'the', 'honorable', 'defeat', 'of', 'the', 'first', 'leg', 'by', '3', '–', '1', ',', 'in', 'the', 'return', 'leg', 'the', 'Savoia', 'entered', 'the', 'history', 'of', 'Italian', 'football', 'by', 'drawing', 'for', '1', '-', '1', 'with', 'the', 'eight', '-', 'time', 'champions', 'of', 'Italy', ',', 'becoming', 'the', 'first', 'club', 'in', 'central', '-', 'southern', 'Italy', 'to', 'end', 'a', 'race', '.']\n",
      "{'token_span': [3, 4], 'label': 'champion%1:18:01::'}\n",
      "{'token_span': [15, 16], 'label': 'great%5:00:01:important:00'}\n",
      "{'token_span': [16, 17], 'label': 'club%1:14:01::'}\n",
      "{'token_span': [19, 20], 'label': 'north%1:15:02::'}\n",
      "{'token_span': [29, 30], 'label': 'leg%1:04:00::'}\n",
      "{'token_span': [37, 38], 'label': 'leg%1:04:00::'}\n",
      "{'token_span': [40, 41], 'label': 'enter%2:42:00::'}\n",
      "{'token_span': [45, 46], 'label': 'football%1:04:00::'}\n",
      "{'token_span': [57, 58], 'label': 'champion%1:18:01::'}\n",
      "{'token_span': [61, 62], 'label': 'become%2:30:00::'}\n",
      "{'token_span': [64, 65], 'label': 'club%1:14:01::'}\n",
      "\n",
      "His parents , Haïm Aaron Prosper Charles ( Aimé ) Derrida ( 1896–1970 ) and Georgette Sultana Esther Safar ( 1901 –1991 ) , named him \" Jackie \" , \" which they considered to be an American name \" , although he would later adopt a more \" correct \" version of his first name when he moved to Paris ; some reports indicate that he was named Jackie after the American child actor Jackie Coogan , who had become well - known around the world via his role in the 1921 Charlie Chaplin film The Kid .\n",
      "['His', 'parents', ',', 'Haïm', 'Aaron', 'Prosper', 'Charles', '(', 'Aimé', ')', 'Derrida', '(', '1896–1970', ')', 'and', 'Georgette', 'Sultana', 'Esther', 'Safar', '(', '1901', '–1991', ')', ',', 'named', 'him', '\"', 'Jackie', '\"', ',', '\"', 'which', 'they', 'considered', 'to', 'be', 'an', 'American', 'name', '\"', ',', 'although', 'he', 'would', 'later', 'adopt', 'a', 'more', '\"', 'correct', '\"', 'version', 'of', 'his', 'first', 'name', 'when', 'he', 'moved', 'to', 'Paris', ';', 'some', 'reports', 'indicate', 'that', 'he', 'was', 'named', 'Jackie', 'after', 'the', 'American', 'child', 'actor', 'Jackie', 'Coogan', ',', 'who', 'had', 'become', 'well', '-', 'known', 'around', 'the', 'world', 'via', 'his', 'role', 'in', 'the', '1921', 'Charlie', 'Chaplin', 'film', 'The', 'Kid', '.']\n",
      "['His', 'parents', ',', 'Haïm', 'Aaron', 'Prosper', 'Charles', '(', 'Aimé', ')', 'Derrida', '(', '1896–1970', ')', 'and', 'Georgette', 'Sultana', 'Esther', 'Safar', '(', '1901', '–', '1991', ')', ',', 'named', 'him', '\"', 'Jackie', '\"', ',', '\"', 'which', 'they', 'considered', 'to', 'be', 'an', 'American', 'name', '\"', ',', 'although', 'he', 'would', 'later', 'adopt', 'a', 'more', '\"', 'correct', '\"', 'version', 'of', 'his', 'first', 'name', 'when', 'he', 'moved', 'to', 'Paris', ';', 'some', 'reports', 'indicate', 'that', 'he', 'was', 'named', 'Jackie', 'after', 'the', 'American', 'child', 'actor', 'Jackie', 'Coogan', ',', 'who', 'had', 'become', 'well', '-', 'known', 'around', 'the', 'world', 'via', 'his', 'role', 'in', 'the', '1921', 'Charlie', 'Chaplin', 'film', 'The', 'Kid', '.']\n",
      "{'token_span': [1, 2], 'label': 'parent%1:18:00::'}\n",
      "{'token_span': [73, 74], 'label': 'child%1:18:00::'}\n",
      "{'token_span': [74, 75], 'label': 'actor%1:18:00::'}\n",
      "{'token_span': [80, 81], 'label': 'become%2:30:00::'}\n",
      "{'token_span': [83, 84], 'label': 'know%2:31:00::'}\n",
      "{'token_span': [86, 87], 'label': 'world%1:17:00::'}\n",
      "{'token_span': [95, 96], 'label': 'film%1:10:01::'}\n",
      "{'token_span': [97, 98], 'label': 'kid%1:18:00::'}\n",
      "\n",
      "I cannot imagine another actor . \"\n",
      "['I', 'cannot', 'imagine', 'another', 'actor', '.', '\"']\n",
      "['I', 'can', 'not', 'imagine', 'another', 'actor', '.', '\"']\n",
      "{'token_span': [2, 3], 'label': 'imagine%2:36:00::'}\n",
      "{'token_span': [4, 5], 'label': 'actor%1:18:00::'}\n",
      "\n",
      "The plants have many , 2 –3 mm ( 0.079 –0.118 in ) thick strongly branching roots .\n",
      "['The', 'plants', 'have', 'many', ',', '2', '–3', 'mm', '(', '0.079', '–0.118', 'in', ')', 'thick', 'strongly', 'branching', 'roots', '.']\n",
      "['The', 'plants', 'have', 'many', ',', '2', '–', '3', 'mm', '(', '0.079', '–', '0.118', 'in', ')', 'thick', 'strongly', 'branching', 'roots', '.']\n",
      "{'token_span': [16, 17], 'label': 'root%1:20:00::'}\n",
      "\n",
      "The alternate true leaves are in a rosette , each of which consist of a leaf stem that is about 4 × as long as the kidney - shaped leaf blade , itself between 3 – 25 cm ( 1.2 –9.8 in ) long and 3 – 20 cm ( 1.2 –7.9 in ) wide , with a heart - shaped foot , a blunt tip , and a scalloped to toothed , sometime almost entire margin particularly towards the tip .\n",
      "['The', 'alternate', 'true', 'leaves', 'are', 'in', 'a', 'rosette', ',', 'each', 'of', 'which', 'consist', 'of', 'a', 'leaf', 'stem', 'that', 'is', 'about', '4', '×', 'as', 'long', 'as', 'the', 'kidney', '-', 'shaped', 'leaf', 'blade', ',', 'itself', 'between', '3', '–', '25', 'cm', '(', '1.2', '–9.8', 'in', ')', 'long', 'and', '3', '–', '20', 'cm', '(', '1.2', '–7.9', 'in', ')', 'wide', ',', 'with', 'a', 'heart', '-', 'shaped', 'foot', ',', 'a', 'blunt', 'tip', ',', 'and', 'a', 'scalloped', 'to', 'toothed', ',', 'sometime', 'almost', 'entire', 'margin', 'particularly', 'towards', 'the', 'tip', '.']\n",
      "['The', 'alternate', 'true', 'leaves', 'are', 'in', 'a', 'rosette', ',', 'each', 'of', 'which', 'consist', 'of', 'a', 'leaf', 'stem', 'that', 'is', 'about', '4', '×', 'as', 'long', 'as', 'the', 'kidney', '-', 'shaped', 'leaf', 'blade', ',', 'itself', 'between', '3', '–', '25', 'cm', '(', '1.2', '–', '9.8', 'in', ')', 'long', 'and', '3', '–', '20', 'cm', '(', '1.2', '–', '7.9', 'in', ')', 'wide', ',', 'with', 'a', 'heart', '-', 'shaped', 'foot', ',', 'a', 'blunt', 'tip', ',', 'and', 'a', 'scalloped', 'to', 'toothed', ',', 'sometime', 'almost', 'entire', 'margin', 'particularly', 'towards', 'the', 'tip', '.']\n",
      "{'token_span': [3, 4], 'label': 'leave%1:10:00::'}\n",
      "{'token_span': [23, 24], 'label': 'long%4:02:03::'}\n",
      "{'token_span': [26, 27], 'label': 'kidney%1:08:00::'}\n",
      "{'token_span': [43, 44], 'label': 'long%4:02:03::'}\n",
      "{'token_span': [54, 55], 'label': 'wide%3:00:00::'}\n",
      "{'token_span': [58, 59], 'label': 'heart%1:25:00::'}\n",
      "{'token_span': [61, 62], 'label': 'foot%1:06:01::'}\n",
      "\n",
      "Insignia of the Knights Hospitaller Portrait of Archduke Wenceslaus of Austria wearing the habit of the Order of Malta ( Alonso Sánchez Coello 1577 ) Ornamental Maltese cross on the ceiling of St. John 's Co-Cathedral , Valletta , Malta ( Mattia Preti , 1660s ) .\n",
      "['Insignia', 'of', 'the', 'Knights', 'Hospitaller', 'Portrait', 'of', 'Archduke', 'Wenceslaus', 'of', 'Austria', 'wearing', 'the', 'habit', 'of', 'the', 'Order', 'of', 'Malta', '(', 'Alonso', 'Sánchez', 'Coello', '1577', ')', 'Ornamental', 'Maltese', 'cross', 'on', 'the', 'ceiling', 'of', 'St.', 'John', \"'s\", 'Co-Cathedral', ',', 'Valletta', ',', 'Malta', '(', 'Mattia', 'Preti', ',', '1660s', ')', '.']\n",
      "['Insignia', 'of', 'the', 'Knights', 'Hospitaller', 'Portrait', 'of', 'Archduke', 'Wenceslaus', 'of', 'Austria', 'wearing', 'the', 'habit', 'of', 'the', 'Order', 'of', 'Malta', '(', 'Alonso', 'Sánchez', 'Coello', '1577', ')', 'Ornamental', 'Maltese', 'cross', 'on', 'the', 'ceiling', 'of', 'St.', 'John', \"'s\", 'Co', '-', 'Cathedral', ',', 'Valletta', ',', 'Malta', '(', 'Mattia', 'Preti', ',', '1660s', ')', '.']\n",
      "{'token_span': [16, 17], 'label': 'order%1:14:03::'}\n",
      "{'token_span': [30, 31], 'label': 'ceiling%1:06:00::'}\n",
      "\n",
      "Because Franconian texts are almost non-existent and Old Dutch texts scarce and fragmentary , it is difficult to determine when such a transition occurred , but it is thought to have happened by the end of the 9th century and perhaps earlier .\n",
      "['Because', 'Franconian', 'texts', 'are', 'almost', 'non-existent', 'and', 'Old', 'Dutch', 'texts', 'scarce', 'and', 'fragmentary', ',', 'it', 'is', 'difficult', 'to', 'determine', 'when', 'such', 'a', 'transition', 'occurred', ',', 'but', 'it', 'is', 'thought', 'to', 'have', 'happened', 'by', 'the', 'end', 'of', 'the', '9th', 'century', 'and', 'perhaps', 'earlier', '.']\n",
      "['Because', 'Franconian', 'texts', 'are', 'almost', 'non', '-', 'existent', 'and', 'Old', 'Dutch', 'texts', 'scarce', 'and', 'fragmentary', ',', 'it', 'is', 'difficult', 'to', 'determine', 'when', 'such', 'a', 'transition', 'occurred', ',', 'but', 'it', 'is', 'thought', 'to', 'have', 'happened', 'by', 'the', 'end', 'of', 'the', '9th', 'century', 'and', 'perhaps', 'earlier', '.']\n",
      "{'token_span': [2, 3], 'label': 'text%1:10:00::'}\n",
      "{'token_span': [7, 8], 'label': 'old%5:00:03:past:00'}\n",
      "{'token_span': [9, 10], 'label': 'text%1:10:00::'}\n",
      "{'token_span': [10, 11], 'label': 'scarce%3:00:00::'}\n",
      "{'token_span': [16, 17], 'label': 'difficult%3:00:00::'}\n",
      "{'token_span': [18, 19], 'label': 'determine%2:32:00::'}\n",
      "{'token_span': [23, 24], 'label': 'occur%2:30:00::'}\n",
      "{'token_span': [28, 29], 'label': 'think%2:31:01::'}\n",
      "{'token_span': [31, 32], 'label': 'happen%2:30:00::'}\n",
      "{'token_span': [38, 39], 'label': 'century%1:28:00::'}\n",
      "\n",
      "The psychodynamic nature of the daughter –mother relationship in the Electra complex derives from penis envy , caused by the mother , who also caused the girl 's castration ; however , upon re-aligning her sexual attraction to her father ( heterosexuality ) , the girl represses the hostile female competition , for fear of losing the love of her mother .\n",
      "['The', 'psychodynamic', 'nature', 'of', 'the', 'daughter', '–mother', 'relationship', 'in', 'the', 'Electra', 'complex', 'derives', 'from', 'penis', 'envy', ',', 'caused', 'by', 'the', 'mother', ',', 'who', 'also', 'caused', 'the', 'girl', \"'s\", 'castration', ';', 'however', ',', 'upon', 're-aligning', 'her', 'sexual', 'attraction', 'to', 'her', 'father', '(', 'heterosexuality', ')', ',', 'the', 'girl', 'represses', 'the', 'hostile', 'female', 'competition', ',', 'for', 'fear', 'of', 'losing', 'the', 'love', 'of', 'her', 'mother', '.']\n",
      "['The', 'psychodynamic', 'nature', 'of', 'the', 'daughter', '–', 'mother', 'relationship', 'in', 'the', 'Electra', 'complex', 'derives', 'from', 'penis', 'envy', ',', 'caused', 'by', 'the', 'mother', ',', 'who', 'also', 'caused', 'the', 'girl', \"'s\", 'castration', ';', 'however', ',', 'upon', 're', '-', 'aligning', 'her', 'sexual', 'attraction', 'to', 'her', 'father', '(', 'heterosexuality', ')', ',', 'the', 'girl', 'represses', 'the', 'hostile', 'female', 'competition', ',', 'for', 'fear', 'of', 'losing', 'the', 'love', 'of', 'her', 'mother', '.']\n",
      "{'token_span': [5, 6], 'label': 'daughter%1:18:00::'}\n",
      "{'token_span': [20, 21], 'label': 'mother%1:18:00::'}\n",
      "{'token_span': [26, 27], 'label': 'girl%1:18:02::'}\n",
      "{'token_span': [39, 40], 'label': 'father%1:18:00::'}\n",
      "{'token_span': [45, 46], 'label': 'girl%1:18:02::'}\n",
      "{'token_span': [55, 56], 'label': 'lose%2:40:02::'}\n",
      "{'token_span': [60, 61], 'label': 'mother%1:18:00::'}\n",
      "\n",
      "Front vowels ( [ i , e , ɛ ] and , to a lesser extent [ ɨ , ɘ , ɜ , æ ] , etc. ) , can be secondarily qualified as close or open , as in the traditional conception , but this refers to jaw rather than tongue position .\n",
      "['Front', 'vowels', '(', '[', 'i', ',', 'e', ',', 'ɛ', ']', 'and', ',', 'to', 'a', 'lesser', 'extent', '[', 'ɨ', ',', 'ɘ', ',', 'ɜ', ',', 'æ', ']', ',', 'etc.', ')', ',', 'can', 'be', 'secondarily', 'qualified', 'as', 'close', 'or', 'open', ',', 'as', 'in', 'the', 'traditional', 'conception', ',', 'but', 'this', 'refers', 'to', 'jaw', 'rather', 'than', 'tongue', 'position', '.']\n",
      "['Front', 'vowels', '(', '[', 'i', ',', 'e', ',', 'ɛ', ']', 'and', ',', 'to', 'a', 'lesser', 'extent', '[', 'ɨ', ',', 'ɘ', ',', 'ɜ', ',', 'æ', ']', ',', 'etc', '.', ')', ',', 'can', 'be', 'secondarily', 'qualified', 'as', 'close', 'or', 'open', ',', 'as', 'in', 'the', 'traditional', 'conception', ',', 'but', 'this', 'refers', 'to', 'jaw', 'rather', 'than', 'tongue', 'position', '.']\n",
      "{'token_span': [48, 49], 'label': 'jaw%1:08:00::'}\n",
      "{'token_span': [51, 52], 'label': 'tongue%1:08:00::'}\n",
      "\n",
      "The Journal of Geophysical Research – Atmospheres subsequently published a study by Menne et al. which examined the record of stations picked out by Watts ' Surfacestations.org and found that , if anything , the poorly sited stations showed a slight cool bias rather than the warm bias which Watts had anticipated .\n",
      "['The', 'Journal', 'of', 'Geophysical', 'Research', '–', 'Atmospheres', 'subsequently', 'published', 'a', 'study', 'by', 'Menne', 'et', 'al.', 'which', 'examined', 'the', 'record', 'of', 'stations', 'picked', 'out', 'by', 'Watts', \"'\", 'Surfacestations.org', 'and', 'found', 'that', ',', 'if', 'anything', ',', 'the', 'poorly', 'sited', 'stations', 'showed', 'a', 'slight', 'cool', 'bias', 'rather', 'than', 'the', 'warm', 'bias', 'which', 'Watts', 'had', 'anticipated', '.']\n",
      "['The', 'Journal', 'of', 'Geophysical', 'Research', '–', 'Atmospheres', 'subsequently', 'published', 'a', 'study', 'by', 'Menne', 'et', 'al', '.', 'which', 'examined', 'the', 'record', 'of', 'stations', 'picked', 'out', 'by', 'Watts', \"'\", 'Surfacestations.org', 'and', 'found', 'that', ',', 'if', 'anything', ',', 'the', 'poorly', 'sited', 'stations', 'showed', 'a', 'slight', 'cool', 'bias', 'rather', 'than', 'the', 'warm', 'bias', 'which', 'Watts', 'had', 'anticipated', '.']\n",
      "{'token_span': [16, 17], 'label': 'examine%2:31:00::'}\n",
      "{'token_span': [28, 29], 'label': 'find%2:32:00::'}\n",
      "{'token_span': [42, 43], 'label': 'bias%1:09:00::'}\n",
      "{'token_span': [47, 48], 'label': 'bias%1:09:00::'}\n",
      "\n",
      "The Nintendo Switch , PlayStation 5 and Xbox Series X/S versions of the game will be released on 6 July 2021 ; the Nintendo Switch will be playable via cloud .\n",
      "['The', 'Nintendo', 'Switch', ',', 'PlayStation', '5', 'and', 'Xbox', 'Series', 'X/S', 'versions', 'of', 'the', 'game', 'will', 'be', 'released', 'on', '6', 'July', '2021', ';', 'the', 'Nintendo', 'Switch', 'will', 'be', 'playable', 'via', 'cloud', '.']\n",
      "['The', 'Nintendo', 'Switch', ',', 'PlayStation', '5', 'and', 'Xbox', 'Series', 'X', '/', 'S', 'versions', 'of', 'the', 'game', 'will', 'be', 'released', 'on', '6', 'July', '2021', ';', 'the', 'Nintendo', 'Switch', 'will', 'be', 'playable', 'via', 'cloud', '.']\n",
      "{'token_span': [13, 14], 'label': 'game%1:04:01::'}\n",
      "{'token_span': [29, 30], 'label': 'cloud%1:19:01::'}\n",
      "\n",
      "73/1000 sentences are tokenized differently with the standard tokenizer\n"
     ]
    }
   ],
   "source": [
    "def count_different_tokenization_indexes(lines, model, print_lines=False):\n",
    "\n",
    "    count = 0\n",
    "    for line in lines:\n",
    "\n",
    "        text = line['text']\n",
    "        annotations = line['annotations']\n",
    "\n",
    "        tokens = text.split()\n",
    "\n",
    "        doc = model(text)\n",
    "        tokens_doc = [token.text for token in doc]\n",
    "\n",
    "        if len(tokens) != len(tokens_doc):\n",
    "            count += 1\n",
    "\n",
    "            if print_lines:\n",
    "                print(text)\n",
    "                print(f'{tokens}')\n",
    "                print(f'{tokens_doc}')\n",
    "                for annotation in annotations:\n",
    "                    print(annotation)\n",
    "                print()\n",
    "\n",
    "    return count\n",
    "\n",
    "test_limit = 1000\n",
    "count = count_different_tokenization_indexes(lines[:test_limit], nlp, print_lines=True)\n",
    "print(f'{count}/{test_limit} sentences are tokenized differently with the standard tokenizer')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyphens (-) are used to join two or more words that act as a single term, to form some compound word. Looping through the tokens produced by the model as it is we will end up with two separate tokens for intra-hyphen words; we can consider both of them, but the position of the senses in the annotations will differ from the original one. Looping through the tokens produced by a split we will end up with a single token for intra-hyphen words; this will ensure that the annotations indexes are respected but the single token won't be correctly lemmatized by the spacy model unless we implement some complex logic to account for hyphens. We could split the text with respect to spaces and dashes but even in that case the index of the annotated words will differ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['laptop', 'cover', 'co', 'authored', 'well', 'known', 'stay', 'at', 'home', '40.000', \"L'hymnaire\"]\n"
     ]
    }
   ],
   "source": [
    "dashed_text = \"laptop-cover co-authored well-known stay-at-home 40.000 L'hymnaire\"\n",
    "\n",
    "# Define the splitting pattern to match spaces and dashes\n",
    "split_pattern = re.compile(r'[ -]')\n",
    "\n",
    "# Split the string using the pattern\n",
    "result = re.split(split_pattern, dashed_text)\n",
    "\n",
    "# Filter out empty strings resulting from consecutive spaces or dashes\n",
    "result = [s for s in result if s]\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a [custom tokenizer](https://stackoverflow.com/questions/55241927/spacy-intra-word-hyphens-how-to-treat-them-one-word) for the model that treats hyphenated words this as a single token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: In Hinduism , the 60th birthday of a man is called Sashti poorthi  .\n",
      "       0        1 2   3    4        5  6 7   8  9     10     11      12 13\n",
      "annotations: {'token_span': [5, 6], 'label': 'birthday%1:28:00::'}\n",
      "             {'token_span': [8, 9], 'label': 'man%1:18:00::'}\n",
      "Original tokenizer: ['in', 'Hinduism', ',', 'the', '60th', 'birthday', 'of', 'a', 'man', 'be', 'call', 'Sashti', 'poorthi', '.']\n",
      "Split             : ['In', 'Hinduism', ',', 'the', '60th', 'birthday', 'of', 'a', 'man', 'is', 'called', 'Sashti', 'poorthi', '.']\n",
      "Custom tokenizer  : ['in', 'Hinduism', ',', 'the', '60th', 'birthday', 'of', 'a', 'man', 'be', 'call', 'Sashti', 'poorthi', '.']\n",
      "\n",
      "text: The new world of English words came out in 1658 and  a dictionary of 40,000 words had been prepared in 1721 by Nathan Bailey  , though none was as comprehensive in breadth or style as Johnson's  .\n",
      "        0   1     2  3       4     5    6   7  8    9  10 11         12 13     14    15  16   17       18 19   20 21     22     23 24     25   26  27 28            29 30      31 32    33 34        35 36\n",
      "annotations: {'token_span': [1, 2], 'label': 'world%1:14:01::'}\n",
      "             {'token_span': [4, 5], 'label': 'word%1:10:00::'}\n",
      "             {'token_span': [11, 12], 'label': 'dictionary%1:10:00::'}\n",
      "             {'token_span': [14, 15], 'label': 'word%1:10:00::'}\n",
      "             {'token_span': [32, 33], 'label': 'style%1:10:00::'}\n",
      "Original tokenizer: ['the', 'new', 'world', 'of', 'english', 'word', 'come', 'out', 'in', '1658', 'and', 'a', 'dictionary', 'of', '40,000', 'word', 'have', 'be', 'prepare', 'in', '1721', 'by', 'Nathan', 'Bailey', ',', 'though', 'none', 'be', 'as', 'comprehensive', 'in', 'breadth', 'or', 'style', 'as', 'Johnson', \"'s\", '.']\n",
      "Split             : ['The', 'new', 'world', 'of', 'English', 'words', 'came', 'out', 'in', '1658', 'and', 'a', 'dictionary', 'of', '40,000', 'words', 'had', 'been', 'prepared', 'in', '1721', 'by', 'Nathan', 'Bailey', ',', 'though', 'none', 'was', 'as', 'comprehensive', 'in', 'breadth', 'or', 'style', 'as', \"Johnson's\", '.']\n",
      "Custom tokenizer  : ['the', 'new', 'world', 'of', 'english', 'word', 'come', 'out', 'in', '1658', 'and', 'a', 'dictionary', 'of', '40,000', 'word', 'have', 'be', 'prepare', 'in', '1721', 'by', 'Nathan', 'Bailey', ',', 'though', 'none', 'be', 'as', 'comprehensive', 'in', 'breadth', 'or', 'style', 'as', \"Johnson's\", '.']\n",
      "\n",
      "text: Even without the availability of either co-receptor ( even CCR5  )  , the virus can still invade cells if gp41 were to go through an alteration  ( including its cytoplasmic tail  ) that resulted in the independence of CD4 without the need of CCR5 and  / or CXCR4 as  a doorway  .\n",
      "         0       1   2            3  4      5           6 7    8    9 10 11  12    13  14    15     16    17 18   19   20 21 22      23 24         25 26        27  28          29   30 31   32       33 34  35           36 37  38      39  40   41 42   43  44 45 46    47 48 49      50 51\n",
      "annotations: {'token_span': [17, 18], 'label': 'cell%1:03:00::'}\n",
      "             {'token_span': [30, 31], 'label': 'tail%1:05:00::'}\n",
      "             {'token_span': [50, 51], 'label': 'doorway%1:06:00::'}\n",
      "Original tokenizer: ['even', 'without', 'the', 'availability', 'of', 'either', 'co', '-', 'receptor', '(', 'even', 'CCR5', ')', ',', 'the', 'virus', 'can', 'still', 'invade', 'cell', 'if', 'gp41', 'be', 'to', 'go', 'through', 'an', 'alteration', '(', 'include', 'its', 'cytoplasmic', 'tail', ')', 'that', 'result', 'in', 'the', 'independence', 'of', 'CD4', 'without', 'the', 'need', 'of', 'CCR5', 'and', '/', 'or', 'CXCR4', 'as', 'a', 'doorway', '.']\n",
      "Split             : ['Even', 'without', 'the', 'availability', 'of', 'either', 'co-receptor', '(', 'even', 'CCR5', ')', ',', 'the', 'virus', 'can', 'still', 'invade', 'cells', 'if', 'gp41', 'were', 'to', 'go', 'through', 'an', 'alteration', '(', 'including', 'its', 'cytoplasmic', 'tail', ')', 'that', 'resulted', 'in', 'the', 'independence', 'of', 'CD4', 'without', 'the', 'need', 'of', 'CCR5', 'and', '/', 'or', 'CXCR4', 'as', 'a', 'doorway', '.']\n",
      "Custom tokenizer  : ['even', 'without', 'the', 'availability', 'of', 'either', 'co-receptor', '(', 'even', 'CCR5', ')', ',', 'the', 'virus', 'can', 'still', 'invade', 'cell', 'if', 'gp41', 'be', 'to', 'go', 'through', 'an', 'alteration', '(', 'include', 'its', 'cytoplasmic', 'tail', ')', 'that', 'result', 'in', 'the', 'independence', 'of', 'CD4', 'without', 'the', 'need', 'of', 'CCR5', 'and', '/', 'or', 'CXCR4', 'as', 'a', 'doorway', '.']\n",
      "\n",
      "text: Typically , NATO inert munitions are painted entirely in light blue and  / or have the word  \" INERT  \" stenciled on them in prominent locations .[ citation needed  ] IED  ( barrel bomb  , nail bomb  , pipe bomb  , pressure cooker bomb  , fertilizer bomb  , molotov cocktail  )\n",
      "              0 1    2     3         4   5       6        7  8     9   10  11 12 13   14  15   16 17    18 19        20 21   22 23        24        25 26       27     28 29  30 31     32   33 34   35   36 37   38   39 40       41     42   43 44         45   46 47      48       49 50\n",
      "annotations: {'token_span': [16, 17], 'label': 'word%1:10:00::'}\n",
      "             {'token_span': [35, 36], 'label': 'nail%1:06:00::'}\n",
      "             {'token_span': [48, 49], 'label': 'cocktail%1:13:00::'}\n",
      "Original tokenizer: ['typically', ',', 'NATO', 'inert', 'munition', 'be', 'paint', 'entirely', 'in', 'light', 'blue', 'and', '/', 'or', 'have', 'the', 'word', '\"', 'inert', '\"', 'stencil', 'on', 'they', 'in', 'prominent', 'location', '.', '[', 'citation', 'need', ']', 'IED', '(', 'barrel', 'bomb', ',', 'nail', 'bomb', ',', 'pipe', 'bomb', ',', 'pressure', 'cooker', 'bomb', ',', 'fertilizer', 'bomb', ',', 'molotov', 'cocktail', ')']\n",
      "Split             : ['Typically', ',', 'NATO', 'inert', 'munitions', 'are', 'painted', 'entirely', 'in', 'light', 'blue', 'and', '/', 'or', 'have', 'the', 'word', '\"', 'INERT', '\"', 'stenciled', 'on', 'them', 'in', 'prominent', 'locations', '.[', 'citation', 'needed', ']', 'IED', '(', 'barrel', 'bomb', ',', 'nail', 'bomb', ',', 'pipe', 'bomb', ',', 'pressure', 'cooker', 'bomb', ',', 'fertilizer', 'bomb', ',', 'molotov', 'cocktail', ')']\n",
      "Custom tokenizer  : ['typically', ',', 'NATO', 'inert', 'munition', 'be', 'paint', 'entirely', 'in', 'light', 'blue', 'and', '/', 'or', 'have', 'the', 'word', '\"', 'inert', '\"', 'stencil', 'on', 'they', 'in', 'prominent', 'location', '.[', 'citation', 'need', ']', 'IED', '(', 'barrel', 'bomb', ',', 'nail', 'bomb', ',', 'pipe', 'bomb', ',', 'pressure', 'cooker', 'bomb', ',', 'fertilizer', 'bomb', ',', 'molotov', 'cocktail', ')']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def custom_tokenizer(nlp):\n",
    "    infix_re = re.compile(r'(\\w+)-(\\w+)')  # Define an infix regex pattern for hyphens between words\n",
    "\n",
    "    return Tokenizer(nlp.vocab, infix_finditer=infix_re.finditer)\n",
    "\n",
    "custom_nlp = spacy.load(\"en_core_web_sm\")\n",
    "custom_nlp.tokenizer = custom_tokenizer(custom_nlp)\n",
    "\n",
    "for line in sample_lines:\n",
    "    \n",
    "    custom_doc = custom_nlp(line['text'])\n",
    "    doc = nlp(line['text'])\n",
    "\n",
    "    pretty_print(line, print_indexes=True)\n",
    "    print(f'Original tokenizer: {[t.lemma_ for t in doc]}')\n",
    "    print(f'Split             : {[t for t in line[\"text\"].split()]}')\n",
    "    print(f'Custom tokenizer  : {[t.lemma_ for t in custom_doc]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1000 sentences are tokenized differently\n"
     ]
    }
   ],
   "source": [
    "count_custom_nlp = count_different_tokenization_indexes(lines[:test_limit], custom_nlp, print_lines=True)\n",
    "print(f'{count_custom_nlp}/{test_limit} sentences are tokenized differently')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have another problem: sometimes the index in the annotations is off. We need to account for that as, for example in the second sample line, in some cases the word we need to replace will remain there along with its explicit senses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word_in_window(lemmatized_tokens, window_span, sense_key):\n",
    "    \"\"\"\n",
    "    We have a few tokens and a sense_key, that contains the lemma of a word inside the text.\n",
    "    We want to find the index of that word.\n",
    "    \"\"\"\n",
    "\n",
    "    start_idx = window_span[0]\n",
    "    end_idx = window_span[1]\n",
    "\n",
    "    lemma_from_sense_key = sense_key[:sense_key.index('%')]\n",
    "\n",
    "    target_idx = -1\n",
    "\n",
    "    for index in range(start_idx, end_idx + 1):\n",
    "        lemma = lemmatized_tokens[index]\n",
    "        # print(f'Token [{token}] vs lemma [{lemma}]')\n",
    "        if lemma == lemma_from_sense_key:\n",
    "            target_idx = index\n",
    "            break\n",
    "\n",
    "    if target_idx == -1:\n",
    "        raise ValueError(f'No match in window for token {sense_key} in window {lemmatized_tokens[start_idx:end_idx + 1]}')\n",
    "    return target_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: In Hinduism , the 60th birthday of a man is called Sashti poorthi  .\n",
      "       0        1 2   3    4        5  6 7   8  9     10     11      12 13\n",
      "annotations: {'token_span': [5, 6], 'label': 'birthday%1:28:00::'}\n",
      "             {'token_span': [8, 9], 'label': 'man%1:18:00::'}\n",
      "birthday%1:28:00:: in range (5 - 6) -> true index = 5\n",
      "man%1:18:00:: in range (8 - 9) -> true index = 8\n",
      "\n",
      "text: The new world of English words came out in 1658 and  a dictionary of 40,000 words had been prepared in 1721 by Nathan Bailey  , though none was as comprehensive in breadth or style as Johnson's  .\n",
      "        0   1     2  3       4     5    6   7  8    9  10 11         12 13     14    15  16   17       18 19   20 21     22     23 24     25   26  27 28            29 30      31 32    33 34        35 36\n",
      "annotations: {'token_span': [1, 2], 'label': 'world%1:14:01::'}\n",
      "             {'token_span': [4, 5], 'label': 'word%1:10:00::'}\n",
      "             {'token_span': [11, 12], 'label': 'dictionary%1:10:00::'}\n",
      "             {'token_span': [14, 15], 'label': 'word%1:10:00::'}\n",
      "             {'token_span': [32, 33], 'label': 'style%1:10:00::'}\n",
      "world%1:14:01:: in range (1 - 2) -> true index = 2\n",
      "word%1:10:00:: in range (4 - 5) -> true index = 5\n",
      "dictionary%1:10:00:: in range (11 - 12) -> true index = 12\n",
      "word%1:10:00:: in range (14 - 15) -> true index = 15\n",
      "style%1:10:00:: in range (32 - 33) -> true index = 33\n",
      "\n",
      "text: Even without the availability of either co-receptor ( even CCR5  )  , the virus can still invade cells if gp41 were to go through an alteration  ( including its cytoplasmic tail  ) that resulted in the independence of CD4 without the need of CCR5 and  / or CXCR4 as  a doorway  .\n",
      "         0       1   2            3  4      5           6 7    8    9 10 11  12    13  14    15     16    17 18   19   20 21 22      23 24         25 26        27  28          29   30 31   32       33 34  35           36 37  38      39  40   41 42   43  44 45 46    47 48 49      50 51\n",
      "annotations: {'token_span': [17, 18], 'label': 'cell%1:03:00::'}\n",
      "             {'token_span': [30, 31], 'label': 'tail%1:05:00::'}\n",
      "             {'token_span': [50, 51], 'label': 'doorway%1:06:00::'}\n",
      "cell%1:03:00:: in range (17 - 18) -> true index = 17\n",
      "tail%1:05:00:: in range (30 - 31) -> true index = 30\n",
      "doorway%1:06:00:: in range (50 - 51) -> true index = 50\n",
      "\n",
      "text: Typically , NATO inert munitions are painted entirely in light blue and  / or have the word  \" INERT  \" stenciled on them in prominent locations .[ citation needed  ] IED  ( barrel bomb  , nail bomb  , pipe bomb  , pressure cooker bomb  , fertilizer bomb  , molotov cocktail  )\n",
      "              0 1    2     3         4   5       6        7  8     9   10  11 12 13   14  15   16 17    18 19        20 21   22 23        24        25 26       27     28 29  30 31     32   33 34   35   36 37   38   39 40       41     42   43 44         45   46 47      48       49 50\n",
      "annotations: {'token_span': [16, 17], 'label': 'word%1:10:00::'}\n",
      "             {'token_span': [35, 36], 'label': 'nail%1:06:00::'}\n",
      "             {'token_span': [48, 49], 'label': 'cocktail%1:13:00::'}\n",
      "word%1:10:00:: in range (16 - 17) -> true index = 16\n",
      "nail%1:06:00:: in range (35 - 36) -> true index = 35\n",
      "cocktail%1:13:00:: in range (48 - 49) -> true index = 49\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the wordnet lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for sample_line in sample_lines:\n",
    "    pretty_print(sample_line, print_indexes=True)\n",
    "    lemmatized_tokens = [token.lemma_ for token in custom_nlp(sample_line['text'])]\n",
    "    for annotation in sample_line['annotations']:\n",
    "        window_start = annotation['token_span'][0]\n",
    "        window_end = annotation['token_span'][1]\n",
    "        idx = find_word_in_window(lemmatized_tokens, (window_start, window_end), annotation['label'])\n",
    "        print(f'{annotation[\"label\"]} in range ({window_start} - {window_end}) -> true index = {idx}')\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build the final preprocessing routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_line(line, \n",
    "                    model,\n",
    "                    stopwords=None, \n",
    "                    punctuation=None,\n",
    "                    lemmatize=None,\n",
    "                    filtering_regex=None, \n",
    "                    replace_with_sense_key=True,\n",
    "                    produce_both=False\n",
    "                    ):\n",
    "\n",
    "\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    "\n",
    "    if punctuation is None:\n",
    "        punctuation = []\n",
    "\n",
    "    # Take text and annotations from the line\n",
    "    text = line['text'].lower()\n",
    "    annotations = line['annotations']\n",
    "\n",
    "    # Indexes of the words we may want to replace with a sense key\n",
    "    # {token1_index: token1, ...}\n",
    "    lemmatized_tokens = model(text)\n",
    "    annotations_data = {(annotation['token_span'][0], annotation['token_span'][1]): annotation['label'] for annotation in annotations}\n",
    "    annotation_indexes = {find_word_in_window([token.lemma_ for token in lemmatized_tokens], (k[0], k[1]), v): v for k, v in annotations_data.items()}\n",
    "    \n",
    "    # This will be then joined to create a new preprocessed sentence\n",
    "    new_tokens = []\n",
    "    new_tokens_sense = []\n",
    "    \n",
    "    # For each token, check if it is a stopword, punctuation, if it matches the filter regex \n",
    "    # and if it needs to be retained or replaced with its sense key. Doc now tokenizes as the\n",
    "    # standard string split, so there will be no differences in the token indexes\n",
    "    for i, token in enumerate(lemmatized_tokens):\n",
    "\n",
    "        token_text = token.lemma_ if lemmatize else token.text\n",
    "        \n",
    "        # The token is a word we can replace with the lemma key\n",
    "        if i in annotation_indexes.keys():\n",
    "\n",
    "            # We may want to produce both sentences, one with lemma key and one without\n",
    "            # in order to train a model for word embeddings and another model with\n",
    "            # sense embeddings\n",
    "            if produce_both:\n",
    "                new_tokens.append(token_text)\n",
    "                new_tokens_sense.append(annotation_indexes[i]) # [i][1])\n",
    "            else:\n",
    "                if replace_with_sense_key:\n",
    "                    # print(f'Token {token_text:20s} is good, we keep it as {annotation_indexes[i][1]}')\n",
    "                    new_tokens.append(annotation_indexes[i]) # [i][1])\n",
    "                else:\n",
    "                    # print(f'Token {token_text:20s} is good, we keep it as {token_text}')\n",
    "                    new_tokens.append(token_text)\n",
    "                \n",
    "        # The token is a stopword\n",
    "        elif token_text in stopwords:\n",
    "            # print(f'Token {token_text:20s} is a stopword')\n",
    "            continue\n",
    "\n",
    "        # The token is punctuation\n",
    "        elif token_text in punctuation:\n",
    "            # print(f'Token {token_text:20s} is punctuation')\n",
    "            continue\n",
    "\n",
    "        # The token matches the regex, it is not valid\n",
    "        elif filtering_regex is not None and filtering_regex.search(token_text):\n",
    "            # print(f'Token {token_text:20s} is filtered out from regex')\n",
    "            continue\n",
    "\n",
    "        # The token is valid, we can retain it\n",
    "        else:\n",
    "            # print(f'Token {token_text:20s} is good, we keep it')\n",
    "            new_tokens.append(token_text)\n",
    "            if produce_both:\n",
    "                new_tokens_sense.append(token_text)\n",
    "\n",
    "    return ' '.join(new_tokens), ' '.join(new_tokens_sense)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data before: \n",
      "In Hinduism , the 60th birthday of a man is called Sashti poorthi .\n",
      "[In, Hinduism, ,, the, 60th, birthday, of, a, man, is, called, Sashti, poorthi, .]\n",
      "Preprocessed data:\n",
      "('hinduism birthday%1:28:00:: man%1:18:00:: called sashti poorthi', '')\n",
      "\n",
      "Data before: \n",
      "The new world of English words came out in 1658 and a dictionary of 40,000 words had been prepared in 1721 by Nathan Bailey , though none was as comprehensive in breadth or style as Johnson's .\n",
      "[The, new, world, of, English, words, came, out, in, 1658, and, a, dictionary, of, 40,000, words, had, been, prepared, in, 1721, by, Nathan, Bailey, ,, though, none, was, as, comprehensive, in, breadth, or, style, as, Johnson's, .]\n",
      "Preprocessed data:\n",
      "('new world%1:14:01:: english word%1:10:00:: came dictionary%1:10:00:: word%1:10:00:: prepared nathan bailey though none comprehensive breadth style%1:10:00::', '')\n",
      "\n",
      "Data before: \n",
      "Even without the availability of either co-receptor ( even CCR5 ) , the virus can still invade cells if gp41 were to go through an alteration ( including its cytoplasmic tail ) that resulted in the independence of CD4 without the need of CCR5 and / or CXCR4 as a doorway .\n",
      "[Even, without, the, availability, of, either, co-receptor, (, even, CCR5, ), ,, the, virus, can, still, invade, cells, if, gp41, were, to, go, through, an, alteration, (, including, its, cytoplasmic, tail, ), that, resulted, in, the, independence, of, CD4, without, the, need, of, CCR5, and, /, or, CXCR4, as, a, doorway, .]\n",
      "Preprocessed data:\n",
      "('even without availability either co-receptor even virus still invade cell%1:03:00:: go alteration including cytoplasmic tail%1:05:00:: resulted independence without need doorway%1:06:00::', '')\n",
      "\n",
      "Data before: \n",
      "Typically , NATO inert munitions are painted entirely in light blue and / or have the word \" INERT \" stenciled on them in prominent locations .[ citation needed ] IED ( barrel bomb , nail bomb , pipe bomb , pressure cooker bomb , fertilizer bomb , molotov cocktail )\n",
      "[Typically, ,, NATO, inert, munitions, are, painted, entirely, in, light, blue, and, /, or, have, the, word, \", INERT, \", stenciled, on, them, in, prominent, locations, .[, citation, needed, ], IED, (, barrel, bomb, ,, nail, bomb, ,, pipe, bomb, ,, pressure, cooker, bomb, ,, fertilizer, bomb, ,, molotov, cocktail, )]\n",
      "Preprocessed data:\n",
      "('typically nato inert munitions painted entirely light blue word%1:10:00:: inert stenciled prominent locations citation needed ied barrel bomb nail%1:06:00:: bomb pipe bomb pressure cooker bomb fertilizer bomb molotov cocktail%1:13:00::', '')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sample_line in sample_lines:\n",
    "    print('Data before: ')\n",
    "    print(sample_line['text'])\n",
    "    print([token for token in custom_nlp(sample_line['text'])])\n",
    "    print('Preprocessed data:')\n",
    "    print(preprocess_line(sample_line, custom_nlp, stopwords=stop_words, filtering_regex=regexp_alphbetic))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: Later on 9 January , Samuel scored a last - minute winner in the 4–3 win versus Siena by beating goalkeeper Gianluca Curci with  a left  - footed shot  .\n",
      "          0  1 2       3 4      5      6 7    8 9     10     11 12  13  14  15     16    17 18      19         20       21    22   23 24   25 26     27   28 29\n",
      "annotations: {'token_span': [10, 11], 'label': 'minute%1:28:00::'}\n",
      "             {'token_span': [11, 12], 'label': 'winner%1:18:00::'}\n",
      "             {'token_span': [15, 16], 'label': 'win%1:11:00::'}\n",
      "             {'token_span': [27, 28], 'label': 'foot%2:38:00::'}\n",
      "['later', 'on', '9', 'January', ',', 'Samuel', 'score', 'a', 'last', '-', 'minute', 'winner', 'in', 'the', '4–3', 'win', 'versus', 'Siena', 'by', 'beat', 'goalkeeper', 'Gianluca', 'Curci', 'with', 'a', 'left', '-', 'footed', 'shot', '.']\n"
     ]
    }
   ],
   "source": [
    "pretty_print(lines[51].copy(), print_indexes=True)\n",
    "print([token.lemma_ for token in custom_nlp(lines[51]['text'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 534300/534300 [1:07:41<00:00, 131.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed sentence            : hinduism birthday man called sashti poorthi\n",
      "Preprocessed sentence with senses: hinduism birthday%1:28:00:: man%1:18:00:: called sashti poorthi\n",
      "Discarded 26906 out of 500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the actual datasets \n",
    "word_sentences = []\n",
    "sense_sentences = []\n",
    "discarded_count = 0\n",
    "for line in tqdm(lines):\n",
    "    try:\n",
    "        word_sentence, sense_sentence = preprocess_line(line, custom_nlp, stopwords=stop_words, filtering_regex=regexp_alphbetic, produce_both=True)\n",
    "        word_sentences.append(word_sentence)\n",
    "        sense_sentences.append(sense_sentence)\n",
    "    except:\n",
    "        discarded_count += 1\n",
    "\n",
    "print(f'Preprocessed sentence            : {word_sentences[0]}')\n",
    "print(f'Preprocessed sentence with senses: {sense_sentences[0]}')\n",
    "print(f'Discarded {discarded_count} out of 500000')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now despite discarding some sentences at least we can rest assured that only senses for the specified tokens appear in the dataset (before we could've had \"word\", \"word%1:10:00\" and so on and this would've affected the similarity score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could filter out uncommon words that have a low frequency but we can do the same specifying the `min_count` parameter of the Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_frequency(sentences, frequency):\n",
    "\n",
    "    ## remove words that appear only once\n",
    "    frequency_dict = defaultdict(int)\n",
    "    for sentence in tqdm(sentences):\n",
    "        tokens = sentence.split()\n",
    "        for token in tokens:\n",
    "            frequency_dict[token] += 1\n",
    "\n",
    "    texts = [[token for token in sentence.split() if frequency_dict[token] > frequency]\n",
    "            for sentence in sentences]\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ff_word_sentences = filter_frequency(word_sentences, 1)\n",
    "# ff_sense_sentences = filter_frequency(sense_sentences, 1)\n",
    "\n",
    "# for i in range(10):\n",
    "#   print(ff_word_sentences[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the preprocessed dictionary to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "507394it [00:03, 145866.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# Specify the output JSONLines file. Set it to None to skip saving the dataset\n",
    "output_file = 'preprocessed_dataset_new.jsonl'\n",
    "# output_file = None\n",
    "\n",
    "if output_file is not None:\n",
    "\n",
    "    # Take all the original lines\n",
    "    original_sentences = [line['text'] for line in lines]\n",
    "\n",
    "    # Open the JSONLines file for writing\n",
    "    with jsonlines.open(output_file, mode='w') as writer:\n",
    "\n",
    "        for original, preprocessed, preprocessed_with_senses in tqdm(zip(original_sentences, word_sentences, sense_sentences)):\n",
    "\n",
    "            # Create a dictionary for each set of strings\n",
    "            data = {\n",
    "                'original': original,\n",
    "                'preprocessed': preprocessed,\n",
    "                'preprocessed_with_lemma_keys': preprocessed_with_senses\n",
    "            }\n",
    "            \n",
    "            # Write the dictionary to the JSONLines file\n",
    "            writer.write(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the preprocessed dictionary from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "507394it [00:02, 208607.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# Read the preprocessed dictionary from file\n",
    "input_file = 'preprocessed_dataset_new.jsonl'\n",
    "# input_file = None\n",
    "\n",
    "read_original_sentences = []\n",
    "read_word_sentences = []\n",
    "read_sense_sentences = []\n",
    "\n",
    "if input_file is not None:\n",
    "    with jsonlines.open(input_file) as reader:\n",
    "        for line in tqdm(reader.iter()):\n",
    "            read_original_sentences.append(line['original'])\n",
    "            read_word_sentences.append(line['preprocessed'])\n",
    "            read_sense_sentences.append(line['preprocessed_with_lemma_keys'])\n",
    "\n",
    "    word_sentences = [[token for token in sentence.split()] for sentence in read_word_sentences]\n",
    "    sense_sentences = [[token for token in sentence.split()] for sentence in read_sense_sentences]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `Word2Vec` is a word embedding technique that learns vector representations of words, capturing semantic relationships through context.\n",
    "2. `GloVe` combines global and local word context through matrix factorization to create word embeddings.\n",
    "3. `FastText` represents words as character n-grams, handling morphological details and out-of-vocabulary words.\n",
    "4. `Doc2Vec` extends Word2Vec to learn document-level embeddings by treating each document as a unique word.\n",
    "5. `BERT`, a transformer-based model, learns contextual word embeddings by considering both left and right sentence context.\n",
    "6. `ELMo` uses bi-directional LSTM to create word embeddings based on entire sentence context, enhancing syntactic and semantic understanding.\n",
    "7. `USE` is a universal sentence encoder for generating fixed-length embeddings, applicable to various NLP tasks.\n",
    "\n",
    "More [here](https://medium.com/@vaibhav1403/embedding-techniques-in-natural-language-processing-nlp-29e424ab0cd9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x7f7f06904d30>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_size = 10\n",
    "window = 2\n",
    "min_count = 1 \n",
    "\n",
    "word_embeddings_model = gensim.models.Word2Vec(word_sentences, vector_size=vector_size, window=window, min_count=min_count)\n",
    "word_embeddings_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load non semantic simlex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old\tnew\t1.58\n",
      "smart\tintelligent\t9.2\n",
      "hard\tdifficult\t8.77\n",
      "happy\tcheerful\t9.55\n",
      "hard\teasy\t0.95\n",
      "fast\trapid\t8.75\n",
      "happy\tglad\t9.17\n",
      "short\tlong\t1.23\n",
      "stupid\tdumb\t9.58\n",
      "weird\tstrange\t8.93\n",
      "wide\tnarrow\t1.03\n",
      "bad\tawful\t8.42\n",
      "easy\tdifficult\t0.58\n",
      "bad\tterrible\t7.78\n",
      "hard\tsimple\t1.38\n",
      "smart\tdumb\t0.55\n",
      "insane\tcrazy\t9.57\n",
      "happy\tmad\t0.95\n",
      "large\thuge\t9.47\n",
      "hard\ttough\t8.05\n",
      "new\tfresh\t6.83\n",
      "sharp\tdull\t0.6\n",
      "quick\trapid\t9.7\n",
      "dumb\tfoolish\t6.67\n",
      "wonderful\tterrific\t8.63\n",
      "strange\todd\t9.02\n",
      "happy\tangry\t1.28\n",
      "narrow\tbroad\t1.18\n",
      "simple\teasy\t9.4\n",
      "old\tfresh\t0.87\n",
      "apparent\tobvious\t8.47\n",
      "inexpensive\tcheap\t8.72\n",
      "nice\tgenerous\t5.0\n",
      "weird\tnormal\t0.72\n",
      "weird\todd\t9.2\n",
      "bad\timmoral\t7.62\n",
      "sad\tfunny\t0.95\n",
      "wonderful\tgreat\t8.05\n",
      "guilty\tashamed\t6.38\n",
      "beautiful\twonderful\t6.5\n",
      "confident\tsure\t8.27\n",
      "dumb\tdense\t7.27\n",
      "large\tbig\t9.55\n",
      "nice\tcruel\t0.67\n",
      "impatient\tanxious\t6.03\n",
      "big\tbroad\t6.73\n",
      "strong\tproud\t3.17\n",
      "unnecessary\tnecessary\t0.63\n",
      "restless\tyoung\t1.6\n",
      "dumb\tintelligent\t0.75\n",
      "bad\tgreat\t0.35\n",
      "difficult\tsimple\t0.87\n",
      "necessary\timportant\t7.37\n",
      "bad\tterrific\t0.65\n",
      "mad\tglad\t1.45\n",
      "honest\tguilty\t1.18\n",
      "easy\ttough\t0.52\n",
      "easy\tflexible\t4.1\n",
      "certain\tsure\t8.42\n",
      "essential\tnecessary\t8.97\n",
      "different\tnormal\t1.08\n",
      "sly\tclever\t7.25\n",
      "crucial\timportant\t8.82\n",
      "harsh\tcruel\t8.18\n",
      "childish\tfoolish\t5.5\n",
      "scarce\trare\t9.17\n",
      "friendly\tgenerous\t5.9\n",
      "fragile\tfrigid\t2.38\n",
      "long\tnarrow\t3.57\n",
      "big\theavy\t6.18\n",
      "rough\tfrigid\t2.47\n",
      "bizarre\tstrange\t9.37\n",
      "illegal\timmoral\t4.28\n",
      "bad\tguilty\t4.2\n",
      "modern\tancient\t0.73\n",
      "new\tancient\t0.23\n",
      "dull\tfunny\t0.55\n",
      "happy\tyoung\t2.0\n",
      "easy\tbig\t1.12\n",
      "great\tawful\t1.17\n",
      "tiny\thuge\t0.6\n",
      "polite\tproper\t7.63\n",
      "modest\tashamed\t2.65\n",
      "exotic\trare\t8.05\n",
      "dumb\tclever\t1.17\n",
      "delightful\twonderful\t8.65\n",
      "noticeable\tobvious\t8.48\n",
      "afraid\tanxious\t5.07\n",
      "formal\tproper\t8.02\n",
      "dreary\tdull\t8.25\n",
      "delightful\tcheerful\t6.58\n",
      "unhappy\tmad\t5.95\n",
      "sad\tterrible\t5.4\n",
      "sick\tcrazy\t3.57\n",
      "violent\tangry\t6.98\n",
      "laden\theavy\t5.9\n",
      "dirty\tcheap\t1.6\n",
      "elastic\tflexible\t7.78\n",
      "hard\tdense\t5.9\n",
      "recent\tnew\t7.05\n",
      "bold\tproud\t3.97\n",
      "sly\tstrange\t1.97\n",
      "strange\tsly\t2.07\n",
      "dumb\trare\t0.48\n",
      "sly\ttough\t0.58\n",
      "terrific\tmad\t0.4\n",
      "modest\tflexible\t0.98\n",
      "fresh\twide\t0.4\n",
      "huge\tdumb\t0.48\n",
      "large\tflexible\t0.48\n",
      "dirty\tnarrow\t0.3\n",
      "wife\thusband\t2.3\n",
      "book\ttext\t6.35\n",
      "groom\tbride\t3.17\n",
      "night\tday\t1.88\n",
      "south\tnorth\t2.2\n",
      "plane\tairport\t3.65\n",
      "uncle\taunt\t5.5\n",
      "horse\tmare\t8.33\n",
      "bottom\ttop\t0.7\n",
      "friend\tbuddy\t8.78\n",
      "student\tpupil\t9.35\n",
      "world\tglobe\t6.67\n",
      "leg\tarm\t2.88\n",
      "plane\tjet\t8.1\n",
      "woman\tman\t3.33\n",
      "horse\tcolt\t7.07\n",
      "actress\tactor\t7.12\n",
      "teacher\tinstructor\t9.25\n",
      "movie\tfilm\t8.87\n",
      "bird\thawk\t7.85\n",
      "word\tdictionary\t3.68\n",
      "money\tsalary\t7.88\n",
      "dog\tcat\t1.75\n",
      "area\tregion\t9.47\n",
      "navy\tarmy\t6.43\n",
      "book\tliterature\t7.53\n",
      "clothes\tcloset\t3.27\n",
      "sunset\tsunrise\t2.47\n",
      "child\tadult\t2.98\n",
      "cow\tcattle\t9.52\n",
      "book\tstory\t5.63\n",
      "winter\tsummer\t2.38\n",
      "taxi\tcab\t9.2\n",
      "tree\tmaple\t5.53\n",
      "bed\tbedroom\t3.4\n",
      "roof\tceiling\t7.58\n",
      "disease\tinfection\t7.15\n",
      "arm\tshoulder\t4.85\n",
      "sheep\tlamb\t8.42\n",
      "lady\tgentleman\t3.42\n",
      "boat\tanchor\t2.25\n",
      "priest\tmonk\t6.28\n",
      "toe\tfinger\t4.68\n",
      "river\tstream\t7.3\n",
      "anger\tfury\t8.73\n",
      "date\tcalendar\t4.42\n",
      "sea\tocean\t8.27\n",
      "second\tminute\t4.62\n",
      "hand\tthumb\t3.88\n",
      "wood\tlog\t7.3\n",
      "mud\tdirt\t7.32\n",
      "hallway\tcorridor\t9.28\n",
      "way\tmanner\t7.62\n",
      "mouse\tcat\t1.12\n",
      "cop\tsheriff\t9.05\n",
      "death\tburial\t4.93\n",
      "music\tmelody\t6.98\n",
      "beer\talcohol\t7.5\n",
      "mouth\tlip\t7.1\n",
      "storm\thurricane\t6.38\n",
      "tax\tincome\t2.38\n",
      "flower\tviolet\t6.95\n",
      "paper\tcardboard\t5.38\n",
      "floor\tceiling\t1.73\n",
      "beach\tseashore\t8.33\n",
      "rod\tcurtain\t3.03\n",
      "hound\tfox\t2.38\n",
      "street\talley\t5.48\n",
      "boat\tdeck\t4.28\n",
      "car\thorn\t2.57\n",
      "friend\tguest\t4.25\n",
      "employer\temployee\t3.65\n",
      "hand\twrist\t3.97\n",
      "ball\tcannon\t2.58\n",
      "alcohol\tbrandy\t6.98\n",
      "victory\ttriumph\t8.98\n",
      "telephone\tbooth\t3.63\n",
      "door\tdoorway\t5.4\n",
      "motel\tinn\t8.17\n",
      "clothes\tcloth\t5.47\n",
      "steak\tmeat\t7.47\n",
      "nail\tthumb\t3.55\n",
      "band\torchestra\t7.08\n",
      "book\tbible\t5.0\n",
      "business\tindustry\t7.02\n",
      "winter\tseason\t6.27\n",
      "decade\tcentury\t3.48\n",
      "alcohol\tgin\t8.65\n",
      "hat\tcoat\t2.67\n",
      "window\tdoor\t3.33\n",
      "arm\twrist\t3.57\n",
      "house\tapartment\t5.8\n",
      "glass\tcrystal\t6.27\n",
      "wine\tbrandy\t5.15\n",
      "creator\tmaker\t9.62\n",
      "dinner\tbreakfast\t3.33\n",
      "arm\tmuscle\t3.72\n",
      "bubble\tsuds\t8.57\n",
      "bread\tflour\t3.33\n",
      "death\ttragedy\t5.8\n",
      "absence\tpresence\t0.4\n",
      "gun\tcannon\t5.68\n",
      "grass\tblade\t4.57\n",
      "ball\tbasket\t1.67\n",
      "hose\tgarden\t1.67\n",
      "boy\tkid\t7.5\n",
      "church\tchoir\t2.95\n",
      "clothes\tdrawer\t3.02\n",
      "tower\tbell\t1.9\n",
      "father\tparent\t7.07\n",
      "school\tgrade\t4.42\n",
      "parent\tadult\t5.37\n",
      "bar\tjail\t1.9\n",
      "car\thighway\t3.4\n",
      "dictionary\tdefinition\t6.25\n",
      "door\tcellar\t1.97\n",
      "army\tlegion\t5.95\n",
      "metal\taluminum\t7.25\n",
      "chair\tbench\t6.67\n",
      "cloud\tfog\t6.0\n",
      "boy\tson\t6.75\n",
      "water\tice\t6.47\n",
      "bed\tblanket\t3.02\n",
      "attorney\tlawyer\t9.35\n",
      "area\tzone\t8.33\n",
      "business\tcompany\t9.02\n",
      "clothes\tfabric\t5.87\n",
      "sweater\tjacket\t7.15\n",
      "money\tcapital\t6.67\n",
      "hand\tfoot\t4.17\n",
      "alcohol\tcocktail\t6.73\n",
      "yard\tinch\t3.78\n",
      "molecule\tatom\t6.45\n",
      "lens\tcamera\t4.28\n",
      "meal\tdinner\t7.15\n",
      "eye\ttear\t3.55\n",
      "god\tdevil\t1.8\n",
      "loop\tbelt\t3.1\n",
      "rat\tmouse\t7.78\n",
      "motor\tengine\t8.65\n",
      "car\tcab\t7.42\n",
      "cat\tlion\t6.75\n",
      "size\tmagnitude\t6.33\n",
      "reality\tfantasy\t1.03\n",
      "door\tgate\t5.25\n",
      "cat\tpet\t5.95\n",
      "tin\taluminum\t6.42\n",
      "bone\tjaw\t4.17\n",
      "cereal\twheat\t3.75\n",
      "house\tkey\t1.9\n",
      "blood\tflesh\t4.28\n",
      "door\tcorridor\t3.73\n",
      "god\tspirit\t7.3\n",
      "capability\tcompetence\t7.62\n",
      "abundance\tplenty\t8.97\n",
      "sofa\tchair\t6.67\n",
      "wall\tbrick\t4.68\n",
      "horn\tdrum\t2.68\n",
      "organ\tliver\t6.15\n",
      "strength\tmight\t7.07\n",
      "phrase\tword\t5.48\n",
      "band\tparade\t3.92\n",
      "stomach\twaist\t5.9\n",
      "cloud\tstorm\t5.6\n",
      "joy\tpride\t5.0\n",
      "noise\trattle\t6.17\n",
      "rain\tmist\t5.97\n",
      "beer\tbeverage\t5.42\n",
      "man\tuncle\t3.92\n",
      "apple\tjuice\t2.88\n",
      "intelligence\tlogic\t6.5\n",
      "communication\tlanguage\t7.47\n",
      "mink\tfur\t6.83\n",
      "mob\tcrowd\t7.85\n",
      "shore\tcoast\t8.83\n",
      "wire\tcord\t7.62\n",
      "bird\tturkey\t6.58\n",
      "bed\tcrib\t7.3\n",
      "competence\tability\t7.5\n",
      "cloud\thaze\t7.32\n",
      "supper\tmeal\t7.53\n",
      "bar\tcage\t2.8\n",
      "water\tsalt\t1.3\n",
      "sense\tintuition\t7.68\n",
      "situation\tcondition\t6.58\n",
      "crime\ttheft\t7.53\n",
      "style\tfashion\t8.5\n",
      "boundary\tborder\t9.08\n",
      "arm\tbody\t4.05\n",
      "boat\tcar\t2.37\n",
      "sandwich\tlunch\t6.3\n",
      "bride\tprincess\t2.8\n",
      "heroine\thero\t8.78\n",
      "car\tgauge\t1.13\n",
      "insect\tbee\t6.07\n",
      "crib\tcradle\t8.55\n",
      "animal\tperson\t3.05\n",
      "marijuana\therb\t6.5\n",
      "bed\thospital\t0.92\n",
      "cheek\ttongue\t4.52\n",
      "disc\tcomputer\t3.2\n",
      "curve\tangle\t3.33\n",
      "grass\tmoss\t5.0\n",
      "school\tlaw\t1.13\n",
      "foot\thead\t2.3\n",
      "mother\tguardian\t6.5\n",
      "orthodontist\tdentist\t8.27\n",
      "alcohol\twhiskey\t7.27\n",
      "mouth\ttooth\t6.3\n",
      "breakfast\tbacon\t4.37\n",
      "bathroom\tbedroom\t3.4\n",
      "plate\tbowl\t5.23\n",
      "meat\tbacon\t5.8\n",
      "air\thelium\t3.63\n",
      "worker\temployer\t5.37\n",
      "body\tchest\t4.45\n",
      "son\tfather\t3.82\n",
      "heart\tsurgery\t1.08\n",
      "woman\tsecretary\t1.98\n",
      "man\tfather\t4.83\n",
      "beach\tisland\t5.6\n",
      "story\ttopic\t5.0\n",
      "game\tfun\t3.42\n",
      "weekend\tweek\t4.0\n",
      "couple\tpair\t8.33\n",
      "woman\twife\t5.72\n",
      "sheep\tcattle\t4.77\n",
      "purse\tbag\t8.33\n",
      "ceiling\tcathedral\t2.42\n",
      "bean\tcoffee\t5.15\n",
      "wood\tpaper\t2.88\n",
      "top\tside\t1.9\n",
      "crime\tfraud\t5.65\n",
      "pain\tharm\t5.38\n",
      "lover\tcompanion\t5.97\n",
      "evening\tdusk\t7.78\n",
      "father\tdaughter\t2.62\n",
      "wine\tliquor\t7.85\n",
      "cow\tgoat\t2.93\n",
      "belief\topinion\t7.7\n",
      "reality\tillusion\t1.42\n",
      "pact\tagreement\t9.02\n",
      "wealth\tpoverty\t1.27\n",
      "accident\temergency\t4.93\n",
      "battle\tconquest\t7.22\n",
      "friend\tteacher\t2.62\n",
      "illness\tinfection\t6.9\n",
      "game\ttrick\t2.32\n",
      "brother\tson\t3.48\n",
      "aunt\tnephew\t3.1\n",
      "worker\tmechanic\t4.92\n",
      "doctor\torthodontist\t5.58\n",
      "oak\tmaple\t6.03\n",
      "bee\tqueen\t3.27\n",
      "car\tbicycle\t3.47\n",
      "goal\tquest\t5.83\n",
      "august\tmonth\t5.53\n",
      "army\tsquad\t5.08\n",
      "cloud\tweather\t4.87\n",
      "physician\tdoctor\t8.88\n",
      "canyon\tvalley\t6.75\n",
      "river\tvalley\t1.67\n",
      "sun\tsky\t2.27\n",
      "target\tarrow\t3.25\n",
      "chocolate\tpie\t2.27\n",
      "circumstance\tsituation\t7.85\n",
      "opinion\tchoice\t5.43\n",
      "rhythm\tmelody\t6.12\n",
      "gut\tnerve\t4.93\n",
      "day\tdawn\t5.47\n",
      "cattle\tbeef\t7.03\n",
      "doctor\tprofessor\t4.65\n",
      "arm\tvein\t3.65\n",
      "room\tbath\t3.33\n",
      "corporation\tbusiness\t9.02\n",
      "fun\tfootball\t1.97\n",
      "hill\tcliff\t4.28\n",
      "bone\tankle\t3.82\n",
      "apple\tcandy\t2.08\n",
      "helper\tmaid\t5.58\n",
      "leader\tmanager\t7.27\n",
      "lemon\ttea\t1.6\n",
      "bee\tant\t2.78\n",
      "basketball\tbaseball\t4.92\n",
      "rice\tbean\t2.72\n",
      "bed\tfurniture\t6.08\n",
      "emotion\tpassion\t7.72\n",
      "anarchy\tchaos\t7.93\n",
      "crime\tviolation\t7.12\n",
      "machine\tengine\t5.58\n",
      "beach\tsea\t4.68\n",
      "alley\tbowl\t1.53\n",
      "jar\tbottle\t7.83\n",
      "strength\tcapability\t5.28\n",
      "seed\tmustard\t3.48\n",
      "guitar\tdrum\t3.78\n",
      "opinion\tidea\t5.7\n",
      "north\twest\t3.63\n",
      "diet\tsalad\t2.98\n",
      "mother\twife\t3.02\n",
      "dad\tmother\t3.55\n",
      "captain\tsailor\t5.0\n",
      "meter\tyard\t5.6\n",
      "beer\tchampagne\t4.45\n",
      "motor\tboat\t2.57\n",
      "card\tbridge\t1.97\n",
      "science\tpsychology\t4.92\n",
      "sinner\tsaint\t1.6\n",
      "destruction\tconstruction\t0.98\n",
      "crowd\tbunch\t7.42\n",
      "beach\treef\t3.77\n",
      "man\tchild\t4.13\n",
      "bread\tcheese\t1.95\n",
      "champion\twinner\t8.73\n",
      "celebration\tceremony\t7.72\n",
      "menu\torder\t3.62\n",
      "king\tprincess\t3.27\n",
      "wealth\tprestige\t6.07\n",
      "endurance\tstrength\t6.58\n",
      "danger\tthreat\t8.78\n",
      "god\tpriest\t4.5\n",
      "men\tfraternity\t3.13\n",
      "buddy\tcompanion\t8.65\n",
      "teacher\thelper\t4.28\n",
      "body\tstomach\t3.93\n",
      "tongue\tthroat\t3.1\n",
      "house\tcarpet\t1.38\n",
      "intelligence\tskill\t5.35\n",
      "journey\tconquest\t4.72\n",
      "god\tprey\t1.23\n",
      "brother\tsoul\t0.97\n",
      "adversary\topponent\t9.05\n",
      "death\tcatastrophe\t4.13\n",
      "monster\tdemon\t6.95\n",
      "day\tmorning\t4.87\n",
      "man\tvictor\t1.9\n",
      "friend\tguy\t3.88\n",
      "song\tstory\t3.97\n",
      "ray\tsunshine\t6.83\n",
      "guy\tstud\t5.83\n",
      "chicken\trice\t1.43\n",
      "box\televator\t1.32\n",
      "butter\tpotato\t1.22\n",
      "apartment\tfurniture\t1.28\n",
      "lake\tswamp\t4.92\n",
      "salad\tvinegar\t1.13\n",
      "flower\tbulb\t4.48\n",
      "cloud\tmist\t6.67\n",
      "driver\tpilot\t6.28\n",
      "sugar\thoney\t5.13\n",
      "body\tshoulder\t2.88\n",
      "idea\timage\t3.55\n",
      "father\tbrother\t4.2\n",
      "moon\tplanet\t5.87\n",
      "ball\tcostume\t2.32\n",
      "rail\tfence\t5.22\n",
      "room\tbed\t2.35\n",
      "flower\tbush\t4.25\n",
      "bone\tknee\t4.17\n",
      "arm\tknee\t2.75\n",
      "bottom\tside\t2.63\n",
      "vessel\tvein\t5.15\n",
      "cat\trabbit\t2.37\n",
      "meat\tsandwich\t2.35\n",
      "belief\tconcept\t5.08\n",
      "intelligence\tinsight\t5.9\n",
      "attention\tinterest\t7.22\n",
      "attitude\tconfidence\t4.35\n",
      "right\tjustice\t7.05\n",
      "argument\tagreement\t1.45\n",
      "depth\tmagnitude\t6.12\n",
      "medium\tnews\t3.65\n",
      "winner\tcandidate\t2.78\n",
      "birthday\tdate\t5.08\n",
      "fee\tpayment\t7.15\n",
      "bible\thymn\t5.15\n",
      "exit\tdoorway\t5.5\n",
      "man\tsentry\t3.25\n",
      "aisle\thall\t6.35\n",
      "whiskey\tgin\t6.28\n",
      "blood\tmarrow\t3.4\n",
      "oil\tmink\t1.23\n",
      "floor\tdeck\t5.55\n",
      "roof\tfloor\t2.62\n",
      "door\tfloor\t1.67\n",
      "shoulder\thead\t3.42\n",
      "wagon\tcarriage\t7.7\n",
      "car\tcarriage\t5.13\n",
      "elbow\tankle\t3.13\n",
      "wealth\tfame\t4.02\n",
      "sorrow\tshame\t4.77\n",
      "administration\tmanagement\t7.25\n",
      "communication\tconversation\t8.02\n",
      "pollution\tatmosphere\t4.25\n",
      "anatomy\tbiology\t5.33\n",
      "college\tprofession\t3.12\n",
      "book\ttopic\t2.07\n",
      "formula\tequation\t7.95\n",
      "book\tinformation\t5.0\n",
      "boy\tpartner\t1.9\n",
      "sky\tuniverse\t4.68\n",
      "population\tpeople\t7.68\n",
      "college\tclass\t4.13\n",
      "chief\tmayor\t4.85\n",
      "rabbi\tminister\t7.62\n",
      "meter\tinch\t5.08\n",
      "polyester\tcotton\t5.63\n",
      "lawyer\tbanker\t1.88\n",
      "violin\tinstrument\t6.58\n",
      "camp\tcabin\t4.2\n",
      "pot\tappliance\t2.53\n",
      "linen\tfabric\t7.47\n",
      "whiskey\tchampagne\t5.33\n",
      "girl\tchild\t5.38\n",
      "cottage\tcabin\t7.72\n",
      "bird\then\t7.03\n",
      "racket\tnoise\t8.1\n",
      "sunset\tevening\t5.98\n",
      "drizzle\train\t9.17\n",
      "adult\tbaby\t2.22\n",
      "charcoal\tcoal\t7.63\n",
      "body\tspine\t4.78\n",
      "head\tnail\t2.47\n",
      "log\ttimber\t8.05\n",
      "spoon\tcup\t2.02\n",
      "body\tnerve\t3.13\n",
      "man\thusband\t5.32\n",
      "bone\tneck\t2.53\n",
      "frustration\tanger\t6.5\n",
      "river\tsea\t5.72\n",
      "task\tjob\t8.87\n",
      "club\tsociety\t5.23\n",
      "reflection\timage\t7.27\n",
      "prince\tking\t5.92\n",
      "snow\tweather\t5.48\n",
      "people\tparty\t2.2\n",
      "boy\tbrother\t6.67\n",
      "root\tgrass\t3.55\n",
      "brow\teye\t3.82\n",
      "money\tpearl\t2.1\n",
      "money\tdiamond\t3.42\n",
      "vehicle\tbus\t6.47\n",
      "cab\tbus\t5.6\n",
      "house\tbarn\t4.33\n",
      "finger\tpalm\t3.33\n",
      "car\tbridge\t0.95\n",
      "effort\tdifficulty\t4.45\n",
      "fact\tinsight\t4.77\n",
      "job\tmanagement\t3.97\n",
      "cancer\tsickness\t7.93\n",
      "word\tnewspaper\t2.47\n",
      "composer\twriter\t6.58\n",
      "actor\tsinger\t4.52\n",
      "shelter\thut\t6.47\n",
      "bathroom\tkitchen\t3.1\n",
      "cabin\thut\t6.53\n",
      "door\tkitchen\t1.67\n",
      "value\tbelief\t7.07\n",
      "wisdom\tintelligence\t7.47\n",
      "ignorance\tintelligence\t1.5\n",
      "happiness\tluck\t2.38\n",
      "idea\tscheme\t6.75\n",
      "mood\temotion\t8.12\n",
      "happiness\tpeace\t6.03\n",
      "despair\tmisery\t7.22\n",
      "logic\tarithmetic\t3.97\n",
      "denial\tconfession\t1.03\n",
      "argument\tcriticism\t5.08\n",
      "aggression\thostility\t8.48\n",
      "hysteria\tconfusion\t6.33\n",
      "chemistry\ttheory\t3.17\n",
      "trial\tverdict\t3.33\n",
      "comfort\tsafety\t5.8\n",
      "confidence\tself\t3.12\n",
      "vision\tperception\t6.88\n",
      "era\tdecade\t5.4\n",
      "biography\tfiction\t1.38\n",
      "discussion\targument\t5.48\n",
      "code\tsymbol\t6.03\n",
      "danger\tdisease\t3.0\n",
      "accident\tcatastrophe\t5.9\n",
      "journey\ttrip\t8.88\n",
      "activity\tmovement\t7.15\n",
      "gossip\tnews\t5.22\n",
      "father\tgod\t3.57\n",
      "action\tcourse\t5.45\n",
      "fever\tillness\t7.65\n",
      "aviation\tflight\t8.18\n",
      "game\taction\t4.85\n",
      "molecule\tair\t3.05\n",
      "home\tstate\t2.58\n",
      "word\tliterature\t4.77\n",
      "adult\tguardian\t6.9\n",
      "newspaper\tinformation\t5.65\n",
      "communication\ttelevision\t5.6\n",
      "cousin\tuncle\t4.63\n",
      "author\treader\t1.6\n",
      "guy\tpartner\t3.57\n",
      "area\tcorner\t2.07\n",
      "ballad\tsong\t7.53\n",
      "wall\tdecoration\t2.62\n",
      "word\tpage\t2.92\n",
      "nurse\tscientist\t2.08\n",
      "politician\tpresident\t7.38\n",
      "president\tmayor\t5.68\n",
      "book\tessay\t4.72\n",
      "man\twarrior\t4.72\n",
      "article\tjournal\t6.18\n",
      "breakfast\tsupper\t4.4\n",
      "crowd\tparade\t3.93\n",
      "aisle\thallway\t6.75\n",
      "teacher\trabbi\t4.37\n",
      "hip\tlip\t1.43\n",
      "book\tarticle\t5.43\n",
      "room\tcell\t4.58\n",
      "box\tbooth\t3.8\n",
      "daughter\tkid\t4.17\n",
      "limb\tleg\t6.9\n",
      "liver\tlung\t2.7\n",
      "classroom\thallway\t2.0\n",
      "mountain\tledge\t3.73\n",
      "car\televator\t1.03\n",
      "bed\tcouch\t3.42\n",
      "clothes\tbutton\t2.3\n",
      "clothes\tcoat\t5.35\n",
      "kidney\torgan\t6.17\n",
      "apple\tsauce\t1.43\n",
      "chicken\tsteak\t3.73\n",
      "car\those\t0.87\n",
      "tobacco\tcigarette\t7.5\n",
      "student\tprofessor\t1.95\n",
      "baby\tdaughter\t5.0\n",
      "pipe\tcigar\t6.03\n",
      "milk\tjuice\t4.05\n",
      "box\tcigar\t1.25\n",
      "apartment\thotel\t3.33\n",
      "cup\tcone\t3.17\n",
      "horse\tox\t3.02\n",
      "throat\tnose\t2.8\n",
      "bone\tteeth\t4.17\n",
      "bone\telbow\t3.78\n",
      "bacon\tbean\t1.22\n",
      "cup\tjar\t5.13\n",
      "proof\tfact\t7.3\n",
      "appointment\tengagement\t6.75\n",
      "birthday\tyear\t1.67\n",
      "word\tclue\t2.53\n",
      "author\tcreator\t8.02\n",
      "atom\tcarbon\t3.1\n",
      "archbishop\tbishop\t7.05\n",
      "letter\tparagraph\t4.0\n",
      "page\tparagraph\t3.03\n",
      "steeple\tchapel\t7.08\n",
      "muscle\tbone\t3.65\n",
      "muscle\ttongue\t5.0\n",
      "boy\tsoldier\t2.15\n",
      "belly\tabdomen\t8.13\n",
      "guy\tgirl\t3.33\n",
      "bed\tchair\t3.5\n",
      "clothes\tjacket\t5.15\n",
      "gun\tknife\t3.65\n",
      "tin\tmetal\t5.63\n",
      "bottle\tcontainer\t7.93\n",
      "hen\tturkey\t6.13\n",
      "meat\tbread\t1.67\n",
      "arm\tbone\t3.83\n",
      "neck\tspine\t5.32\n",
      "apple\tlemon\t4.05\n",
      "agony\tgrief\t7.63\n",
      "assignment\ttask\t8.7\n",
      "night\tdawn\t2.95\n",
      "dinner\tsoup\t3.72\n",
      "calf\tbull\t4.93\n",
      "snow\tstorm\t4.8\n",
      "nail\thand\t3.42\n",
      "dog\thorse\t2.38\n",
      "arm\tneck\t1.58\n",
      "ball\tglove\t1.75\n",
      "flu\tfever\t6.08\n",
      "fee\tsalary\t3.72\n",
      "nerve\tbrain\t3.88\n",
      "beast\tanimal\t7.83\n",
      "dinner\tchicken\t2.85\n",
      "girl\tmaid\t2.93\n",
      "child\tboy\t5.75\n",
      "alcohol\twine\t7.42\n",
      "nose\tmouth\t3.73\n",
      "street\tcar\t2.38\n",
      "bell\tdoor\t2.2\n",
      "box\that\t1.3\n",
      "belief\timpression\t5.95\n",
      "bias\topinion\t5.6\n",
      "attention\tawareness\t8.73\n",
      "anger\tmood\t4.1\n",
      "elegance\tstyle\t5.72\n",
      "beauty\tage\t1.58\n",
      "book\ttheme\t2.58\n",
      "friend\tmother\t2.53\n",
      "vitamin\tiron\t5.55\n",
      "car\tfactory\t2.75\n",
      "pact\tcondition\t2.45\n",
      "chapter\tchoice\t0.48\n",
      "arithmetic\trhythm\t2.35\n",
      "winner\tpresence\t1.08\n",
      "belief\tflower\t0.4\n",
      "winner\tgoal\t3.23\n",
      "trick\tsize\t0.48\n",
      "choice\tvein\t0.98\n",
      "hymn\tconquest\t0.68\n",
      "endurance\tband\t0.4\n",
      "jail\tchoice\t1.08\n",
      "condition\tboy\t0.48\n",
      "flower\tendurance\t0.4\n",
      "hole\tagreement\t0.3\n",
      "doctor\ttemper\t0.48\n",
      "fraternity\tdoor\t0.68\n",
      "task\twoman\t0.68\n",
      "fraternity\tbaseball\t0.88\n",
      "cent\tsize\t0.4\n",
      "presence\tdoor\t0.48\n",
      "mouse\tmanagement\t0.48\n",
      "task\thighway\t0.48\n",
      "liquor\tcentury\t0.4\n",
      "task\tstraw\t0.68\n",
      "island\ttask\t0.3\n",
      "night\tchapter\t0.48\n",
      "pollution\tpresident\t0.68\n",
      "gun\ttrick\t0.48\n",
      "bath\ttrick\t0.58\n",
      "diet\tapple\t1.18\n",
      "cent\twife\t0.58\n",
      "chapter\ttail\t0.3\n",
      "course\tstomach\t0.58\n",
      "hymn\tstraw\t0.4\n",
      "dentist\tcolonel\t0.4\n",
      "wife\tstraw\t0.4\n",
      "hole\twife\t0.68\n",
      "pupil\tpresident\t0.78\n",
      "bath\twife\t0.48\n",
      "people\tcent\t0.48\n",
      "formula\tlog\t1.77\n",
      "woman\tfur\t0.58\n",
      "apple\tsunshine\t0.58\n",
      "gun\tdawn\t1.18\n",
      "meal\twaist\t0.98\n",
      "camera\tpresident\t0.48\n",
      "liquor\tband\t0.68\n",
      "stomach\tvein\t2.35\n",
      "gun\tfur\t0.3\n",
      "couch\tbaseball\t0.88\n",
      "worker\tcamera\t0.68\n",
      "deck\tmouse\t0.48\n",
      "rice\tboy\t0.4\n",
      "people\tgun\t0.68\n",
      "cliff\ttail\t0.3\n",
      "ankle\twindow\t0.3\n",
      "princess\tisland\t0.3\n",
      "container\tmouse\t0.3\n",
      "wagon\tcontainer\t2.65\n",
      "people\tballoon\t0.48\n",
      "dollar\tpeople\t0.4\n",
      "bath\tballoon\t0.4\n",
      "stomach\tbedroom\t0.4\n",
      "bicycle\tbedroom\t0.4\n",
      "log\tbath\t0.4\n",
      "bowl\ttail\t0.48\n",
      "go\tcome\t2.42\n",
      "take\tsteal\t6.18\n",
      "listen\thear\t8.17\n",
      "think\trationalize\t8.25\n",
      "occur\thappen\t9.32\n",
      "vanish\tdisappear\t9.8\n",
      "multiply\tdivide\t1.75\n",
      "plead\tbeg\t9.08\n",
      "begin\toriginate\t8.2\n",
      "protect\tdefend\t9.13\n",
      "kill\tdestroy\t5.9\n",
      "create\tmake\t8.72\n",
      "accept\treject\t0.83\n",
      "ignore\tavoid\t6.87\n",
      "carry\tbring\t5.8\n",
      "leave\tenter\t0.95\n",
      "choose\telect\t7.62\n",
      "lose\tfail\t7.33\n",
      "encourage\tdiscourage\t1.58\n",
      "achieve\taccomplish\t8.57\n",
      "make\tconstruct\t8.33\n",
      "listen\tobey\t4.93\n",
      "inform\tnotify\t9.25\n",
      "receive\tgive\t1.47\n",
      "borrow\tbeg\t2.62\n",
      "take\tobtain\t7.1\n",
      "advise\trecommend\t8.1\n",
      "imitate\tportray\t6.75\n",
      "win\tsucceed\t7.9\n",
      "think\tdecide\t5.13\n",
      "greet\tmeet\t6.17\n",
      "agree\targue\t0.77\n",
      "enjoy\tentertain\t5.92\n",
      "destroy\tmake\t1.6\n",
      "save\tprotect\t6.58\n",
      "give\tlend\t7.22\n",
      "understand\tknow\t7.47\n",
      "take\treceive\t5.08\n",
      "accept\tacknowledge\t6.88\n",
      "decide\tchoose\t8.87\n",
      "accept\tbelieve\t6.75\n",
      "keep\tpossess\t8.27\n",
      "roam\twander\t8.83\n",
      "succeed\tfail\t0.83\n",
      "spend\tsave\t0.55\n",
      "leave\tgo\t7.63\n",
      "come\tattend\t8.1\n",
      "know\tbelieve\t5.5\n",
      "gather\tmeet\t7.3\n",
      "make\tearn\t7.62\n",
      "forget\tignore\t3.07\n",
      "multiply\tadd\t2.7\n",
      "shrink\tgrow\t0.23\n",
      "arrive\tleave\t1.33\n",
      "succeed\ttry\t3.98\n",
      "accept\tdeny\t1.75\n",
      "arrive\tcome\t7.05\n",
      "agree\tdiffer\t1.05\n",
      "send\treceive\t1.08\n",
      "win\tdominate\t5.68\n",
      "add\tdivide\t2.3\n",
      "kill\tchoke\t4.92\n",
      "acquire\tget\t8.82\n",
      "participate\tjoin\t7.7\n",
      "leave\tremain\t2.53\n",
      "go\tenter\t4.0\n",
      "take\tcarry\t5.23\n",
      "forget\tlearn\t1.18\n",
      "appoint\telect\t8.17\n",
      "engage\tmarry\t5.43\n",
      "ask\tpray\t3.72\n",
      "go\tsend\t3.75\n",
      "take\tdeliver\t4.37\n",
      "speak\thear\t3.02\n",
      "analyze\tevaluate\t8.03\n",
      "argue\trationalize\t4.2\n",
      "lose\tkeep\t1.05\n",
      "compare\tanalyze\t8.1\n",
      "disorganize\torganize\t1.45\n",
      "go\tallow\t3.62\n",
      "take\tpossess\t7.2\n",
      "learn\tlisten\t3.88\n",
      "destroy\tconstruct\t0.92\n",
      "create\tbuild\t8.48\n",
      "steal\tbuy\t1.13\n",
      "kill\thang\t4.45\n",
      "forget\tknow\t0.92\n",
      "create\timagine\t5.13\n",
      "do\thappen\t4.23\n",
      "win\taccomplish\t7.85\n",
      "give\tdeny\t1.43\n",
      "deserve\tearn\t5.8\n",
      "get\tput\t1.98\n",
      "locate\tfind\t8.73\n",
      "appear\tattend\t6.28\n",
      "know\tcomprehend\t7.63\n",
      "pretend\timagine\t8.47\n",
      "satisfy\tplease\t7.67\n",
      "cherish\tkeep\t4.85\n",
      "argue\tdiffer\t5.15\n",
      "overcome\tdominate\t6.25\n",
      "behave\tobey\t7.3\n",
      "cooperate\tparticipate\t6.43\n",
      "achieve\ttry\t4.42\n",
      "fail\tdiscourage\t3.33\n",
      "begin\tquit\t1.28\n",
      "say\tparticipate\t3.82\n",
      "come\tbring\t2.42\n",
      "declare\tannounce\t9.08\n",
      "read\tcomprehend\t4.7\n",
      "take\tleave\t2.47\n",
      "proclaim\tannounce\t8.18\n",
      "acquire\tobtain\t8.57\n",
      "conclude\tdecide\t7.75\n",
      "please\tplead\t2.98\n",
      "argue\tprove\t4.83\n",
      "ask\tplead\t6.47\n",
      "find\tdisappear\t0.77\n",
      "inspect\texamine\t8.75\n",
      "verify\tjustify\t4.08\n",
      "assume\tpredict\t4.85\n",
      "learn\tevaluate\t4.17\n",
      "argue\tjustify\t5.0\n",
      "make\tbecome\t4.77\n",
      "discover\toriginate\t4.83\n",
      "achieve\tsucceed\t7.5\n",
      "give\tput\t3.65\n",
      "understand\tlisten\t4.68\n",
      "expand\tgrow\t8.27\n",
      "borrow\tsell\t1.73\n",
      "keep\tprotect\t5.4\n",
      "explain\tprove\t4.1\n",
      "assume\tpretend\t3.72\n",
      "agree\tplease\t4.13\n",
      "forgive\tforget\t3.92\n",
      "clarify\texplain\t8.33\n",
      "understand\tforgive\t4.87\n",
      "remind\tforget\t0.87\n",
      "get\tremain\t1.6\n",
      "realize\tdiscover\t7.47\n",
      "require\tinquire\t1.82\n",
      "ignore\task\t1.07\n",
      "think\tinquire\t4.77\n",
      "reject\tavoid\t4.78\n",
      "argue\tpersuade\t6.23\n",
      "pursue\tpersuade\t3.17\n",
      "accept\tforgive\t3.73\n",
      "do\tquit\t1.17\n",
      "investigate\texamine\t8.1\n",
      "discuss\texplain\t6.67\n",
      "owe\tlend\t2.32\n",
      "explore\tdiscover\t8.48\n",
      "complain\targue\t4.8\n",
      "withdraw\treject\t6.38\n",
      "keep\tborrow\t2.25\n",
      "beg\task\t6.0\n",
      "arrange\torganize\t8.27\n",
      "reduce\tshrink\t8.02\n",
      "speak\tacknowledge\t4.67\n",
      "give\tborrow\t2.22\n",
      "kill\tdefend\t2.63\n",
      "disappear\tshrink\t5.8\n",
      "deliver\tcarry\t3.88\n",
      "breathe\tchoke\t1.37\n",
      "acknowledge\tnotify\t5.3\n",
      "become\tseem\t2.63\n",
      "pretend\tseem\t4.68\n",
      "accomplish\tbecome\t4.0\n",
      "contemplate\tthink\t8.82\n",
      "determine\tpredict\t5.8\n",
      "please\tentertain\t5.0\n",
      "remain\tretain\t5.75\n",
      "pretend\tportray\t7.03\n",
      "forget\tretain\t0.63\n",
      "want\tchoose\t4.78\n",
      "lose\tget\t0.77\n",
      "try\tthink\t2.62\n",
      "become\tappear\t4.77\n",
      "leave\tignore\t4.42\n",
      "accept\trecommend\t2.75\n",
      "leave\twander\t3.57\n",
      "keep\tgive\t1.05\n",
      "give\tallow\t5.15\n",
      "bring\tsend\t2.97\n",
      "absorb\tlearn\t5.48\n",
      "acquire\tfind\t6.38\n",
      "leave\tappear\t0.97\n",
      "create\tdestroy\t0.63\n",
      "begin\tgo\t7.42\n",
      "get\tbuy\t5.08\n",
      "collect\tsave\t6.67\n",
      "replace\trestore\t5.73\n",
      "join\tadd\t8.1\n",
      "join\tmarry\t5.35\n",
      "accept\tdeliver\t1.58\n",
      "attach\tjoin\t7.75\n",
      "put\thang\t3.0\n",
      "go\tsell\t0.97\n",
      "communicate\tpray\t3.55\n",
      "give\tsteal\t0.5\n",
      "add\tbuild\t4.92\n",
      "bring\trestore\t2.62\n",
      "comprehend\tsatisfy\t2.55\n",
      "portray\tdecide\t1.18\n",
      "organize\tbecome\t1.77\n",
      "give\tknow\t0.88\n",
      "say\tverify\t4.9\n",
      "cooperate\tjoin\t5.18\n",
      "arrange\trequire\t0.98\n",
      "borrow\twant\t1.77\n",
      "investigate\tpursue\t7.15\n",
      "ignore\texplore\t0.4\n",
      "bring\tcomplain\t0.98\n",
      "enter\towe\t0.68\n",
      "portray\tnotify\t0.78\n",
      "remind\tsell\t0.4\n",
      "absorb\tpossess\t5.0\n",
      "join\tacquire\t2.85\n",
      "send\tattend\t1.67\n",
      "gather\tattend\t4.8\n",
      "absorb\twithdraw\t2.97\n",
      "attend\tarrive\t6.08\n"
     ]
    }
   ],
   "source": [
    "# simlex_data = wget.download(\"https://fh295.github.io/SimLex-999.zip\")\n",
    "simlex_data = 'SimLex-999.zip'\n",
    "\n",
    "simple_simlex_path = \"data/simlex999/simlex999.tsv\"\n",
    "\n",
    "simplex_pairs = dict()\n",
    "with zipfile.ZipFile(simlex_data, 'r') as zip, open(simple_simlex_path, \"wb\") as fw:\n",
    "   with zip.open('SimLex-999/SimLex-999.txt') as myfile:\n",
    "    next(myfile)\n",
    "    for line in myfile:\n",
    "      w1, w2, pos, score, *_ = line.strip().split()\n",
    "      w1 = w1.decode('utf-8')\n",
    "      w2 = w2.decode('utf-8')\n",
    "      score = float(score)\n",
    "      simplex_pairs[(w1, w2)] = score\n",
    "      fw.write(f'{w1}\\t{w2}\\t{score}\\n'.encode('utf-8'))\n",
    "      print(f'{w1}\\t{w2}\\t{score}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model on the SimLex999 dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correlation_score(model, word_pair2score, print_warning=True):\n",
    "    human_scores = []\n",
    "    system_scores = []\n",
    "    count_print_warnings = 0\n",
    "    for (w1, w2), score in word_pair2score.items():\n",
    "        if (w1 not in model) or (w2 not in model):\n",
    "            system_scores.append(-1)\n",
    "            human_scores.append(score)\n",
    "            if print_warning:\n",
    "                print(f\"({count_print_warnings:6d}) | WARNING ({w1} and {w2}) are not present in the embedding model!!\" )\n",
    "                count_print_warnings += 1\n",
    "            continue\n",
    "        system_similarity = model.similarity(w1, w2)\n",
    "        human_scores.append(score)\n",
    "        system_scores.append(system_similarity)\n",
    "\n",
    "    human_scores = np.array(human_scores)\n",
    "    system_scores = np.array(system_scores)\n",
    "    pearson_r, _ = scipy.stats.pearsonr(human_scores, system_scores)    # Pearson's r\n",
    "    spearman_rho = scipy.stats.spearmanr(human_scores, system_scores).statistic   # Spearman's rho\n",
    "    \n",
    "    return pearson_r, spearman_rho\n",
    "\n",
    "\n",
    "\n",
    "def compute_semantic_correlation_score(model, senses2score,  print_warning=True):\n",
    "    human_scores = []\n",
    "    system_scores = []\n",
    "    for (senses_1, senses_2), score in senses2score.items():\n",
    "        senses_1_in_model = [s for s in senses_1 if s in model]\n",
    "        senses_2_in_model = [s for s in senses_2 if s in model]\n",
    "\n",
    "        if len(senses_1_in_model) == 0 or len(senses_1_in_model) == 0:\n",
    "            # sense is not present in the model\n",
    "            s1_str = \" \".join(senses_1)\n",
    "            s2_str = \" \".join(senses_2)\n",
    "            if print_warning:\n",
    "                print(f\"WARNING ({s1_str} and {s2_str}) are not present in the embedding model!!\" )\n",
    "            system_scores.append(-1)\n",
    "        # Calculate semantic similarities between all pairs of senses\n",
    "        all_similarities = []\n",
    "        for s1 in senses_1_in_model:\n",
    "            for s2 in senses_2_in_model:\n",
    "                all_similarities.append(model.similarity(s1, s2))\n",
    "\n",
    "        system_similarity = sum(all_similarities) / len(all_similarities)\n",
    "        human_scores.append(score)\n",
    "        system_scores.append(system_similarity)\n",
    "    human_scores = np.array(human_scores)\n",
    "    system_scores = np.array(system_scores)\n",
    "    # Calculate Pearson's r (Pearson correlation coefficient) and Spearman's rho (Spearman rank correlation coefficient)\n",
    "    pearson_r, _ = scipy.stats.pearsonr(human_scores, system_scores)    # Pearson's r\n",
    "    spearman_rho = scipy.stats.spearmanr(human_scores, system_scores).statistic   # Spearman's rho\n",
    "    return pearson_r, spearman_rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson and Spearman scores: (0.24703068953450824, 0.183647205126096)\n"
     ]
    }
   ],
   "source": [
    "word_embeddings_model_score = compute_correlation_score(word_embeddings_model.wv, simplex_pairs, print_warning=True)\n",
    "print(f'Pearson and Spearman scores: {word_embeddings_model_score}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can search for the best hyperparameters.\n",
    "- The `vector_size` determines the dimensionality of the word vectors or embeddings.\n",
    "- The `window_size` defines the context window for the Word2Vec model.\n",
    "- The `min_count` parameter is used to control the minimum count of a word in the corpus for it to be considered during the training of the Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 22/63 [16:26<36:10, 52.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(     0) | WARNING (bubble and suds) are not present in the embedding model!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 23/63 [16:51<29:48, 44.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(     0) | WARNING (bubble and suds) are not present in the embedding model!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 24/63 [17:17<25:18, 38.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(     0) | WARNING (bubble and suds) are not present in the embedding model!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 25/63 [17:45<22:40, 35.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(     0) | WARNING (bubble and suds) are not present in the embedding model!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 26/63 [18:15<20:57, 33.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(     0) | WARNING (bubble and suds) are not present in the embedding model!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 27/63 [18:46<19:52, 33.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(     0) | WARNING (bubble and suds) are not present in the embedding model!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 28/63 [19:16<18:48, 32.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(     0) | WARNING (bubble and suds) are not present in the embedding model!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 29/63 [19:47<17:58, 31.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(     0) | WARNING (bubble and suds) are not present in the embedding model!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 30/63 [20:18<17:18, 31.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(     0) | WARNING (bubble and suds) are not present in the embedding model!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 31/63 [20:47<16:28, 30.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(     0) | WARNING (bubble and suds) are not present in the embedding model!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 32/63 [21:18<15:56, 30.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(     0) | WARNING (bubble and suds) are not present in the embedding model!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 33/63 [21:50<15:32, 31.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(     0) | WARNING (bubble and suds) are not present in the embedding model!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 34/63 [22:22<15:12, 31.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(     0) | WARNING (bubble and suds) are not present in the embedding model!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 35/63 [22:57<15:09, 32.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(     0) | WARNING (bubble and suds) are not present in the embedding model!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 36/63 [23:33<15:06, 33.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(     0) | WARNING (bubble and suds) are not present in the embedding model!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 37/63 [24:15<15:37, 36.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(     0) | WARNING (bubble and suds) are not present in the embedding model!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 38/63 [24:58<15:55, 38.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(     0) | WARNING (bubble and suds) are not present in the embedding model!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 39/63 [25:43<16:07, 40.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(     0) | WARNING (bubble and suds) are not present in the embedding model!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 40/63 [26:36<16:54, 44.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(     0) | WARNING (bubble and suds) are not present in the embedding model!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 41/63 [27:33<17:32, 47.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(     0) | WARNING (bubble and suds) are not present in the embedding model!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 42/63 [28:28<17:31, 50.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(     0) | WARNING (bubble and suds) are not present in the embedding model!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 43/63 [28:51<13:57, 41.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(     0) | WARNING (bubble and suds) are not present in the embedding model!!\n",
      "(     1) | WARNING (disorganize and organize) are not present in the embedding model!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 44/63 [29:15<11:33, 36.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(     0) | WARNING (bubble and suds) are not present in the embedding model!!\n",
      "(     1) | WARNING (disorganize and organize) are not present in the embedding model!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 45/63 [29:39<09:50, 32.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(     0) | WARNING (bubble and suds) are not present in the embedding model!!\n",
      "(     1) | WARNING (disorganize and organize) are not present in the embedding model!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 46/63 [30:05<08:43, 30.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(     0) | WARNING (bubble and suds) are not present in the embedding model!!\n",
      "(     1) | WARNING (disorganize and organize) are not present in the embedding model!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 47/63 [30:33<08:01, 30.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(     0) | WARNING (bubble and suds) are not present in the embedding model!!\n",
      "(     1) | WARNING (disorganize and organize) are not present in the embedding model!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 48/63 [31:03<07:26, 29.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(     0) | WARNING (bubble and suds) are not present in the embedding model!!\n",
      "(     1) | WARNING (disorganize and organize) are not present in the embedding model!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 49/63 [31:29<06:42, 28.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(     0) | WARNING (bubble and suds) are not present in the embedding model!!\n",
      "(     1) | WARNING (disorganize and organize) are not present in the embedding model!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 50/63 [31:58<06:15, 28.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(     0) | WARNING (bubble and suds) are not present in the embedding model!!\n",
      "(     1) | WARNING (disorganize and organize) are not present in the embedding model!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 51/63 [32:29<05:52, 29.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(     0) | WARNING (bubble and suds) are not present in the embedding model!!\n",
      "(     1) | WARNING (disorganize and organize) are not present in the embedding model!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 52/63 [32:59<05:26, 29.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(     0) | WARNING (bubble and suds) are not present in the embedding model!!\n",
      "(     1) | WARNING (disorganize and organize) are not present in the embedding model!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 53/63 [33:30<05:01, 30.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(     0) | WARNING (bubble and suds) are not present in the embedding model!!\n",
      "(     1) | WARNING (disorganize and organize) are not present in the embedding model!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 54/63 [34:02<04:35, 30.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(     0) | WARNING (bubble and suds) are not present in the embedding model!!\n",
      "(     1) | WARNING (disorganize and organize) are not present in the embedding model!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 55/63 [34:33<04:06, 30.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(     0) | WARNING (bubble and suds) are not present in the embedding model!!\n",
      "(     1) | WARNING (disorganize and organize) are not present in the embedding model!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 56/63 [35:05<03:38, 31.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(     0) | WARNING (bubble and suds) are not present in the embedding model!!\n",
      "(     1) | WARNING (disorganize and organize) are not present in the embedding model!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 57/63 [35:40<03:13, 32.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(     0) | WARNING (bubble and suds) are not present in the embedding model!!\n",
      "(     1) | WARNING (disorganize and organize) are not present in the embedding model!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 58/63 [36:24<02:58, 35.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(     0) | WARNING (bubble and suds) are not present in the embedding model!!\n",
      "(     1) | WARNING (disorganize and organize) are not present in the embedding model!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 59/63 [37:08<02:33, 38.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(     0) | WARNING (bubble and suds) are not present in the embedding model!!\n",
      "(     1) | WARNING (disorganize and organize) are not present in the embedding model!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 60/63 [37:55<02:02, 40.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(     0) | WARNING (bubble and suds) are not present in the embedding model!!\n",
      "(     1) | WARNING (disorganize and organize) are not present in the embedding model!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 61/63 [38:50<01:30, 45.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(     0) | WARNING (bubble and suds) are not present in the embedding model!!\n",
      "(     1) | WARNING (disorganize and organize) are not present in the embedding model!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 62/63 [39:46<00:48, 48.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(     0) | WARNING (bubble and suds) are not present in the embedding model!!\n",
      "(     1) | WARNING (disorganize and organize) are not present in the embedding model!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [40:44<00:00, 38.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(     0) | WARNING (bubble and suds) are not present in the embedding model!!\n",
      "(     1) | WARNING (disorganize and organize) are not present in the embedding model!!\n",
      "Best parameters: {'min_count': 1, 'vector_size': 10, 'window': 5}\n",
      "Best score: 6.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set up your grid of hyperparameters\n",
    "param_grid = {\n",
    "    'vector_size': [10, 25, 50, 75, 100, 200, 300],\n",
    "    'window': [5, 10, 15],\n",
    "    'min_count': [1, 5, 10]\n",
    "}\n",
    "\n",
    "best_score = 0\n",
    "best_params = None\n",
    "\n",
    "# Perform grid search\n",
    "for params in tqdm(ParameterGrid(param_grid)):\n",
    "\n",
    "    # Train Word2Vec model\n",
    "    word_embeddings_model = Word2Vec(word_sentences, **params)\n",
    "    \n",
    "    pearson_score, spearman_score = compute_correlation_score(word_embeddings_model.wv, simplex_pairs, print_warning=False)\n",
    "\n",
    "    # Normalize the scores [-1, 1] -> [0, 1]\n",
    "    pearson_score_norm = (pearson_score + 1) / 2\n",
    "    spearman_score_norm = (spearman_score + 1) / 2\n",
    "\n",
    "    # Compute the combined score using a weighted average\n",
    "    alpha = 0.5\n",
    "    combined_score = alpha * pearson_score_norm + (1 - alpha) * spearman_score_norm\n",
    "\n",
    "    print(f'Params {params} gives pearson {pearson_score} ({pearson_score_norm}) and spearman {spearman_score} ({spearman_score_norm}) scores ({combined_score})')\n",
    "\n",
    "    # Update best parameters if current score is better\n",
    "    if combined_score > best_score:\n",
    "        best_score = combined_score\n",
    "        best_params = params\n",
    "\n",
    "# Print and use the best parameters\n",
    "print(f'Best parameters: {best_params}')\n",
    "print(f'Best score: {best_score}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sense embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The returned dictionary should be similar to previous word_pair2score \n",
    "# but instead of words we consider the senses from the dataset \n",
    "# associated with this words\n",
    "def load_semantic_simplex(path):\n",
    "  senses2score = dict()\n",
    "  with open(path) as fr:\n",
    "    next(fr)\n",
    "    for line in fr:\n",
    "      chunks = line.strip().split()\n",
    "      w1 = chunks[0]\n",
    "      w2 = chunks[1]\n",
    "      sim_lex_score = chunks[3]\n",
    "      senses_w1 = chunks[10].split(\",\")\n",
    "      senses_w2 = chunks[11].split(\",\")\n",
    "      senses2score[tuple(tuple(senses_w1), tuple(senses_w2))] = sim_lex_score\n",
    "\n",
    "  return senses2score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 10\n",
    "window = 2\n",
    "min_count = 1 \n",
    "\n",
    "sense_sentences = [[token for token in sentence.split()] for sentence in sense_sentences]\n",
    "sense_embeddings_model = gensim.models.Word2Vec(sense_sentences, vector_size=vector_size, window=window, min_count=min_count)\n",
    "sense_embeddings_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute correlation between human scores and our similarities\n",
    "\n",
    "Compute the non-semantic (between word embeddings, e.g. word2vec) and semantic similarity scores (e.g. word2vec on our sense annotated corpus) for all SimLex pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings_evaluation = compute_correlation_score(model.wv, simplex_pairs, print_warning=False)\n",
    "sense_embeddings_evaluation = compute_correlation_score(model.wv, simplex_pairs, print_warning=False)\n",
    "model_pretrained_evaluation = compute_correlation_score(model_pretrained, simplex_pairs)\n",
    "\n",
    "print(f'Bla Bla Bla')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
