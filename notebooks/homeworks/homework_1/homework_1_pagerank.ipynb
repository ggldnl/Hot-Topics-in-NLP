{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wget in /home/daniel/.local/lib/python3.10/site-packages (3.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy in /home/daniel/.local/lib/python3.10/site-packages (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (0.3.3)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/lib/python3/dist-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (2.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (1.24.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/daniel/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /home/daniel/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/daniel/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.8.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/daniel/.local/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/daniel/.local/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/daniel/.local/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/daniel/.local/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/daniel/.local/lib/python3.10/site-packages (from jinja2->spacy) (2.1.3)\n",
      "/bin/bash: line 1: python: command not found\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gensim in /home/daniel/.local/lib/python3.10/site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/daniel/.local/lib/python3.10/site-packages (from gensim) (1.24.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /home/daniel/.local/lib/python3.10/site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/daniel/.local/lib/python3.10/site-packages (from gensim) (6.4.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/daniel/.local/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /home/daniel/.local/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/daniel/.local/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/daniel/.local/lib/python3.10/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /home/daniel/.local/lib/python3.10/site-packages (from nltk) (4.65.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /home/daniel/.local/lib/python3.10/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /home/daniel/.local/lib/python3.10/site-packages (from scikit-learn) (1.24.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/daniel/.local/lib/python3.10/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/daniel/.local/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/daniel/.local/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install wget # to download data\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install gensim\n",
    "!pip install nltk\n",
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import wget\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import scipy.stats\n",
    "import tqdm\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import jsonlines\n",
    "\n",
    "import zipfile\n",
    "\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the preprocessed dictionary from file\n",
    "input_file = 'preprocessed_dataset_v2.jsonl'\n",
    "# input_file = None\n",
    "\n",
    "non_semantic_tokens = []\n",
    "semantic_tokens = []\n",
    "\n",
    "if input_file is not None:\n",
    "\n",
    "    \"\"\"\n",
    "    with jsonlines.open(input_file) as reader:\n",
    "        for line in tqdm(reader.iter()):\n",
    "            read_non_semantic_sentences.append(line['non_semantic'])\n",
    "            read_semantic_sentences.append(line['semantic'])\n",
    "\n",
    "    non_semantic_sentences = [[token for token in sentence.split()] for sentence in read_non_semantic_sentences]\n",
    "    semantic_sentences = [[token for token in sentence.split()] for sentence in read_semantic_sentences]\n",
    "    \"\"\"\n",
    "\n",
    "    with jsonlines.open(input_file, 'r') as reader:\n",
    "        for line in tqdm(reader):\n",
    "            non_semantic_tokens_for_sentence = line.get(\"non_semantic\", [])\n",
    "            semantic_tokens_for_sentence = line.get(\"semantic\", [])\n",
    "            non_semantic_tokens.append(non_semantic_tokens_for_sentence)\n",
    "            semantic_tokens.append(semantic_tokens_for_sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def build_word_graph(corpus):\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add nodes for each unique word\n",
    "    for sentence in tqdm(corpus):\n",
    "        for word in sentence:\n",
    "            if word not in G:\n",
    "                G.add_node(word)\n",
    "\n",
    "    # Add edges based on co-occurrence in sentences\n",
    "    for sentence in tqdm(corpus):\n",
    "        for i in range(len(sentence)):\n",
    "            for j in range(i+1, len(sentence)):\n",
    "                if G.has_edge(sentence[i], sentence[j]):\n",
    "                    G[sentence[i]][sentence[j]]['weight'] += 1\n",
    "                else:\n",
    "                    G.add_edge(sentence[i], sentence[j], weight=1)\n",
    "\n",
    "    return G\n",
    "\n",
    "# Example corpus\n",
    "custom_corpus = [['apple', 'orange', 'banana'], ['apple', 'pear', 'kiwi'], ['apple', 'banana', 'kiwi', 'orange']]\n",
    "\n",
    "# Build the word graph\n",
    "word_graph = build_word_graph(custom_corpus)\n",
    "\n",
    "# Accessing edge weights between two words\n",
    "edge_weight = word_graph['apple']['orange']['weight']\n",
    "print(f\"Weight between 'apple' and 'orange': {edge_weight}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_graph(graph):\n",
    "\n",
    "    # Clear the graph\n",
    "    plt.cla()\n",
    "\n",
    "    pos = nx.spring_layout(graph)  # Positions for all nodes\n",
    "\n",
    "    # Draw nodes\n",
    "    nx.draw_networkx_nodes(graph, pos, node_size=700)\n",
    "\n",
    "    # Draw edges\n",
    "    nx.draw_networkx_edges(graph, pos, width=2, alpha=0.6)\n",
    "\n",
    "    # Draw labels\n",
    "    nx.draw_networkx_labels(graph, pos, font_size=10, font_family=\"sans-serif\")\n",
    "\n",
    "    # Draw edge labels (weights)\n",
    "    edge_labels = nx.get_edge_attributes(graph, 'weight')\n",
    "    nx.draw_networkx_edge_labels(graph, pos, edge_labels=edge_labels)\n",
    "\n",
    "    # Display the graph\n",
    "    plt.title(\"Word Similarity Graph\")\n",
    "    plt.axis('off')  # Turn off the axis\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "display_graph(word_graph)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PageRank\n",
    "\n",
    "Given a graph, ranks nodes according to their relative structural importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def pagerank(graph, alpha=0.85, max_iter=100, tol=1e-6):\n",
    "    # Initialize node scores\n",
    "    scores = {node: 1.0 / len(graph) for node in graph.nodes}\n",
    "\n",
    "    # PageRank iteration\n",
    "    for _ in range(max_iter):\n",
    "        prev_scores = scores.copy()\n",
    "        for node in graph.nodes:\n",
    "            rank_sum = sum(graph[neighbor][node].get('weight', 1.0) * prev_scores[neighbor] for neighbor in graph.neighbors(node))\n",
    "            scores[node] = (1 - alpha) + alpha * rank_sum\n",
    "\n",
    "        # Check for convergence\n",
    "        if all(abs(scores[node] - prev_scores[node]) < tol for node in graph.nodes):\n",
    "            break\n",
    "\n",
    "    return scores\n",
    "\n",
    "# Example usage\n",
    "word_scores = pagerank(word_graph)\n",
    "\n",
    "# Print word scores\n",
    "print(\"Word Scores:\")\n",
    "for word, score in word_scores.items():\n",
    "    print(f\"{word}: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def pagerank_similarity(word1, word2, graph):\n",
    "    # Get the edge weights between the two words\n",
    "    edge_weight_word1 = graph[word1][word2].get('weight', 0.0)\n",
    "    edge_weight_word2 = graph[word2][word1].get('weight', 0.0)\n",
    "\n",
    "    # Create vectors from the edge weights\n",
    "    vector1 = [edge_weight_word1, edge_weight_word2]\n",
    "    vector2 = [edge_weight_word2, edge_weight_word1]\n",
    "\n",
    "    # Reshape the vectors for cosine similarity calculation\n",
    "    vectors = [vector1, vector2]\n",
    "    similarity_matrix = cosine_similarity(vectors)\n",
    "\n",
    "    # The similarity score is at position (0, 1) or (1, 0) in the similarity matrix\n",
    "    similarity_score = similarity_matrix[0, 1]\n",
    "\n",
    "    return similarity_score\n",
    "\n",
    "def get_pagerank_similarity_scores(graph):\n",
    "    \n",
    "    # Extracting edge weights as a matrix\n",
    "    edge_weights_matrix = nx.to_numpy_matrix(graph, weight='weight')\n",
    "\n",
    "    # Computing cosine similarity between word vectors\n",
    "    similarity_matrix = cosine_similarity(edge_weights_matrix)\n",
    "\n",
    "    # Creating a dictionary to store word similarities\n",
    "    word_similarity = {}\n",
    "    words = list(graph.nodes)\n",
    "\n",
    "    # Filling the dictionary with similarity scores\n",
    "    for i in range(len(words)):\n",
    "        for j in range(i+1, len(words)):\n",
    "            word_similarity[(words[i], words[j])] = similarity_matrix[i, j]\n",
    "\n",
    "    return word_similarity\n",
    "\n",
    "def pagerank_similarity_from_scores(word1, word2, word_similarity_scores):\n",
    "    # Ensure words are in alphabetical order to maintain consistency\n",
    "    word_pair = tuple(sorted([word1, word2]))\n",
    "\n",
    "    # Check if the similarity score is available\n",
    "    if word_pair in word_similarity_scores:\n",
    "        return word_similarity_scores[word_pair]\n",
    "    else:\n",
    "        return 0.0  # Return 0 if the pair is not found (no similarity)\n",
    "\n",
    "# Example usage\n",
    "word1 = 'apple'\n",
    "word2 = 'orange'\n",
    "similarity_score = pagerank_similarity(word1, word2, word_graph)\n",
    "\n",
    "print(f\"Similarity between '{word1}' and '{word2}': {similarity_score}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try the simple PageRank implementation on our corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_graph = build_word_graph(non_semantic_tokens)\n",
    "\n",
    "pagerank_scores = get_pagerank_similarity_scores(word_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = 'old'\n",
    "word2 = 'new'\n",
    "similarity_score = pagerank_similarity_from_scores(word1, word2, pagerank_scores)\n",
    "\n",
    "print(f\"Similarity between '{word1}' and '{word2}': {similarity_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
