{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wget in /home/daniel/.local/lib/python3.10/site-packages (3.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy in /home/daniel/.local/lib/python3.10/site-packages (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (0.3.3)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/lib/python3/dist-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (2.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/daniel/.local/lib/python3.10/site-packages (from spacy) (1.24.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/daniel/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /home/daniel/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/daniel/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.8.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/daniel/.local/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/daniel/.local/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/daniel/.local/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/daniel/.local/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/daniel/.local/lib/python3.10/site-packages (from jinja2->spacy) (2.1.3)\n",
      "/bin/bash: line 1: python: command not found\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gensim in /home/daniel/.local/lib/python3.10/site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/daniel/.local/lib/python3.10/site-packages (from gensim) (1.24.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /home/daniel/.local/lib/python3.10/site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/daniel/.local/lib/python3.10/site-packages (from gensim) (6.4.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/daniel/.local/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /home/daniel/.local/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/daniel/.local/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/daniel/.local/lib/python3.10/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /home/daniel/.local/lib/python3.10/site-packages (from nltk) (4.65.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /home/daniel/.local/lib/python3.10/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /home/daniel/.local/lib/python3.10/site-packages (from scikit-learn) (1.24.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/daniel/.local/lib/python3.10/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/daniel/.local/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/daniel/.local/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install wget # to download data\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install gensim\n",
    "!pip install nltk\n",
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import wget\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import scipy.stats\n",
    "import tqdm\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import jsonlines\n",
    "\n",
    "import zipfile\n",
    "\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implicit representations\n",
    "\n",
    "1. Create an embedding for the words using the (cleaned) Mosaico dataset by fitting a model.\n",
    "2. Replace WordNet senses for each word and then create embeddings as before.\n",
    "\n",
    "We can use different models to create word/sense embeddings and log which one performs better."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0 | {'text': 'In Hinduism , the 60th birthday of a man is called Sashti poorthi .', 'annotations': [{'token_span': [5, 6], 'label': 'birthday%1:28:00::'}, {'token_span': [8, 9], 'label': 'man%1:18:00::'}]}\n",
      "  1 | {'text': 'In New Zealand , both Metal Box and Second Edition briefly entered the Top 50 Albums Chart .', 'annotations': [{'token_span': [6, 7], 'label': 'box%1:06:00::'}, {'token_span': [8, 9], 'label': 'second%5:00:00:ordinal:00'}, {'token_span': [11, 12], 'label': 'enter%2:33:00::'}, {'token_span': [13, 14], 'label': 'top%3:00:00::'}]}\n",
      "  2 | {'text': 'Article with extensive bibliography Article about the book \" L\\'hymnaire d\\'Adonis \" of Jacques d\\'Adelswärd - Fersen ( in Spanish ) Pictures of Villa Lysis today [ 1 ] [ 2 ]', 'annotations': [{'token_span': [0, 1], 'label': 'article%1:10:00::'}, {'token_span': [4, 5], 'label': 'article%1:10:00::'}, {'token_span': [7, 8], 'label': 'book%1:10:00::'}]}\n",
      "  3 | {'text': 'The root cluster attached to the basal plate of the bulb is the only part not typically considered palatable in any form .', 'annotations': [{'token_span': [1, 2], 'label': 'root%1:20:00::'}, {'token_span': [3, 4], 'label': 'attach%2:35:02::'}, {'token_span': [7, 8], 'label': 'plate%1:06:02::'}, {'token_span': [10, 11], 'label': 'bulb%1:20:00::'}]}\n",
      "  4 | {'text': \"The new world of English words came out in 1658 and a dictionary of 40,000 words had been prepared in 1721 by Nathan Bailey , though none was as comprehensive in breadth or style as Johnson's .\", 'annotations': [{'token_span': [1, 2], 'label': 'world%1:14:01::'}, {'token_span': [4, 5], 'label': 'word%1:10:00::'}, {'token_span': [11, 12], 'label': 'dictionary%1:10:00::'}, {'token_span': [14, 15], 'label': 'word%1:10:00::'}, {'token_span': [32, 33], 'label': 'style%1:10:00::'}]}\n",
      "  5 | {'text': 'Garlic softens and can be extracted from the cloves by squeezing the ( root ) end of the bulb , or individually by squeezing one end of the clove .', 'annotations': [{'token_span': [13, 14], 'label': 'root%1:20:00::'}, {'token_span': [18, 19], 'label': 'bulb%1:20:00::'}]}\n",
      "  6 | {'text': 'Under normal circumstances , this deflagration occurs too slowly to produce a significant pressure wave ; low explosives , therefore , must generally be used in large quantities or confined in a container with a high burst pressure to be useful as a bomb .', 'annotations': [{'token_span': [1, 2], 'label': 'normal%3:00:01::'}, {'token_span': [2, 3], 'label': 'circumstance%1:26:01::'}, {'token_span': [6, 7], 'label': 'occur%2:30:00::'}, {'token_span': [26, 27], 'label': 'large%3:00:00::'}, {'token_span': [32, 33], 'label': 'container%1:06:00::'}]}\n",
      "  7 | {'text': 'JOBD is a version of OBD - II for vehicles sold in Japan .', 'annotations': [{'token_span': [9, 10], 'label': 'vehicle%1:06:00::'}, {'token_span': [10, 11], 'label': 'sell%2:40:00::'}]}\n",
      "  8 | {'text': 'The indigenous peoples made an alcoholic beverage from fruits of the palm Attalea maripa found at the lower elevations .', 'annotations': [{'token_span': [3, 4], 'label': 'make%2:36:11::'}, {'token_span': [11, 12], 'label': 'palm%1:20:00::'}, {'token_span': [14, 15], 'label': 'find%2:40:02::'}]}\n",
      "  9 | {'text': 'It is a sanctuary as well as a tourist attraction , because it offers different climate , terrain , flora and fauna environments , ranging from beaches to snowy mountain peaks .', 'annotations': [{'token_span': [14, 15], 'label': 'different%3:00:00::'}, {'token_span': [26, 27], 'label': 'beach%1:17:00::'}]}\n"
     ]
    }
   ],
   "source": [
    "# Take all the lines in the file\n",
    "lines = []\n",
    "with jsonlines.open('data/sample_annotated_sentences/500000.jsonl') as reader:\n",
    "\n",
    "    limit = 10\n",
    "    for i, line in enumerate(reader):\n",
    "\n",
    "        if i < limit:\n",
    "            print(f'{i:3d} | {line}')\n",
    "\n",
    "        lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_digits(number):\n",
    "    return len(str(abs(number)))\n",
    "\n",
    "def get_token_and_index_strings(text):\n",
    "    line_tokens = text.split()\n",
    "    line_tokens_print = []\n",
    "    line_tokens_ids_print = []\n",
    "    for i, token in enumerate(line_tokens):\n",
    "        if get_num_digits(i) > len(token):\n",
    "            line_tokens_print.append((' ' * (get_num_digits(i) - len(token))) + token)\n",
    "        else:\n",
    "            line_tokens_print.append(token)\n",
    "        line_tokens_ids_print.append((' '*(len(line_tokens[i]) - len(str(i)))) + f'{i}')\n",
    "    return ' '.join(line_tokens_print), ' '.join(line_tokens_ids_print)\n",
    "\n",
    "def pretty_print(line, print_indexes=False):\n",
    "    \n",
    "    # Print the text\n",
    "    text = line['text']\n",
    "    \n",
    "    # Print the indexes\n",
    "    tokens_string, indexes_string = get_token_and_index_strings(text)\n",
    "    print(f'text: {tokens_string}')\n",
    "    if print_indexes:\n",
    "        print(f'      {\"\".join(indexes_string)}')\n",
    "\n",
    "    # Print the annotations\n",
    "    annotations = line['annotations']\n",
    "    if len(annotations) > 0:\n",
    "        print(f'annotations: {annotations[0]}')\n",
    "        for i in range(1, len(annotations)):\n",
    "            print(f\"{' '*(len('annotations: '))}{annotations[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: In Hinduism , the 60th birthday of a man is called Sashti poorthi  .\n",
      "       0        1 2   3    4        5  6 7   8  9     10     11      12 13\n",
      "annotations: {'token_span': [5, 6], 'label': 'birthday%1:28:00::'}\n",
      "             {'token_span': [8, 9], 'label': 'man%1:18:00::'}\n",
      "\n",
      "text: The new world of English words came out in 1658 and  a dictionary of 40,000 words had been prepared in 1721 by Nathan Bailey  , though none was as comprehensive in breadth or style as Johnson's  .\n",
      "        0   1     2  3       4     5    6   7  8    9  10 11         12 13     14    15  16   17       18 19   20 21     22     23 24     25   26  27 28            29 30      31 32    33 34        35 36\n",
      "annotations: {'token_span': [1, 2], 'label': 'world%1:14:01::'}\n",
      "             {'token_span': [4, 5], 'label': 'word%1:10:00::'}\n",
      "             {'token_span': [11, 12], 'label': 'dictionary%1:10:00::'}\n",
      "             {'token_span': [14, 15], 'label': 'word%1:10:00::'}\n",
      "             {'token_span': [32, 33], 'label': 'style%1:10:00::'}\n",
      "\n",
      "text: Even without the availability of either co-receptor ( even CCR5  )  , the virus can still invade cells if gp41 were to go through an alteration  ( including its cytoplasmic tail  ) that resulted in the independence of CD4 without the need of CCR5 and  / or CXCR4 as  a doorway  .\n",
      "         0       1   2            3  4      5           6 7    8    9 10 11  12    13  14    15     16    17 18   19   20 21 22      23 24         25 26        27  28          29   30 31   32       33 34  35           36 37  38      39  40   41 42   43  44 45 46    47 48 49      50 51\n",
      "annotations: {'token_span': [17, 18], 'label': 'cell%1:03:00::'}\n",
      "             {'token_span': [30, 31], 'label': 'tail%1:05:00::'}\n",
      "             {'token_span': [50, 51], 'label': 'doorway%1:06:00::'}\n",
      "\n",
      "text: Typically , NATO inert munitions are painted entirely in light blue and  / or have the word  \" INERT  \" stenciled on them in prominent locations .[ citation needed  ] IED  ( barrel bomb  , nail bomb  , pipe bomb  , pressure cooker bomb  , fertilizer bomb  , molotov cocktail  )\n",
      "              0 1    2     3         4   5       6        7  8     9   10  11 12 13   14  15   16 17    18 19        20 21   22 23        24        25 26       27     28 29  30 31     32   33 34   35   36 37   38   39 40       41     42   43 44         45   46 47      48       49 50\n",
      "annotations: {'token_span': [16, 17], 'label': 'word%1:10:00::'}\n",
      "             {'token_span': [35, 36], 'label': 'nail%1:06:00::'}\n",
      "             {'token_span': [48, 49], 'label': 'cocktail%1:13:00::'}\n",
      "\n",
      "text: Later on 9 January , Samuel scored a last - minute winner in the 4–3 win versus Siena by beating goalkeeper Gianluca Curci with  a left  - footed shot  .\n",
      "          0  1 2       3 4      5      6 7    8 9     10     11 12  13  14  15     16    17 18      19         20       21    22   23 24   25 26     27   28 29\n",
      "annotations: {'token_span': [10, 11], 'label': 'minute%1:28:00::'}\n",
      "             {'token_span': [11, 12], 'label': 'winner%1:18:00::'}\n",
      "             {'token_span': [15, 16], 'label': 'win%1:11:00::'}\n",
      "             {'token_span': [27, 28], 'label': 'foot%2:38:00::'}\n",
      "\n",
      "text: In a 1960 piano performance in Cologne , he played Chopin  , threw himself on the piano and rushed into the audience  , attacking Cage and pianist David Tudor by cutting their clothes with scissors and dumping shampoo on their heads  .\n",
      "       0 1    2     3           4  5       6 7  8      9     10 11    12      13 14  15    16  17     18   19  20       21 22        23   24  25      26    27    28 29      30    31      32   33       34  35      36      37 38    39    40 41\n",
      "annotations: {'token_span': [24, 25], 'label': 'cage%1:18:00::'}\n",
      "             {'token_span': [32, 33], 'label': 'clothes%1:06:00::'}\n",
      "             {'token_span': [40, 41], 'label': 'head%1:08:00::'}\n",
      "\n",
      "text: Here everybody uses a hookah , and it is impossible to get on without ...  [  I  ] have frequently heard men declare they would much rather be deprived of their dinner than their hookah  .  \"\n",
      "         0         1    2 3      4 5   6  7  8          9 10  11 12      13  14 15 16 17   18         19    20  21      22   23    24   25     26 27       28 29    30     31   32    33     34 35 36\n",
      "annotations: {'token_span': [20, 21], 'label': 'hear%2:39:00::'}\n",
      "             {'token_span': [21, 22], 'label': 'man%1:18:00::'}\n",
      "             {'token_span': [22, 23], 'label': 'declare%2:32:00::'}\n",
      "             {'token_span': [31, 32], 'label': 'dinner%1:13:00::'}\n",
      "\n",
      "text: It continues , \" בשלשה דברים אדם ניכר בכוסו ובכיסו ובכעסו  \"  , i.e.  ,  \" In three things is  a man revealed  : in his wine goblet  , in his purse  , and in his wrath  .  \"\n",
      "       0         1 2 3     4     5   6    7     8      9     10 11 12   13 14 15 16    17     18 19 20  21       22 23 24  25   26     27 28 29  30    31 32  33 34  35    36 37 38\n",
      "annotations: {'token_span': [21, 22], 'label': 'man%1:18:00::'}\n",
      "             {'token_span': [26, 27], 'label': 'wine%1:13:00::'}\n",
      "             {'token_span': [31, 32], 'label': 'purse%1:06:02::'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sample_line_1 = lines[0].copy()\n",
    "sample_line_1 = {\n",
    "    'text': \"In Hinduism , the 60th birthday of a man is called Sashti poorthi .\",\n",
    "    'annotations': [\n",
    "        {'token_span': [5, 6], 'label': 'birthday%1:28:00::'},\n",
    "        {'token_span': [8, 9], 'label': 'man%1:18:00::'}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# sample_line_2 = lines[5].copy()\n",
    "sample_line_2 = {\n",
    "    'text': \"The new world of English words came out in 1658 and a dictionary of 40,000 words had been prepared in 1721 by Nathan Bailey , though none was as comprehensive in breadth or style as Johnson's .\",\n",
    "    'annotations': [\n",
    "        {'token_span': [1, 2], 'label': 'world%1:14:01::'},\n",
    "        {'token_span': [4, 5], 'label': 'word%1:10:00::'},\n",
    "        {'token_span': [11, 12], 'label': 'dictionary%1:10:00::'},\n",
    "        {'token_span': [14, 15], 'label': 'word%1:10:00::'},\n",
    "        {'token_span': [32, 33], 'label': 'style%1:10:00::'}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# sample_line_3 = lines[35].copy()\n",
    "sample_line_3 = {\n",
    "    'text': \"Even without the availability of either co-receptor ( even CCR5 ) , the virus can still invade cells if gp41 were to go through an alteration ( including its cytoplasmic tail ) that resulted in the independence of CD4 without the need of CCR5 and / or CXCR4 as a doorway .\",\n",
    "    'annotations': [\n",
    "        {'token_span': [17, 18], 'label': 'cell%1:03:00::'},\n",
    "        {'token_span': [30, 31], 'label': 'tail%1:05:00::'},\n",
    "        {'token_span': [50, 51], 'label': 'doorway%1:06:00::'}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# sample_line_4 = lines[15].copy()\n",
    "sample_line_4 = {\n",
    "    'text': \"Typically , NATO inert munitions are painted entirely in light blue and / or have the word \\\" INERT \\\" stenciled on them in prominent locations .[ citation needed ] IED ( barrel bomb , nail bomb , pipe bomb , pressure cooker bomb , fertilizer bomb , molotov cocktail )\", \n",
    "    'annotations': [\n",
    "        {\"token_span\": [16, 17], \"label\": \"word%1:10:00::\"}, \n",
    "        {\"token_span\": [35, 36], \"label\": \"nail%1:06:00::\"}, \n",
    "        {\"token_span\": [48, 49], \"label\": \"cocktail%1:13:00::\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# sample_line_5 = lines[51].copy()\n",
    "sample_line_5 = {\n",
    "    'text': \"Later on 9 January , Samuel scored a last - minute winner in the 4\\u20133 win versus Siena by beating goalkeeper Gianluca Curci with a left - footed shot .\", \n",
    "    'annotations': [\n",
    "        {\"token_span\": [10, 11], \"label\": \"minute%1:28:00::\"}, \n",
    "        {\"token_span\": [11, 12], \"label\": \"winner%1:18:00::\"}, \n",
    "        {\"token_span\": [15, 16], \"label\": \"win%1:11:00::\"}, \n",
    "        {\"token_span\": [27, 28], \"label\": \"foot%2:38:00::\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# sample_line_6 = lines[502].copy()\n",
    "sample_line_6 = {\n",
    "    'text': \"In a 1960 piano performance in Cologne , he played Chopin , threw himself on the piano and rushed into the audience , attacking Cage and pianist David Tudor by cutting their clothes with scissors and dumping shampoo on their heads .\", \n",
    "    'annotations': [\n",
    "        {\"token_span\": [24, 25], \"label\": \"cage%1:18:00::\"}, \n",
    "        {\"token_span\": [32, 33], \"label\": \"clothes%1:06:00::\"}, \n",
    "        {\"token_span\": [40, 41], \"label\": \"head%1:08:00::\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "sample_lines = [\n",
    "    sample_line_1,\n",
    "    sample_line_2,\n",
    "    sample_line_3,\n",
    "    sample_line_4,\n",
    "    sample_line_5,\n",
    "    sample_line_6,\n",
    "    lines[709].copy(),\n",
    "    lines[704].copy()\n",
    "]\n",
    "\n",
    "for sample_line in sample_lines:\n",
    "    pretty_print(sample_line, print_indexes=True)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cafe francais\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "def remove_accents_from_string(input_string):\n",
    "    \"\"\"\n",
    "    Replace accented characters in a string with their plain versions using unicodedata.\n",
    "    \"\"\"\n",
    "    normalized_string = unicodedata.normalize('NFKD', input_string)\n",
    "    return ''.join([c for c in normalized_string if not unicodedata.combining(c)])\n",
    "\n",
    "# Example usage:\n",
    "original_string = \"Café français\"\n",
    "result = remove_accents_from_string(original_string)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-19 10:24:47 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d8c0b53c78442dab82f9e05835f9522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-19 10:24:49 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| lemma     | combined_nocharlm |\n",
      "=================================\n",
      "\n",
      "2023-11-19 10:24:49 INFO: Using device: cpu\n",
      "2023-11-19 10:24:49 INFO: Loading: tokenize\n",
      "2023-11-19 10:24:49 INFO: Loading: lemma\n",
      "2023-11-19 10:24:54 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,lemma', tokenize_pretokenized=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize with python splitting into sentences. This way we can use stanza to lemmatize the sentence pretokenizing the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['This', 'is', 'the', 'first', 'sentence', '.'], ['Here', 'is', 'the', 'second', 'one', '.'], ['And', 'finally', ',', 'the', 'third', 'sentence', '!']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/daniel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the punkt tokenizer if not already downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Example string with multiple sentences\n",
    "input_string = \"This is the first sentence. Here is the second one. And finally, the third sentence!\"\n",
    "\n",
    "# Tokenize sentences and words using nltk\n",
    "sentences = nltk.sent_tokenize(input_string)\n",
    "list_of_lists = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "# Print the result\n",
    "print(list_of_lists)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement the same thing simply with a split in our case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['In', 'Hinduism', ',', 'the', '60th', 'birthday', 'of', 'a', 'man', 'is', 'called', 'Sashti', 'poorthi', '.']]\n",
      "text: In Hinduism , the 60th birthday of a man is called Sashti poorthi  .\n",
      "       0        1 2   3    4        5  6 7   8  9     10     11      12 13\n",
      "annotations: {'token_span': [5, 6], 'label': 'birthday%1:28:00::'}\n",
      "             {'token_span': [8, 9], 'label': 'man%1:18:00::'}\n",
      "[['The', 'new', 'world', 'of', 'English', 'words', 'came', 'out', 'in', '1658', 'and', 'a', 'dictionary', 'of', '40,000', 'words', 'had', 'been', 'prepared', 'in', '1721', 'by', 'Nathan', 'Bailey', ',', 'though', 'none', 'was', 'as', 'comprehensive', 'in', 'breadth', 'or', 'style', 'as', \"Johnson's\", '.']]\n",
      "text: The new world of English words came out in 1658 and  a dictionary of 40,000 words had been prepared in 1721 by Nathan Bailey  , though none was as comprehensive in breadth or style as Johnson's  .\n",
      "        0   1     2  3       4     5    6   7  8    9  10 11         12 13     14    15  16   17       18 19   20 21     22     23 24     25   26  27 28            29 30      31 32    33 34        35 36\n",
      "annotations: {'token_span': [1, 2], 'label': 'world%1:14:01::'}\n",
      "             {'token_span': [4, 5], 'label': 'word%1:10:00::'}\n",
      "             {'token_span': [11, 12], 'label': 'dictionary%1:10:00::'}\n",
      "             {'token_span': [14, 15], 'label': 'word%1:10:00::'}\n",
      "             {'token_span': [32, 33], 'label': 'style%1:10:00::'}\n",
      "[['Even', 'without', 'the', 'availability', 'of', 'either', 'co-receptor', '(', 'even', 'CCR5', ')', ',', 'the', 'virus', 'can', 'still', 'invade', 'cells', 'if', 'gp41', 'were', 'to', 'go', 'through', 'an', 'alteration', '(', 'including', 'its', 'cytoplasmic', 'tail', ')', 'that', 'resulted', 'in', 'the', 'independence', 'of', 'CD4', 'without', 'the', 'need', 'of', 'CCR5', 'and', '/', 'or', 'CXCR4', 'as', 'a', 'doorway', '.']]\n",
      "text: Even without the availability of either co-receptor ( even CCR5  )  , the virus can still invade cells if gp41 were to go through an alteration  ( including its cytoplasmic tail  ) that resulted in the independence of CD4 without the need of CCR5 and  / or CXCR4 as  a doorway  .\n",
      "         0       1   2            3  4      5           6 7    8    9 10 11  12    13  14    15     16    17 18   19   20 21 22      23 24         25 26        27  28          29   30 31   32       33 34  35           36 37  38      39  40   41 42   43  44 45 46    47 48 49      50 51\n",
      "annotations: {'token_span': [17, 18], 'label': 'cell%1:03:00::'}\n",
      "             {'token_span': [30, 31], 'label': 'tail%1:05:00::'}\n",
      "             {'token_span': [50, 51], 'label': 'doorway%1:06:00::'}\n",
      "[['Typically', ',', 'NATO', 'inert', 'munitions', 'are', 'painted', 'entirely', 'in', 'light', 'blue', 'and', '/', 'or', 'have', 'the', 'word', '\"', 'INERT', '\"', 'stenciled', 'on', 'them', 'in', 'prominent', 'locations', '.'], ['[', 'citation', 'needed', ']', 'IED', '(', 'barrel', 'bomb', ',', 'nail', 'bomb', ',', 'pipe', 'bomb', ',', 'pressure', 'cooker', 'bomb', ',', 'fertilizer', 'bomb', ',', 'molotov', 'cocktail', ')', '.']]\n",
      "text: Typically , NATO inert munitions are painted entirely in light blue and  / or have the word  \" INERT  \" stenciled on them in prominent locations .[ citation needed  ] IED  ( barrel bomb  , nail bomb  , pipe bomb  , pressure cooker bomb  , fertilizer bomb  , molotov cocktail  )\n",
      "              0 1    2     3         4   5       6        7  8     9   10  11 12 13   14  15   16 17    18 19        20 21   22 23        24        25 26       27     28 29  30 31     32   33 34   35   36 37   38   39 40       41     42   43 44         45   46 47      48       49 50\n",
      "annotations: {'token_span': [16, 17], 'label': 'word%1:10:00::'}\n",
      "             {'token_span': [35, 36], 'label': 'nail%1:06:00::'}\n",
      "             {'token_span': [48, 49], 'label': 'cocktail%1:13:00::'}\n",
      "[['Later', 'on', '9', 'January', ',', 'Samuel', 'scored', 'a', 'last', '-', 'minute', 'winner', 'in', 'the', '4–3', 'win', 'versus', 'Siena', 'by', 'beating', 'goalkeeper', 'Gianluca', 'Curci', 'with', 'a', 'left', '-', 'footed', 'shot', '.']]\n",
      "text: Later on 9 January , Samuel scored a last - minute winner in the 4–3 win versus Siena by beating goalkeeper Gianluca Curci with  a left  - footed shot  .\n",
      "          0  1 2       3 4      5      6 7    8 9     10     11 12  13  14  15     16    17 18      19         20       21    22   23 24   25 26     27   28 29\n",
      "annotations: {'token_span': [10, 11], 'label': 'minute%1:28:00::'}\n",
      "             {'token_span': [11, 12], 'label': 'winner%1:18:00::'}\n",
      "             {'token_span': [15, 16], 'label': 'win%1:11:00::'}\n",
      "             {'token_span': [27, 28], 'label': 'foot%2:38:00::'}\n",
      "[['In', 'a', '1960', 'piano', 'performance', 'in', 'Cologne', ',', 'he', 'played', 'Chopin', ',', 'threw', 'himself', 'on', 'the', 'piano', 'and', 'rushed', 'into', 'the', 'audience', ',', 'attacking', 'Cage', 'and', 'pianist', 'David', 'Tudor', 'by', 'cutting', 'their', 'clothes', 'with', 'scissors', 'and', 'dumping', 'shampoo', 'on', 'their', 'heads', '.']]\n",
      "text: In a 1960 piano performance in Cologne , he played Chopin  , threw himself on the piano and rushed into the audience  , attacking Cage and pianist David Tudor by cutting their clothes with scissors and dumping shampoo on their heads  .\n",
      "       0 1    2     3           4  5       6 7  8      9     10 11    12      13 14  15    16  17     18   19  20       21 22        23   24  25      26    27    28 29      30    31      32   33       34  35      36      37 38    39    40 41\n",
      "annotations: {'token_span': [24, 25], 'label': 'cage%1:18:00::'}\n",
      "             {'token_span': [32, 33], 'label': 'clothes%1:06:00::'}\n",
      "             {'token_span': [40, 41], 'label': 'head%1:08:00::'}\n",
      "[['Here', 'everybody', 'uses', 'a', 'hookah', ',', 'and', 'it', 'is', 'impossible', 'to', 'get', 'on', 'without', '.'], ['[', 'I', ']', 'have', 'frequently', 'heard', 'men', 'declare', 'they', 'would', 'much', 'rather', 'be', 'deprived', 'of', 'their', 'dinner', 'than', 'their', 'hookah', '.'], ['\"', '.']]\n",
      "text: Here everybody uses a hookah , and it is impossible to get on without ...  [  I  ] have frequently heard men declare they would much rather be deprived of their dinner than their hookah  .  \"\n",
      "         0         1    2 3      4 5   6  7  8          9 10  11 12      13  14 15 16 17   18         19    20  21      22   23    24   25     26 27       28 29    30     31   32    33     34 35 36\n",
      "annotations: {'token_span': [20, 21], 'label': 'hear%2:39:00::'}\n",
      "             {'token_span': [21, 22], 'label': 'man%1:18:00::'}\n",
      "             {'token_span': [22, 23], 'label': 'declare%2:32:00::'}\n",
      "             {'token_span': [31, 32], 'label': 'dinner%1:13:00::'}\n",
      "[['It', 'continues', ',', '\"', 'בשלשה', 'דברים', 'אדם', 'ניכר', 'בכוסו', 'ובכיסו', 'ובכעסו', '\"', ',', 'i', '.'], ['e', '.'], [',', '\"', 'In', 'three', 'things', 'is', 'a', 'man', 'revealed', ':', 'in', 'his', 'wine', 'goblet', ',', 'in', 'his', 'purse', ',', 'and', 'in', 'his', 'wrath', '.'], ['\"', '.']]\n",
      "text: It continues , \" בשלשה דברים אדם ניכר בכוסו ובכיסו ובכעסו  \"  , i.e.  ,  \" In three things is  a man revealed  : in his wine goblet  , in his purse  , and in his wrath  .  \"\n",
      "       0         1 2 3     4     5   6    7     8      9     10 11 12   13 14 15 16    17     18 19 20  21       22 23 24  25   26     27 28 29  30    31 32  33 34  35    36 37 38\n",
      "annotations: {'token_span': [21, 22], 'label': 'man%1:18:00::'}\n",
      "             {'token_span': [26, 27], 'label': 'wine%1:13:00::'}\n",
      "             {'token_span': [31, 32], 'label': 'purse%1:06:02::'}\n"
     ]
    }
   ],
   "source": [
    "for sample_line in sample_lines:\n",
    "\n",
    "    text = sample_line['text']\n",
    "    text_tokens = [sentence.split() + ['.'] for sentence in text.split('.') if sentence]\n",
    "    print(text_tokens)\n",
    "    pretty_print(sample_line, print_indexes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sentence 0 ---\n",
      "word: in \tlemma: in\n",
      "word: hinduism \tlemma: hinduism\n",
      "word: , \tlemma: ,\n",
      "word: the \tlemma: the\n",
      "word: 60th \tlemma: 100th\n",
      "word: birthday \tlemma: birthday\n",
      "word: of \tlemma: of\n",
      "word: a \tlemma: a\n",
      "word: man \tlemma: man\n",
      "word: is \tlemma: be\n",
      "word: called \tlemma: call\n",
      "word: sashti \tlemma: sashti\n",
      "word: poorthi \tlemma: poorthi\n",
      "word: . \tlemma: .\n",
      "--- Sentence 1 ---\n",
      "word: the \tlemma: the\n",
      "word: new \tlemma: new\n",
      "word: world \tlemma: world\n",
      "word: of \tlemma: of\n",
      "word: english \tlemma: English\n",
      "word: words \tlemma: word\n",
      "word: came \tlemma: come\n",
      "word: out \tlemma: out\n",
      "word: in \tlemma: in\n",
      "word: 1658 \tlemma: 1658\n",
      "word: and \tlemma: and\n",
      "word: a \tlemma: a\n",
      "word: dictionary \tlemma: dictionary\n",
      "word: of \tlemma: of\n",
      "word: 40,000 \tlemma: 40000\n",
      "word: words \tlemma: word\n",
      "word: had \tlemma: have\n",
      "word: been \tlemma: be\n",
      "word: prepared \tlemma: prepare\n",
      "word: in \tlemma: in\n",
      "word: 1721 \tlemma: 1721\n",
      "word: by \tlemma: by\n",
      "word: nathan \tlemma: nathan\n",
      "word: bailey \tlemma: bailly\n",
      "word: , \tlemma: ,\n",
      "word: though \tlemma: though\n",
      "word: none \tlemma: none\n",
      "word: was \tlemma: be\n",
      "word: as \tlemma: as\n",
      "word: comprehensive \tlemma: comprehensive\n",
      "word: in \tlemma: in\n",
      "word: breadth \tlemma: breadth\n",
      "word: or \tlemma: or\n",
      "word: style \tlemma: style\n",
      "word: as \tlemma: as\n",
      "word: johnson's \tlemma: johnson'\n",
      "word: . \tlemma: .\n",
      "--- Sentence 2 ---\n",
      "word: even \tlemma: even\n",
      "word: without \tlemma: without\n",
      "word: the \tlemma: the\n",
      "word: availability \tlemma: availability\n",
      "word: of \tlemma: of\n",
      "word: either \tlemma: either\n",
      "word: co-receptor \tlemma: co-receptor\n",
      "word: ( \tlemma: (\n",
      "word: even \tlemma: even\n",
      "word: ccr5 \tlemma: ccr5\n",
      "word: ) \tlemma: )\n",
      "word: , \tlemma: ,\n",
      "word: the \tlemma: the\n",
      "word: virus \tlemma: virus\n",
      "word: can \tlemma: can\n",
      "word: still \tlemma: still\n",
      "word: invade \tlemma: invade\n",
      "word: cells \tlemma: cell\n",
      "word: if \tlemma: if\n",
      "word: gp41 \tlemma: gp41\n",
      "word: were \tlemma: be\n",
      "word: to \tlemma: to\n",
      "word: go \tlemma: go\n",
      "word: through \tlemma: through\n",
      "word: an \tlemma: a\n",
      "word: alteration \tlemma: alteration\n",
      "word: ( \tlemma: (\n",
      "word: including \tlemma: include\n",
      "word: its \tlemma: its\n",
      "word: cytoplasmic \tlemma: cytoplasmic\n",
      "word: tail \tlemma: tail\n",
      "word: ) \tlemma: )\n",
      "word: that \tlemma: that\n",
      "word: resulted \tlemma: result\n",
      "word: in \tlemma: in\n",
      "word: the \tlemma: the\n",
      "word: independence \tlemma: independence\n",
      "word: of \tlemma: of\n",
      "word: cd4 \tlemma: cd4\n",
      "word: without \tlemma: without\n",
      "word: the \tlemma: the\n",
      "word: need \tlemma: need\n",
      "word: of \tlemma: of\n",
      "word: ccr5 \tlemma: ccr5\n",
      "word: and \tlemma: and\n",
      "word: / \tlemma: /\n",
      "word: or \tlemma: or\n",
      "word: cxcr4 \tlemma: cxcr4\n",
      "word: as \tlemma: as\n",
      "word: a \tlemma: a\n",
      "word: doorway \tlemma: doorway\n",
      "word: . \tlemma: .\n",
      "--- Sentence 3 ---\n",
      "word: typically \tlemma: typically\n",
      "word: , \tlemma: ,\n",
      "word: nato \tlemma: nato\n",
      "word: inert \tlemma: inert\n",
      "word: munitions \tlemma: munition\n",
      "word: are \tlemma: be\n",
      "word: painted \tlemma: paint\n",
      "word: entirely \tlemma: entirely\n",
      "word: in \tlemma: in\n",
      "word: light \tlemma: light\n",
      "word: blue \tlemma: blue\n",
      "word: and \tlemma: and\n",
      "word: / \tlemma: /\n",
      "word: or \tlemma: or\n",
      "word: have \tlemma: have\n",
      "word: the \tlemma: the\n",
      "word: word \tlemma: word\n",
      "word: \" \tlemma: \"\n",
      "word: inert \tlemma: inert\n",
      "word: \" \tlemma: \"\n",
      "word: stenciled \tlemma: stenciled\n",
      "word: on \tlemma: on\n",
      "word: them \tlemma: they\n",
      "word: in \tlemma: in\n",
      "word: prominent \tlemma: prominent\n",
      "word: locations \tlemma: location\n",
      "word: .[ \tlemma: .[\n",
      "word: citation \tlemma: citation\n",
      "word: needed \tlemma: need\n",
      "word: ] \tlemma: ]\n",
      "word: ied \tlemma: ied\n",
      "word: ( \tlemma: (\n",
      "word: barrel \tlemma: barrel\n",
      "word: bomb \tlemma: bomb\n",
      "word: , \tlemma: ,\n",
      "word: nail \tlemma: nail\n",
      "word: bomb \tlemma: bomb\n",
      "word: , \tlemma: ,\n",
      "word: pipe \tlemma: pipe\n",
      "word: bomb \tlemma: bomb\n",
      "word: , \tlemma: ,\n",
      "word: pressure \tlemma: pressure\n",
      "word: cooker \tlemma: cooker\n",
      "word: bomb \tlemma: bomb\n",
      "word: , \tlemma: ,\n",
      "word: fertilizer \tlemma: fertilizer\n",
      "word: bomb \tlemma: bomb\n",
      "word: , \tlemma: ,\n",
      "word: molotov \tlemma: molotov\n",
      "word: cocktail \tlemma: cocktail\n",
      "word: ) \tlemma: )\n",
      "--- Sentence 4 ---\n",
      "word: later \tlemma: late\n",
      "word: on \tlemma: on\n",
      "word: 9 \tlemma: 9\n",
      "word: january \tlemma: january\n",
      "word: , \tlemma: ,\n",
      "word: samuel \tlemma: samuel\n",
      "word: scored \tlemma: score\n",
      "word: a \tlemma: a\n",
      "word: last \tlemma: last\n",
      "word: - \tlemma: -\n",
      "word: minute \tlemma: minute\n",
      "word: winner \tlemma: winner\n",
      "word: in \tlemma: in\n",
      "word: the \tlemma: the\n",
      "word: 4–3 \tlemma: 4–3\n",
      "word: win \tlemma: win\n",
      "word: versus \tlemma: versus\n",
      "word: siena \tlemma: siena\n",
      "word: by \tlemma: by\n",
      "word: beating \tlemma: beat\n",
      "word: goalkeeper \tlemma: goalkeeper\n",
      "word: gianluca \tlemma: gianluca\n",
      "word: curci \tlemma: curci\n",
      "word: with \tlemma: with\n",
      "word: a \tlemma: a\n",
      "word: left \tlemma: leave\n",
      "word: - \tlemma: -\n",
      "word: footed \tlemma: footed\n",
      "word: shot \tlemma: shot\n",
      "word: . \tlemma: .\n",
      "--- Sentence 5 ---\n",
      "word: in \tlemma: in\n",
      "word: a \tlemma: a\n",
      "word: 1960 \tlemma: 1960\n",
      "word: piano \tlemma: piano\n",
      "word: performance \tlemma: performance\n",
      "word: in \tlemma: in\n",
      "word: cologne \tlemma: cologne\n",
      "word: , \tlemma: ,\n",
      "word: he \tlemma: he\n",
      "word: played \tlemma: play\n",
      "word: chopin \tlemma: chopin\n",
      "word: , \tlemma: ,\n",
      "word: threw \tlemma: throw\n",
      "word: himself \tlemma: himself\n",
      "word: on \tlemma: on\n",
      "word: the \tlemma: the\n",
      "word: piano \tlemma: piano\n",
      "word: and \tlemma: and\n",
      "word: rushed \tlemma: rush\n",
      "word: into \tlemma: into\n",
      "word: the \tlemma: the\n",
      "word: audience \tlemma: audience\n",
      "word: , \tlemma: ,\n",
      "word: attacking \tlemma: attack\n",
      "word: cage \tlemma: cage\n",
      "word: and \tlemma: and\n",
      "word: pianist \tlemma: pianist\n",
      "word: david \tlemma: david\n",
      "word: tudor \tlemma: tudor\n",
      "word: by \tlemma: by\n",
      "word: cutting \tlemma: cut\n",
      "word: their \tlemma: their\n",
      "word: clothes \tlemma: clothes\n",
      "word: with \tlemma: with\n",
      "word: scissors \tlemma: scissors\n",
      "word: and \tlemma: and\n",
      "word: dumping \tlemma: dump\n",
      "word: shampoo \tlemma: shampoo\n",
      "word: on \tlemma: on\n",
      "word: their \tlemma: their\n",
      "word: heads \tlemma: head\n",
      "word: . \tlemma: .\n",
      "--- Sentence 6 ---\n",
      "word: here \tlemma: here\n",
      "word: everybody \tlemma: everybody\n",
      "word: uses \tlemma: use\n",
      "word: a \tlemma: a\n",
      "word: hookah \tlemma: hookah\n",
      "word: , \tlemma: ,\n",
      "word: and \tlemma: and\n",
      "word: it \tlemma: it\n",
      "word: is \tlemma: be\n",
      "word: impossible \tlemma: impossible\n",
      "word: to \tlemma: to\n",
      "word: get \tlemma: get\n",
      "word: on \tlemma: on\n",
      "word: without \tlemma: without\n",
      "word: ... \tlemma: ...\n",
      "word: [ \tlemma: [\n",
      "word: i \tlemma: I\n",
      "word: ] \tlemma: ]\n",
      "word: have \tlemma: have\n",
      "word: frequently \tlemma: frequently\n",
      "word: heard \tlemma: hear\n",
      "word: men \tlemma: man\n",
      "word: declare \tlemma: declare\n",
      "word: they \tlemma: they\n",
      "word: would \tlemma: would\n",
      "word: much \tlemma: much\n",
      "word: rather \tlemma: rather\n",
      "word: be \tlemma: be\n",
      "word: deprived \tlemma: deprived\n",
      "word: of \tlemma: of\n",
      "word: their \tlemma: their\n",
      "word: dinner \tlemma: dinner\n",
      "word: than \tlemma: than\n",
      "word: their \tlemma: their\n",
      "word: hookah \tlemma: hookah\n",
      "word: . \tlemma: .\n",
      "word: \" \tlemma: \"\n",
      "--- Sentence 7 ---\n",
      "word: it \tlemma: it\n",
      "word: continues \tlemma: continue\n",
      "word: , \tlemma: ,\n",
      "word: \" \tlemma: \"\n",
      "word: בשלשה \tlemma: בשלשה\n",
      "word: דברים \tlemma: דברים\n",
      "word: אדם \tlemma: אדם\n",
      "word: ניכר \tlemma: ניכר\n",
      "word: בכוסו \tlemma: בכוסו\n",
      "word: ובכיסו \tlemma: ובכיסו\n",
      "word: ובכעסו \tlemma: ובכעסו\n",
      "word: \" \tlemma: \"\n",
      "word: , \tlemma: ,\n",
      "word: i.e. \tlemma: i.e.\n",
      "word: , \tlemma: ,\n",
      "word: \" \tlemma: \"\n",
      "word: in \tlemma: in\n",
      "word: three \tlemma: three\n",
      "word: things \tlemma: thing\n",
      "word: is \tlemma: be\n",
      "word: a \tlemma: a\n",
      "word: man \tlemma: man\n",
      "word: revealed \tlemma: reveal\n",
      "word: : \tlemma: :\n",
      "word: in \tlemma: in\n",
      "word: his \tlemma: his\n",
      "word: wine \tlemma: wine\n",
      "word: goblet \tlemma: goblet\n",
      "word: , \tlemma: ,\n",
      "word: in \tlemma: in\n",
      "word: his \tlemma: his\n",
      "word: purse \tlemma: purse\n",
      "word: , \tlemma: ,\n",
      "word: and \tlemma: and\n",
      "word: in \tlemma: in\n",
      "word: his \tlemma: his\n",
      "word: wrath \tlemma: wrath\n",
      "word: . \tlemma: .\n",
      "word: \" \tlemma: \"\n"
     ]
    }
   ],
   "source": [
    "for i, sample_line in enumerate(sample_lines):\n",
    "\n",
    "    text = sample_line['text'].lower()\n",
    "    text = remove_accents_from_string(text)\n",
    "    text_tokens = text.split()\n",
    "\n",
    "    doc = nlp([text_tokens])\n",
    "\n",
    "    print(f'--- Sentence {i} ---')\n",
    "    print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have another problem: sometimes the index in the annotations is off. We need to account for that as in some cases (e.g. in the second sample line) the word we need to replace will remain along with its explicit senses, that replaced an unrelated token instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    similarity = intersection / union if union != 0 else 0  # Avoid division by zero\n",
    "    return similarity\n",
    "\n",
    "def find_word_in_window(lemmatized_tokens, window_span, sense_key, use_jaccard_similarity=0.6, window_size=1):\n",
    "    \"\"\"\n",
    "    We have a few tokens and a sense_key, that contains the lemma of a word inside the text.\n",
    "    We want to find the index of that word.\n",
    "    \"\"\"\n",
    "\n",
    "    start_idx = max(window_span[0] - window_size, 0)\n",
    "    end_idx = min(window_span[1] + window_size, len(lemmatized_tokens))\n",
    "\n",
    "    lemma_from_sense_key = sense_key[:sense_key.index('%')]\n",
    "\n",
    "    lemma_scores = dict()\n",
    "    for index in range(start_idx, end_idx):\n",
    "        lemma = lemmatized_tokens[index]\n",
    "        # print(f'Token [{token}] vs lemma [{lemma}]')\n",
    "\n",
    "        lemma_scores[index] = jaccard_similarity(set(lemma), set(lemma_from_sense_key)) if use_jaccard_similarity > 0 else lemma == lemma_from_sense_key\n",
    "    \n",
    "    # print(lemma_scores)\n",
    "\n",
    "    max_index = max(lemma_scores, key=lemma_scores.get)\n",
    "    max_score = lemma_scores[max_index]\n",
    "\n",
    "    if max_score < use_jaccard_similarity:\n",
    "        raise ValueError(f'No match in window for token {sense_key} in window {lemmatized_tokens[start_idx:end_idx + 1]}')\n",
    "    return max_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: In Hinduism , the 60th birthday of a man is called Sashti poorthi  .\n",
      "       0        1 2   3    4        5  6 7   8  9     10     11      12 13\n",
      "annotations: {'token_span': [5, 6], 'label': 'birthday%1:28:00::'}\n",
      "             {'token_span': [8, 9], 'label': 'man%1:18:00::'}\n",
      "birthday%1:28:00:: in range (5 - 6) -> true index = 5\n",
      "man%1:18:00:: in range (8 - 9) -> true index = 8\n",
      "\n",
      "text: The new world of English words came out in 1658 and  a dictionary of 40,000 words had been prepared in 1721 by Nathan Bailey  , though none was as comprehensive in breadth or style as Johnson's  .\n",
      "        0   1     2  3       4     5    6   7  8    9  10 11         12 13     14    15  16   17       18 19   20 21     22     23 24     25   26  27 28            29 30      31 32    33 34        35 36\n",
      "annotations: {'token_span': [1, 2], 'label': 'world%1:14:01::'}\n",
      "             {'token_span': [4, 5], 'label': 'word%1:10:00::'}\n",
      "             {'token_span': [11, 12], 'label': 'dictionary%1:10:00::'}\n",
      "             {'token_span': [14, 15], 'label': 'word%1:10:00::'}\n",
      "             {'token_span': [32, 33], 'label': 'style%1:10:00::'}\n",
      "world%1:14:01:: in range (1 - 2) -> true index = 2\n",
      "word%1:10:00:: in range (4 - 5) -> true index = 5\n",
      "dictionary%1:10:00:: in range (11 - 12) -> true index = 12\n",
      "word%1:10:00:: in range (14 - 15) -> true index = 15\n",
      "style%1:10:00:: in range (32 - 33) -> true index = 33\n",
      "\n",
      "text: Even without the availability of either co-receptor ( even CCR5  )  , the virus can still invade cells if gp41 were to go through an alteration  ( including its cytoplasmic tail  ) that resulted in the independence of CD4 without the need of CCR5 and  / or CXCR4 as  a doorway  .\n",
      "         0       1   2            3  4      5           6 7    8    9 10 11  12    13  14    15     16    17 18   19   20 21 22      23 24         25 26        27  28          29   30 31   32       33 34  35           36 37  38      39  40   41 42   43  44 45 46    47 48 49      50 51\n",
      "annotations: {'token_span': [17, 18], 'label': 'cell%1:03:00::'}\n",
      "             {'token_span': [30, 31], 'label': 'tail%1:05:00::'}\n",
      "             {'token_span': [50, 51], 'label': 'doorway%1:06:00::'}\n",
      "cell%1:03:00:: in range (17 - 18) -> true index = 17\n",
      "tail%1:05:00:: in range (30 - 31) -> true index = 30\n",
      "doorway%1:06:00:: in range (50 - 51) -> true index = 50\n",
      "\n",
      "text: Typically , NATO inert munitions are painted entirely in light blue and  / or have the word  \" INERT  \" stenciled on them in prominent locations .[ citation needed  ] IED  ( barrel bomb  , nail bomb  , pipe bomb  , pressure cooker bomb  , fertilizer bomb  , molotov cocktail  )\n",
      "              0 1    2     3         4   5       6        7  8     9   10  11 12 13   14  15   16 17    18 19        20 21   22 23        24        25 26       27     28 29  30 31     32   33 34   35   36 37   38   39 40       41     42   43 44         45   46 47      48       49 50\n",
      "annotations: {'token_span': [16, 17], 'label': 'word%1:10:00::'}\n",
      "             {'token_span': [35, 36], 'label': 'nail%1:06:00::'}\n",
      "             {'token_span': [48, 49], 'label': 'cocktail%1:13:00::'}\n",
      "word%1:10:00:: in range (16 - 17) -> true index = 16\n",
      "nail%1:06:00:: in range (35 - 36) -> true index = 35\n",
      "cocktail%1:13:00:: in range (48 - 49) -> true index = 49\n",
      "\n",
      "text: Later on 9 January , Samuel scored a last - minute winner in the 4–3 win versus Siena by beating goalkeeper Gianluca Curci with  a left  - footed shot  .\n",
      "          0  1 2       3 4      5      6 7    8 9     10     11 12  13  14  15     16    17 18      19         20       21    22   23 24   25 26     27   28 29\n",
      "annotations: {'token_span': [10, 11], 'label': 'minute%1:28:00::'}\n",
      "             {'token_span': [11, 12], 'label': 'winner%1:18:00::'}\n",
      "             {'token_span': [15, 16], 'label': 'win%1:11:00::'}\n",
      "             {'token_span': [27, 28], 'label': 'foot%2:38:00::'}\n",
      "minute%1:28:00:: in range (10 - 11) -> true index = 10\n",
      "winner%1:18:00:: in range (11 - 12) -> true index = 11\n",
      "win%1:11:00:: in range (15 - 16) -> true index = 15\n",
      "foot%2:38:00:: in range (27 - 28) -> true index = 27\n",
      "\n",
      "text: In a 1960 piano performance in Cologne , he played Chopin  , threw himself on the piano and rushed into the audience  , attacking Cage and pianist David Tudor by cutting their clothes with scissors and dumping shampoo on their heads  .\n",
      "       0 1    2     3           4  5       6 7  8      9     10 11    12      13 14  15    16  17     18   19  20       21 22        23   24  25      26    27    28 29      30    31      32   33       34  35      36      37 38    39    40 41\n",
      "annotations: {'token_span': [24, 25], 'label': 'cage%1:18:00::'}\n",
      "             {'token_span': [32, 33], 'label': 'clothes%1:06:00::'}\n",
      "             {'token_span': [40, 41], 'label': 'head%1:08:00::'}\n",
      "cage%1:18:00:: in range (24 - 25) -> true index = 24\n",
      "clothes%1:06:00:: in range (32 - 33) -> true index = 32\n",
      "head%1:08:00:: in range (40 - 41) -> true index = 40\n",
      "\n",
      "text: Here everybody uses a hookah , and it is impossible to get on without ...  [  I  ] have frequently heard men declare they would much rather be deprived of their dinner than their hookah  .  \"\n",
      "         0         1    2 3      4 5   6  7  8          9 10  11 12      13  14 15 16 17   18         19    20  21      22   23    24   25     26 27       28 29    30     31   32    33     34 35 36\n",
      "annotations: {'token_span': [20, 21], 'label': 'hear%2:39:00::'}\n",
      "             {'token_span': [21, 22], 'label': 'man%1:18:00::'}\n",
      "             {'token_span': [22, 23], 'label': 'declare%2:32:00::'}\n",
      "             {'token_span': [31, 32], 'label': 'dinner%1:13:00::'}\n",
      "hear%2:39:00:: in range (20 - 21) -> true index = 20\n",
      "man%1:18:00:: in range (21 - 22) -> true index = 21\n",
      "declare%2:32:00:: in range (22 - 23) -> true index = 22\n",
      "dinner%1:13:00:: in range (31 - 32) -> true index = 31\n",
      "\n",
      "text: It continues , \" בשלשה דברים אדם ניכר בכוסו ובכיסו ובכעסו  \"  , i.e.  ,  \" In three things is  a man revealed  : in his wine goblet  , in his purse  , and in his wrath  .  \"\n",
      "       0         1 2 3     4     5   6    7     8      9     10 11 12   13 14 15 16    17     18 19 20  21       22 23 24  25   26     27 28 29  30    31 32  33 34  35    36 37 38\n",
      "annotations: {'token_span': [21, 22], 'label': 'man%1:18:00::'}\n",
      "             {'token_span': [26, 27], 'label': 'wine%1:13:00::'}\n",
      "             {'token_span': [31, 32], 'label': 'purse%1:06:02::'}\n",
      "man%1:18:00:: in range (21 - 22) -> true index = 21\n",
      "wine%1:13:00:: in range (26 - 27) -> true index = 26\n",
      "purse%1:06:02:: in range (31 - 32) -> true index = 31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sample_line in sample_lines:\n",
    "\n",
    "    pretty_print(sample_line, print_indexes=True)\n",
    "\n",
    "    text = sample_line['text'].lower()\n",
    "    text = remove_accents_from_string(text)\n",
    "    text_tokens = text.split()\n",
    "\n",
    "    doc = nlp([text_tokens])    \n",
    "\n",
    "    lemmatized_tokens = [word.lemma for sent in doc.sentences for word in sent.words]\n",
    "\n",
    "\n",
    "    for annotation in sample_line['annotations']:\n",
    "        window_start = max(annotation['token_span'][0], 0)\n",
    "        window_end = min(annotation['token_span'][1], len(lemmatized_tokens))\n",
    "        idx = find_word_in_window(lemmatized_tokens, (window_start, window_end), annotation['label'], use_jaccard_similarity=0.6, window_size=4)\n",
    "        print(f'{annotation[\"label\"]} in range ({window_start} - {window_end}) -> true index = {idx}')\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build the final preprocessing routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_line(line, \n",
    "                    model,\n",
    "                    stopwords=[], \n",
    "                    punctuation=[],\n",
    "                    lemmatize=True,\n",
    "                    filtering_regex=None, \n",
    "                    replace_with_sense_key=True,\n",
    "                    remove_accents=True,\n",
    "                    use_jaccard_similarity=0.6,\n",
    "                    window_size = 2,\n",
    "                    produce_both=False\n",
    "                    ):\n",
    "\n",
    "\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    "\n",
    "    if punctuation is None:\n",
    "        punctuation = []\n",
    "\n",
    "    text = line['text'].lower()\n",
    "    if remove_accents:\n",
    "        text = remove_accents_from_string(text)\n",
    "\n",
    "    text_split = text.lower().split()\n",
    "    doc = model([text_split])\n",
    "    tokens = [word for sent in doc.sentences for word in sent.words]\n",
    "\n",
    "    annotations = line['annotations']\n",
    "\n",
    "    # Indexes of the words we may want to replace with a sense key\n",
    "    # {token1_index: token1, ...}\n",
    "    annotations_data = {(annotation['token_span'][0], annotation['token_span'][1]): annotation['label'] for annotation in annotations}\n",
    "    annotation_indexes = {find_word_in_window(\n",
    "        [token.lemma for token in tokens], \n",
    "        (k[0], k[1]), \n",
    "        v, \n",
    "        use_jaccard_similarity=use_jaccard_similarity,\n",
    "        window_size=window_size\n",
    "    ): v for k, v in annotations_data.items()}\n",
    "    \n",
    "    # This will be then joined to create a new preprocessed sentence\n",
    "    new_tokens = []\n",
    "    new_semantic_tokens = []\n",
    "    \n",
    "    # For each token, check if it is a stopword, punctuation, if it matches the filter regex \n",
    "    # and if it needs to be retained or replaced with its sense key. Doc now tokenizes as the\n",
    "    # standard string split, so there will be no differences in the token indexes\n",
    "    for i, token in enumerate(tokens):\n",
    "\n",
    "        token_text = token.lemma if lemmatize else token.text\n",
    "        \n",
    "        # The token is a word we can replace with the lemma key\n",
    "        if i in annotation_indexes.keys():\n",
    "\n",
    "            # We may want to produce both sentences, one with lemma key and one without\n",
    "            # in order to train a model for word embeddings and another model with\n",
    "            # sense embeddings\n",
    "            if produce_both:\n",
    "                new_tokens.append(token_text)\n",
    "                new_semantic_tokens.append(annotation_indexes[i]) # [i][1])\n",
    "            else:\n",
    "                if replace_with_sense_key:\n",
    "                    # print(f'Token {token_text:20s} is good, we keep it as {annotation_indexes[i][1]}')\n",
    "                    new_tokens.append(annotation_indexes[i]) # [i][1])\n",
    "                else:\n",
    "                    # print(f'Token {token_text:20s} is good, we keep it as {token_text}')\n",
    "                    new_tokens.append(token_text)\n",
    "                \n",
    "        # The token is a stopword\n",
    "        elif token_text in stopwords:\n",
    "            # print(f'Token {token_text:20s} is a stopword')\n",
    "            continue\n",
    "\n",
    "        # The token is punctuation\n",
    "        elif token_text in punctuation:\n",
    "            # print(f'Token {token_text:20s} is punctuation')\n",
    "            continue\n",
    "\n",
    "        # The token matches the regex, it is not valid\n",
    "        elif filtering_regex is not None and filtering_regex.search(token_text):\n",
    "            # print(f'Token {token_text:20s} is filtered out from regex')\n",
    "            continue\n",
    "\n",
    "        # The token is valid, we can retain it\n",
    "        else:\n",
    "            # print(f'Token {token_text:20s} is good, we keep it')\n",
    "            new_tokens.append(token_text)\n",
    "            if produce_both:\n",
    "                new_semantic_tokens.append(token_text)\n",
    "\n",
    "    return new_tokens, new_semantic_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/daniel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Regex to discard numbers, special characters and punctuation\n",
    "regexp_alphbetic = re.compile('[^a-zA-Z-]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data before: \n",
      "In Hinduism , the 60th birthday of a man is called Sashti poorthi .\n",
      "['in', 'hinduism', ',', 'the', '60th', 'birthday', 'of', 'a', 'man', 'is', 'called', 'sashti', 'poorthi', '.']\n",
      "Lemmatized tokens:\n",
      "['in', 'hinduism', ',', 'the', '100th', 'birthday', 'of', 'a', 'man', 'be', 'call', 'sashti', 'poorthi', '.']\n",
      "Preprocessed data WITHOUT jaccard similarity:\n",
      "hinduism birthday%1:28:00:: man%1:18:00:: call sashti poorthi\n",
      "Preprocessed data WITH jaccard similarity:\n",
      "hinduism birthday%1:28:00:: man%1:18:00:: call sashti poorthi\n",
      "\n",
      "Data before: \n",
      "The new world of English words came out in 1658 and a dictionary of 40,000 words had been prepared in 1721 by Nathan Bailey , though none was as comprehensive in breadth or style as Johnson's .\n",
      "['the', 'new', 'world', 'of', 'english', 'words', 'came', 'out', 'in', '1658', 'and', 'a', 'dictionary', 'of', '40,000', 'words', 'had', 'been', 'prepared', 'in', '1721', 'by', 'nathan', 'bailey', ',', 'though', 'none', 'was', 'as', 'comprehensive', 'in', 'breadth', 'or', 'style', 'as', \"johnson's\", '.']\n",
      "Lemmatized tokens:\n",
      "['the', 'new', 'world', 'of', 'English', 'word', 'come', 'out', 'in', '1658', 'and', 'a', 'dictionary', 'of', '40000', 'word', 'have', 'be', 'prepare', 'in', '1721', 'by', 'nathan', 'bailly', ',', 'though', 'none', 'be', 'as', 'comprehensive', 'in', 'breadth', 'or', 'style', 'as', \"johnson'\", '.']\n",
      "Preprocessed data WITHOUT jaccard similarity:\n",
      "new world%1:14:01:: English word%1:10:00:: come dictionary%1:10:00:: word%1:10:00:: prepare nathan bailly though none comprehensive breadth style%1:10:00::\n",
      "Preprocessed data WITH jaccard similarity:\n",
      "new world%1:14:01:: English word%1:10:00:: come dictionary%1:10:00:: word%1:10:00:: prepare nathan bailly though none comprehensive breadth style%1:10:00::\n",
      "\n",
      "Data before: \n",
      "Even without the availability of either co-receptor ( even CCR5 ) , the virus can still invade cells if gp41 were to go through an alteration ( including its cytoplasmic tail ) that resulted in the independence of CD4 without the need of CCR5 and / or CXCR4 as a doorway .\n",
      "['even', 'without', 'the', 'availability', 'of', 'either', 'co-receptor', '(', 'even', 'ccr5', ')', ',', 'the', 'virus', 'can', 'still', 'invade', 'cells', 'if', 'gp41', 'were', 'to', 'go', 'through', 'an', 'alteration', '(', 'including', 'its', 'cytoplasmic', 'tail', ')', 'that', 'resulted', 'in', 'the', 'independence', 'of', 'cd4', 'without', 'the', 'need', 'of', 'ccr5', 'and', '/', 'or', 'cxcr4', 'as', 'a', 'doorway', '.']\n",
      "Lemmatized tokens:\n",
      "['even', 'without', 'the', 'availability', 'of', 'either', 'co-receptor', '(', 'even', 'ccr5', ')', ',', 'the', 'virus', 'can', 'still', 'invade', 'cell', 'if', 'gp41', 'be', 'to', 'go', 'through', 'a', 'alteration', '(', 'include', 'its', 'cytoplasmic', 'tail', ')', 'that', 'result', 'in', 'the', 'independence', 'of', 'cd4', 'without', 'the', 'need', 'of', 'ccr5', 'and', '/', 'or', 'cxcr4', 'as', 'a', 'doorway', '.']\n",
      "Preprocessed data WITHOUT jaccard similarity:\n",
      "even without availability either co-receptor even virus still invade cell%1:03:00:: go alteration include cytoplasmic tail%1:05:00:: result independence without need doorway%1:06:00::\n",
      "Preprocessed data WITH jaccard similarity:\n",
      "even without availability either co-receptor even virus still invade cell%1:03:00:: go alteration include cytoplasmic tail%1:05:00:: result independence without need doorway%1:06:00::\n",
      "\n",
      "Data before: \n",
      "Typically , NATO inert munitions are painted entirely in light blue and / or have the word \" INERT \" stenciled on them in prominent locations .[ citation needed ] IED ( barrel bomb , nail bomb , pipe bomb , pressure cooker bomb , fertilizer bomb , molotov cocktail )\n",
      "['typically', ',', 'nato', 'inert', 'munitions', 'are', 'painted', 'entirely', 'in', 'light', 'blue', 'and', '/', 'or', 'have', 'the', 'word', '\"', 'inert', '\"', 'stenciled', 'on', 'them', 'in', 'prominent', 'locations', '.[', 'citation', 'needed', ']', 'ied', '(', 'barrel', 'bomb', ',', 'nail', 'bomb', ',', 'pipe', 'bomb', ',', 'pressure', 'cooker', 'bomb', ',', 'fertilizer', 'bomb', ',', 'molotov', 'cocktail', ')']\n",
      "Lemmatized tokens:\n",
      "['typically', ',', 'nato', 'inert', 'munition', 'be', 'paint', 'entirely', 'in', 'light', 'blue', 'and', '/', 'or', 'have', 'the', 'word', '\"', 'inert', '\"', 'stenciled', 'on', 'they', 'in', 'prominent', 'location', '.[', 'citation', 'need', ']', 'ied', '(', 'barrel', 'bomb', ',', 'nail', 'bomb', ',', 'pipe', 'bomb', ',', 'pressure', 'cooker', 'bomb', ',', 'fertilizer', 'bomb', ',', 'molotov', 'cocktail', ')']\n",
      "Preprocessed data WITHOUT jaccard similarity:\n",
      "typically nato inert munition paint entirely light blue word%1:10:00:: inert stenciled prominent location citation need ied barrel bomb nail%1:06:00:: bomb pipe bomb pressure cooker bomb fertilizer bomb molotov cocktail%1:13:00::\n",
      "Preprocessed data WITH jaccard similarity:\n",
      "typically nato inert munition paint entirely light blue word%1:10:00:: inert stenciled prominent location citation need ied barrel bomb nail%1:06:00:: bomb pipe bomb pressure cooker bomb fertilizer bomb molotov cocktail%1:13:00::\n",
      "\n",
      "Data before: \n",
      "Later on 9 January , Samuel scored a last - minute winner in the 4–3 win versus Siena by beating goalkeeper Gianluca Curci with a left - footed shot .\n",
      "['later', 'on', '9', 'january', ',', 'samuel', 'scored', 'a', 'last', '-', 'minute', 'winner', 'in', 'the', '4–3', 'win', 'versus', 'siena', 'by', 'beating', 'goalkeeper', 'gianluca', 'curci', 'with', 'a', 'left', '-', 'footed', 'shot', '.']\n",
      "Lemmatized tokens:\n",
      "['late', 'on', '9', 'january', ',', 'samuel', 'score', 'a', 'last', '-', 'minute', 'winner', 'in', 'the', '4–3', 'win', 'versus', 'siena', 'by', 'beat', 'goalkeeper', 'gianluca', 'curci', 'with', 'a', 'leave', '-', 'footed', 'shot', '.']\n",
      "Preprocessed data WITHOUT jaccard similarity:\n",
      "Exception\n",
      "Preprocessed data WITH jaccard similarity:\n",
      "late january samuel score last - minute%1:28:00:: winner%1:18:00:: win%1:11:00:: versus siena beat goalkeeper gianluca curci leave - foot%2:38:00:: shot\n",
      "\n",
      "Data before: \n",
      "In a 1960 piano performance in Cologne , he played Chopin , threw himself on the piano and rushed into the audience , attacking Cage and pianist David Tudor by cutting their clothes with scissors and dumping shampoo on their heads .\n",
      "['in', 'a', '1960', 'piano', 'performance', 'in', 'cologne', ',', 'he', 'played', 'chopin', ',', 'threw', 'himself', 'on', 'the', 'piano', 'and', 'rushed', 'into', 'the', 'audience', ',', 'attacking', 'cage', 'and', 'pianist', 'david', 'tudor', 'by', 'cutting', 'their', 'clothes', 'with', 'scissors', 'and', 'dumping', 'shampoo', 'on', 'their', 'heads', '.']\n",
      "Lemmatized tokens:\n",
      "['in', 'a', '1960', 'piano', 'performance', 'in', 'cologne', ',', 'he', 'play', 'chopin', ',', 'throw', 'himself', 'on', 'the', 'piano', 'and', 'rush', 'into', 'the', 'audience', ',', 'attack', 'cage', 'and', 'pianist', 'david', 'tudor', 'by', 'cut', 'their', 'clothes', 'with', 'scissors', 'and', 'dump', 'shampoo', 'on', 'their', 'head', '.']\n",
      "Preprocessed data WITHOUT jaccard similarity:\n",
      "piano performance cologne play chopin throw piano rush audience attack cage%1:18:00:: pianist david tudor cut clothes%1:06:00:: scissors dump shampoo head%1:08:00::\n",
      "Preprocessed data WITH jaccard similarity:\n",
      "piano performance cologne play chopin throw piano rush audience attack cage%1:18:00:: pianist david tudor cut clothes%1:06:00:: scissors dump shampoo head%1:08:00::\n",
      "\n",
      "Data before: \n",
      "Here everybody uses a hookah , and it is impossible to get on without ... [ I ] have frequently heard men declare they would much rather be deprived of their dinner than their hookah . \"\n",
      "['here', 'everybody', 'uses', 'a', 'hookah', ',', 'and', 'it', 'is', 'impossible', 'to', 'get', 'on', 'without', '...', '[', 'i', ']', 'have', 'frequently', 'heard', 'men', 'declare', 'they', 'would', 'much', 'rather', 'be', 'deprived', 'of', 'their', 'dinner', 'than', 'their', 'hookah', '.', '\"']\n",
      "Lemmatized tokens:\n",
      "['here', 'everybody', 'use', 'a', 'hookah', ',', 'and', 'it', 'be', 'impossible', 'to', 'get', 'on', 'without', '...', '[', 'I', ']', 'have', 'frequently', 'hear', 'man', 'declare', 'they', 'would', 'much', 'rather', 'be', 'deprived', 'of', 'their', 'dinner', 'than', 'their', 'hookah', '.', '\"']\n",
      "Preprocessed data WITHOUT jaccard similarity:\n",
      "everybody use hookah impossible get without I frequently hear%2:39:00:: man%1:18:00:: declare%2:32:00:: would much rather deprived dinner%1:13:00:: hookah\n",
      "Preprocessed data WITH jaccard similarity:\n",
      "everybody use hookah impossible get without I frequently hear%2:39:00:: man%1:18:00:: declare%2:32:00:: would much rather deprived dinner%1:13:00:: hookah\n",
      "\n",
      "Data before: \n",
      "It continues , \" בשלשה דברים אדם ניכר בכוסו ובכיסו ובכעסו \" , i.e. , \" In three things is a man revealed : in his wine goblet , in his purse , and in his wrath . \"\n",
      "['it', 'continues', ',', '\"', 'בשלשה', 'דברים', 'אדם', 'ניכר', 'בכוסו', 'ובכיסו', 'ובכעסו', '\"', ',', 'i.e.', ',', '\"', 'in', 'three', 'things', 'is', 'a', 'man', 'revealed', ':', 'in', 'his', 'wine', 'goblet', ',', 'in', 'his', 'purse', ',', 'and', 'in', 'his', 'wrath', '.', '\"']\n",
      "Lemmatized tokens:\n",
      "['it', 'continue', ',', '\"', 'בשלשה', 'דברים', 'אדם', 'ניכר', 'בכוסו', 'ובכיסו', 'ובכעסו', '\"', ',', 'i.e.', ',', '\"', 'in', 'three', 'thing', 'be', 'a', 'man', 'reveal', ':', 'in', 'his', 'wine', 'goblet', ',', 'in', 'his', 'purse', ',', 'and', 'in', 'his', 'wrath', '.', '\"']\n",
      "Preprocessed data WITHOUT jaccard similarity:\n",
      "continue three thing man%1:18:00:: reveal wine%1:13:00:: goblet purse%1:06:02:: wrath\n",
      "Preprocessed data WITH jaccard similarity:\n",
      "continue three thing man%1:18:00:: reveal wine%1:13:00:: goblet purse%1:06:02:: wrath\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sample_line in sample_lines:\n",
    "\n",
    "    text = sample_line['text']\n",
    "\n",
    "    print('Data before: ')\n",
    "    print(text)\n",
    "\n",
    "    text_split = text.lower().split()\n",
    "    print(text_split)\n",
    "\n",
    "    doc = nlp([text_split])\n",
    "    tokens = [word.lemma for sent in doc.sentences for word in sent.words]\n",
    "    print('Lemmatized tokens:')\n",
    "    print(tokens)\n",
    "\n",
    "    try:\n",
    "        print('Preprocessed data WITHOUT jaccard similarity:')\n",
    "        print(\" \".join(preprocess_line(sample_line, nlp, stopwords=stop_words, filtering_regex=regexp_alphbetic, use_jaccard_similarity=1)[0]))\n",
    "    except:\n",
    "        print('Exception')\n",
    "\n",
    "    print('Preprocessed data WITH jaccard similarity:')\n",
    "    print(\" \".join(preprocess_line(sample_line, nlp, stopwords=stop_words, filtering_regex=regexp_alphbetic)[0]))\n",
    "\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 534300/534300 [1:39:35<00:00, 89.41it/s, discarded=19706]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence                : In Hinduism , the 60th birthday of a man is called Sashti poorthi .\n",
      "Preprocessed sentence            : hinduism birthday man call sashti poorthi\n",
      "Preprocessed sentence with senses: hinduism birthday%1:28:00:: man%1:18:00:: call sashti poorthi\n",
      "Discarded 19706 out of 500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the actual datasets \n",
    "original_sentences = []\n",
    "non_semantic_tokens = []\n",
    "semantic_tokens = []\n",
    "discarded_count = 0\n",
    "\n",
    "pbar = tqdm(lines)\n",
    "for line in pbar:\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        non_semantic, semantic = preprocess_line(\n",
    "            line, \n",
    "            nlp, \n",
    "            stopwords=stop_words, \n",
    "            filtering_regex=regexp_alphbetic, \n",
    "            produce_both=True, \n",
    "            use_jaccard_similarity=0.6\n",
    "        )\n",
    "\n",
    "        original_sentences.append(line['text'])\n",
    "        non_semantic_tokens.append(non_semantic)\n",
    "        semantic_tokens.append(semantic)\n",
    "\n",
    "        pbar.set_postfix(discarded=discarded_count, refresh=False)\n",
    "\n",
    "    except:\n",
    "        # print('Discarded: ')\n",
    "        # pretty_print(line, print_indexes=True)\n",
    "        # print()\n",
    "        discarded_count += 1\n",
    "\n",
    "print(f'Original sentence                : {original_sentences[0]}')\n",
    "print(f'Preprocessed sentence            : {\" \".join(non_semantic_tokens[0])}')\n",
    "print(f'Preprocessed sentence with senses: {\" \".join(semantic_tokens[0])}')\n",
    "print(f'Discarded {discarded_count} out of 500000')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now despite discarding some sentences at least we can rest assured that only senses for the specified tokens appear in the dataset (before we could've had \"word\", \"word%1:10:00\" and so on and this would've affected the similarity score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could filter out uncommon words that have a low frequency but we can do the same specifying the `min_count` parameter of the Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_frequency(sentences, frequency):\n",
    "\n",
    "    ## remove words that appear only once\n",
    "    frequency_dict = defaultdict(int)\n",
    "    for sentence in tqdm(sentences):\n",
    "        # tokens = sentence.split()\n",
    "        # for token in tokens:\n",
    "        for token in sentence:\n",
    "            frequency_dict[token] += 1\n",
    "\n",
    "    texts = [[token for token in sentence.split() if frequency_dict[token] > frequency]\n",
    "            for sentence in sentences]\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ff_word_sentences = filter_frequency(word_sentences, 1)\n",
    "# ff_sense_sentences = filter_frequency(sense_sentences, 1)\n",
    "\n",
    "# for i in range(10):\n",
    "#   print(ff_word_sentences[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the preprocessed dictionary to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "514594it [00:05, 98033.62it/s] \n"
     ]
    }
   ],
   "source": [
    "# Specify the output JSONLines file. Set it to None to skip saving the dataset\n",
    "output_file = 'preprocessed_dataset_v2_stanza.jsonl'\n",
    "# output_file = None\n",
    "\n",
    "if output_file is not None:\n",
    "\n",
    "    # Open the JSONLines file for writing\n",
    "    with jsonlines.open(output_file, mode='w') as writer:\n",
    "\n",
    "        for original_sentence, non_semantic_tokens_sentence, semantic_tokens_sentence in tqdm(zip(original_sentences, non_semantic_tokens, semantic_tokens)):\n",
    "\n",
    "            # Create a dictionary for each set of strings\n",
    "            data = {\n",
    "                'original': original_sentence,\n",
    "                'non_semantic': non_semantic_tokens_sentence,\n",
    "                'semantic': semantic_tokens_sentence\n",
    "            }\n",
    "            \n",
    "            # Write the dictionary to the JSONLines file\n",
    "            writer.write(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
