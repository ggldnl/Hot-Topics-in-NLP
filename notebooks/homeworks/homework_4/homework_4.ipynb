{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_path = r'/home/daniel/Git/Hot-Topics-in-NLP/notebooks/homeworks/homework_4/test_images_resized'\n",
    "train_images_path = r'/home/daniel/Git/Hot-Topics-in-NLP/notebooks/homeworks/homework_4/train_trial_images/train_v1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP Pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an image, the CLIPVisionModel returns the embedding of the respective text description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1391,  0.1016, -0.2362,  ...,  0.1568,  0.2143,  0.2181],\n",
       "         [ 0.1976,  0.4183, -0.8494,  ...,  0.1030,  1.4616,  0.4321],\n",
       "         [ 0.1630,  0.2004, -1.0608,  ..., -0.0640,  1.3778,  0.5354],\n",
       "         ...,\n",
       "         [ 0.0552, -0.0113, -0.6040,  ...,  0.2935,  1.0303, -0.0316],\n",
       "         [ 0.1673,  0.0491, -0.3833,  ...,  0.3797,  0.9275, -0.0212],\n",
       "         [ 0.1937,  0.2567, -0.4325,  ...,  0.2957,  1.2151, -0.0417]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoProcessor, CLIPVisionModel\n",
    "\n",
    "model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "pooled_output = outputs.pooler_output  # pooled CLS states\n",
    "\n",
    "# Access the image embeddings\n",
    "image_embedding = outputs.last_hidden_state \n",
    "image_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given some text, the CLIPTextModel returns an image corresponding to the embedding of the provided text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abracadabra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each row of the dataset we have:\n",
    "1. the word\n",
    "2. a minimal context\n",
    "3. a list of 10 images, among which only one refers to the real word context\n",
    "\n",
    "We can use the pretrained version of CLIP this way:\n",
    "1. prompt clip injecting the word and the respective context\n",
    "2. compute the text embedding for the prompt\n",
    "3. compute the image embeddings for each of the 10 images\n",
    "4. measure the cosine similarity between the text embedding and each of the 10 image embeddings,\n",
    "    ranking them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def min_cosine(target_vector, vector_list):\n",
    "    similarities = [cosine_similarity([target_vector], [vector])[0, 0] for vector in vector_list]\n",
    "    best_index = np.argmin(similarities)\n",
    "    best_vector = vector_list[best_index]\n",
    "    return best_vector, similarities[best_index]\n",
    "\n",
    "def cosine_similarity_key(vector1, vector2):\n",
    "    return cosine_similarity([vector1], [vector2])[0, 0]\n",
    "\n",
    "def rank_cosine(target_vector, vector_list):\n",
    "    sorted_vectors = sorted(vector_list, key=lambda vector: cosine_similarity_key(target_vector, vector), reverse=True)\n",
    "    return sorted_vectors\n",
    "\n",
    "# Example usage:\n",
    "target_vector = np.array([1, 2, 3])  # Replace with your actual target vector\n",
    "vector_list = [np.array([4, 5, 6]), np.array([7, 8, 9]), np.array([10, 11, 12])]  # Replace with your actual list of vectors\n",
    "\n",
    "best_vector, min_similarity = min_cosine(target_vector, vector_list)\n",
    "\n",
    "print(f\"The vector with the smallest cosine similarity is: {best_vector}\")\n",
    "print(f\"Cosine similarity value: {min_similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for test_entry, gold_entry in zip(test_data, test_goal):\n",
    "    word, minimal_context, image_list = test_entry\n",
    "    prompt = f'This is a picture of {word} in the context of {minimal_context}'\n",
    "\n",
    "    prompt_embedding = ...\n",
    "    images_embeddings = []\n",
    "    for image in image_list:\n",
    "        image_embedding = ...\n",
    "        images_embeddings.append(image_embedding)\n",
    "\n",
    "    ranked_embeddings = rank_cosine ...\n",
    "\n",
    "    predictions.append(ranked_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "1. Use wordnet to inject synonyms and make data augmentation\n",
    "2. Train your own CLIP-like model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
