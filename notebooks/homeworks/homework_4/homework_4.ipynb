{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('goal', 'football goal', ['image.4418.jpg', 'image.4416.jpg', 'image.4417.jpg', 'image.4413.jpg', 'image.4412.jpg', 'image.4415.jpg', 'image.4419.jpg', 'image.4414.jpg', 'image.2166.jpg', 'image.1150.jpg'], 'image.2166.jpg')\n",
      "('mustard', 'mustard seed', ['image.4429.png', 'image.4422.jpg', 'image.4423.jpg', 'image.4424.jpg', 'image.4421.jpg', 'image.4427.jpg', 'image.4426.jpg', 'image.4420.jpg', 'image.4425.jpg', 'image.4428.jpg'], 'image.4429.png')\n",
      "('seat', 'eating seat', ['image.4435.jpg', 'image.4436.jpg', 'image.1166.jpg', 'image.4430.jpg', 'image.4433.jpg', 'image.4432.jpg', 'image.4438.jpg', 'image.4434.jpg', 'image.4431.jpg', 'image.4437.jpg'], 'image.4432.jpg')\n",
      "('navigate', 'navigate the web', ['image.4439.jpg', 'image.4440.jpg', 'image.4441.jpg', 'image.4442.jpg', 'image.4444.jpg', 'image.4445.jpg', 'image.1435.jpg', 'image.4446.png', 'image.1434.jpg', 'image.4443.jpg'], 'image.1435.jpg')\n",
      "('butterball', 'butterball person', ['image.4454.jpg', 'image.4450.jpg', 'image.4455.jpg', 'image.4453.jpg', 'image.4448.jpg', 'image.1253.jpg', 'image.4451.jpg', 'image.4452.jpg', 'image.4447.jpg', 'image.4449.jpg'], 'image.4455.jpg')\n",
      "('neptune', 'neptune statue', ['image.4459.jpg', 'image.4461.jpg', 'image.4456.jpg', 'image.4462.jpg', 'image.4460.jpg', 'image.4457.jpg', 'image.4464.jpg', 'image.4458.jpg', 'image.4465.jpg', 'image.4463.jpg'], 'image.4464.jpg')\n",
      "('tympanum', 'ear tympanum', ['image.4468.jpg', 'image.4475.jpg', 'image.4474.jpg', 'image.4467.jpg', 'image.4471.jpg', 'image.4469.jpg', 'image.4473.png', 'image.4470.jpg', 'image.4466.jpg', 'image.4472.jpg'], 'image.4472.jpg')\n",
      "('thymus', 'wild thymus', ['image.4482.jpg', 'image.4479.jpg', 'image.4480.jpg', 'image.4477.jpg', 'image.4483.jpg', 'image.4478.jpg', 'image.4485.jpg', 'image.4476.jpg', 'image.4481.jpg', 'image.4484.jpg'], 'image.4476.jpg')\n",
      "('tender', 'tender embrace', ['image.4491.jpg', 'image.4489.jpg', 'image.4495.jpg', 'image.4494.jpg', 'image.4492.jpg', 'image.4488.jpg', 'image.4493.jpg', 'image.4487.jpg', 'image.4490.jpg', 'image.4486.jpg'], 'image.4495.jpg')\n",
      "('mill', 'mill grinding', ['image.4498.jpg', 'image.4504.jpg', 'image.4501.jpg', 'image.4500.jpg', 'image.4497.jpg', 'image.2324.jpg', 'image.4496.jpg', 'image.4503.jpg', 'image.4502.jpg', 'image.4499.jpg'], 'image.4499.jpg')\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "test_images_path = r'/home/daniel/Git/Hot-Topics-in-NLP/notebooks/homeworks/homework_4/test_images_resized'\n",
    "en_test_data_path = r'/home/daniel/Git/Hot-Topics-in-NLP/notebooks/homeworks/homework_4/test.data.v1.1.gold/en.test.data.v1.1.txt'\n",
    "en_test_data_gold_path = r'/home/daniel/Git/Hot-Topics-in-NLP/notebooks/homeworks/homework_4/test.data.v1.1.gold/en.test.gold.v1.1.txt'\n",
    "\n",
    "test_data_path = en_test_data_path\n",
    "test_data_gold_path = en_test_data_gold_path\n",
    "\n",
    "# train_images_path = r'/home/daniel/Git/Hot-Topics-in-NLP/notebooks/homeworks/homework_4/train_trial_images/train_v1'\n",
    "\n",
    "# word, word context, images_list, gold\n",
    "test_data = []\n",
    "with open(test_data_path, 'r', encoding='utf-8') as test_data_file, \\\n",
    "    open(test_data_gold_path, 'r', encoding='utf-8') as test_data_gold_file:\n",
    "\n",
    "    test_data_reader = csv.reader(test_data_file, delimiter='\\t')\n",
    "    test_gold_reader = csv.reader(test_data_gold_file, delimiter='\\t')\n",
    "\n",
    "    for data_line, gold_line in zip(test_data_reader, test_gold_reader):\n",
    "        test_data.append((data_line[0], data_line[1], data_line[2:], gold_line[0]))\n",
    "\n",
    "for test_entry in test_data[:10]:\n",
    "    print(test_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP Pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/snap/core20/current/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /lib/x86_64-linux-gnu/libproxy.so.1)\n",
      "Failed to load module: /home/daniel/snap/code/common/.cache/gio-modules/libgiolibproxy.so\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an image, the CLIPVisionModel returns the embedding of the respective text description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1391,  0.1016, -0.2362,  ...,  0.1568,  0.2143,  0.2181],\n",
       "         [ 0.1976,  0.4183, -0.8494,  ...,  0.1030,  1.4616,  0.4321],\n",
       "         [ 0.1630,  0.2004, -1.0608,  ..., -0.0640,  1.3778,  0.5354],\n",
       "         ...,\n",
       "         [ 0.0552, -0.0113, -0.6040,  ...,  0.2935,  1.0303, -0.0316],\n",
       "         [ 0.1673,  0.0491, -0.3833,  ...,  0.3797,  0.9275, -0.0212],\n",
       "         [ 0.1937,  0.2567, -0.4325,  ...,  0.2957,  1.2151, -0.0417]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoProcessor, CLIPVisionModel\n",
    "\n",
    "model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "pooled_output = outputs.pooler_output  # pooled CLS states\n",
    "\n",
    "# Access the image embeddings\n",
    "image_embedding = outputs.last_hidden_state \n",
    "image_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given some text, the CLIPTextModel returns an image corresponding to the embedding of the provided text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3393,  0.1165,  0.1020,  ...,  0.2468,  0.5906,  0.1013],\n",
       "         [ 1.9753, -0.5844,  0.3685,  ...,  1.1658,  0.8050, -0.9801],\n",
       "         [ 1.0580, -0.9600,  1.0018,  ..., -0.5155, -0.1437, -1.9444],\n",
       "         ...,\n",
       "         [ 0.3059, -1.5037, -0.4022,  ..., -0.0224,  0.9105, -0.3916],\n",
       "         [ 1.0118, -0.6701,  1.7742,  ..., -0.1556, -0.0250, -1.5062],\n",
       "         [-0.5152,  0.1658,  0.8876,  ..., -0.0675, -0.4551, -1.7960]],\n",
       "\n",
       "        [[ 0.3393,  0.1165,  0.1020,  ...,  0.2468,  0.5906,  0.1013],\n",
       "         [ 1.9753, -0.5844,  0.3685,  ...,  1.1658,  0.8050, -0.9801],\n",
       "         [ 1.0580, -0.9600,  1.0018,  ..., -0.5155, -0.1437, -1.9444],\n",
       "         ...,\n",
       "         [ 0.3059, -1.5037, -0.4022,  ..., -0.0224,  0.9105, -0.3916],\n",
       "         [-0.1433, -0.5163,  1.7099,  ..., -0.0795,  0.3609, -1.2437],\n",
       "         [ 0.0426,  0.0189,  1.2740,  ..., -0.4217, -0.4393, -1.3016]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, CLIPTextModel\n",
    "\n",
    "model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "pooled_output = outputs.pooler_output  # pooled (EOS token) states\n",
    "\n",
    "text_embedding = outputs.last_hidden_state\n",
    "text_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each row of the dataset we have:\n",
    "1. the word\n",
    "2. a minimal context\n",
    "3. a list of 10 images, among which only one refers to the real word context\n",
    "\n",
    "We can use the pretrained version of CLIP this way:\n",
    "1. prompt clip injecting the word and the respective context\n",
    "2. compute the text embedding for the prompt\n",
    "3. compute the image embeddings for each of the 10 images\n",
    "4. measure the cosine similarity between the text embedding and each of the 10 image embeddings,\n",
    "    ranking them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Example usage:\\ntarget_vector = np.array([1, 2, 3])  # Replace with your actual target vector\\nvector_list = [np.array([4, 5, 6]), np.array([7, 8, 9]), np.array([10, 11, 12])]  # Replace with your actual list of vectors\\n\\n# best_vector, min_similarity = min_cosine(target_vector, vector_list)\\nprint(f\"The vector with the smallest cosine similarity is: {best_vector}\")\\nprint(f\"Cosine similarity value: {min_similarity}\")\\nprint(f\"Ranked by similarity: {rank_cosine(target_vector, vector_list)}\")\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def min_cosine(target_vector, vector_list):\n",
    "    similarities = [cosine_similarity([target_vector], [vector])[0, 0] for vector in vector_list]\n",
    "    best_index = np.argmin(similarities)\n",
    "    best_vector = vector_list[best_index]\n",
    "    return best_vector, similarities[best_index]\n",
    "\n",
    "def cosine_similarity_key(vector1, vector2):\n",
    "    return cosine_similarity([vector1], [vector2])[0, 0]\n",
    "\n",
    "def rank_cosine(target_vector, vector_list):\n",
    "    sorted_vectors = sorted(vector_list, key=lambda vector: cosine_similarity_key(target_vector, vector[1]), reverse=True)\n",
    "    return sorted_vectors\n",
    "\n",
    "\"\"\"\n",
    "# Example usage:\n",
    "target_vector = np.array([1, 2, 3])  # Replace with your actual target vector\n",
    "vector_list = [np.array([4, 5, 6]), np.array([7, 8, 9]), np.array([10, 11, 12])]  # Replace with your actual list of vectors\n",
    "\n",
    "# best_vector, min_similarity = min_cosine(target_vector, vector_list)\n",
    "print(f\"The vector with the smallest cosine similarity is: {best_vector}\")\n",
    "print(f\"Cosine similarity value: {min_similarity}\")\n",
    "print(f\"Ranked by similarity: {rank_cosine(target_vector, vector_list)}\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 463/463 [06:09<00:00,  1.25it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import tqdm\n",
    "import os\n",
    "import torch\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for word, context, images_list, gold in tqdm.tqdm(test_data):\n",
    "\n",
    "        prompt = f'This is a picture of {word} in the context of {context}'\n",
    "    \n",
    "        all_images = [Image.open(os.path.join(test_images_path, image_name)) for image_name in images_list]\n",
    "\n",
    "        inputs = processor(text=[prompt], \n",
    "                        images=all_images, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "        # Model prediction\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # Image-text similarity score\n",
    "        logits_per_image = outputs.logits_per_image\n",
    "\n",
    "        # Take the softmax to get the label probabilities\n",
    "        probs = logits_per_image.softmax(dim=1)\n",
    "\n",
    "        # Filter the scores and return indices\n",
    "        values, indices = logits_per_image.squeeze().topk(10)\n",
    "\n",
    "        top_images, top_scores = [], []\n",
    "        for score, index in zip(values, indices):\n",
    "            top_images.append(images_list[int(index.numpy())])\n",
    "            score = score.numpy().tolist()\n",
    "            top_scores.append(round(score, 3))\n",
    "\n",
    "        predictions.append([(image, score) for image, score in zip(top_images, top_scores)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('image.2166.jpg', 30.073), ('image.4417.jpg', 23.324), ('image.4415.jpg', 22.814), ('image.4413.jpg', 22.526), ('image.4414.jpg', 22.496), ('image.4418.jpg', 21.707), ('image.1150.jpg', 20.921), ('image.4419.jpg', 20.517), ('image.4416.jpg', 19.79), ('image.4412.jpg', 17.379)]\n",
      "[('image.4420.jpg', 30.64), ('image.4429.png', 30.581), ('image.4423.jpg', 27.307), ('image.4427.jpg', 25.468), ('image.4424.jpg', 21.728), ('image.4422.jpg', 21.315), ('image.4426.jpg', 20.691), ('image.4425.jpg', 19.997), ('image.4421.jpg', 18.283), ('image.4428.jpg', 17.721)]\n",
      "[('image.4432.jpg', 24.159), ('image.4438.jpg', 23.431), ('image.4431.jpg', 21.651), ('image.4434.jpg', 21.015), ('image.4437.jpg', 20.835), ('image.4433.jpg', 20.452), ('image.4435.jpg', 20.379), ('image.4430.jpg', 19.616), ('image.4436.jpg', 18.831), ('image.1166.jpg', 17.808)]\n",
      "[('image.1435.jpg', 25.301), ('image.4446.png', 24.578), ('image.4445.jpg', 23.133), ('image.1434.jpg', 22.789), ('image.4441.jpg', 21.76), ('image.4442.jpg', 20.913), ('image.4443.jpg', 19.547), ('image.4439.jpg', 18.672), ('image.4444.jpg', 18.63), ('image.4440.jpg', 18.445)]\n",
      "[('image.4455.jpg', 25.655), ('image.4447.jpg', 22.444), ('image.1253.jpg', 21.056), ('image.4451.jpg', 20.519), ('image.4452.jpg', 20.228), ('image.4454.jpg', 20.174), ('image.4450.jpg', 18.818), ('image.4448.jpg', 17.807), ('image.4449.jpg', 17.402), ('image.4453.jpg', 17.361)]\n",
      "[('image.4464.jpg', 33.139), ('image.4457.jpg', 30.829), ('image.4461.jpg', 26.378), ('image.4462.jpg', 24.349), ('image.4463.jpg', 24.007), ('image.4458.jpg', 23.741), ('image.4465.jpg', 23.651), ('image.4456.jpg', 22.565), ('image.4459.jpg', 21.613), ('image.4460.jpg', 20.519)]\n",
      "[('image.4472.jpg', 28.864), ('image.4471.jpg', 28.374), ('image.4466.jpg', 25.866), ('image.4469.jpg', 24.126), ('image.4474.jpg', 22.839), ('image.4475.jpg', 22.075), ('image.4468.jpg', 20.377), ('image.4467.jpg', 19.159), ('image.4473.png', 17.175), ('image.4470.jpg', 16.639)]\n",
      "[('image.4476.jpg', 26.989), ('image.4477.jpg', 26.851), ('image.4482.jpg', 25.695), ('image.4484.jpg', 22.732), ('image.4485.jpg', 20.359), ('image.4480.jpg', 19.81), ('image.4478.jpg', 18.791), ('image.4479.jpg', 17.404), ('image.4481.jpg', 17.223), ('image.4483.jpg', 12.742)]\n",
      "[('image.4495.jpg', 24.789), ('image.4486.jpg', 23.173), ('image.4493.jpg', 21.11), ('image.4490.jpg', 20.443), ('image.4494.jpg', 19.299), ('image.4489.jpg', 18.881), ('image.4488.jpg', 18.644), ('image.4491.jpg', 18.623), ('image.4487.jpg', 16.549), ('image.4492.jpg', 16.326)]\n",
      "[('image.4499.jpg', 32.407), ('image.4501.jpg', 26.191), ('image.2324.jpg', 25.601), ('image.4504.jpg', 24.619), ('image.4496.jpg', 24.058), ('image.4502.jpg', 23.713), ('image.4503.jpg', 23.122), ('image.4500.jpg', 21.155), ('image.4497.jpg', 20.583), ('image.4498.jpg', 19.127)]\n"
     ]
    }
   ],
   "source": [
    "for pred in predictions[:10]:\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.5269978401727862\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "ground_truth = [gold for word, context, images_list, gold in test_data]\n",
    "predicted = [pred[0][0] for pred in predictions]\n",
    "accuracy = accuracy_score(ground_truth, predicted)\n",
    "print(f'Model accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "1. Use wordnet to inject synonyms and make data augmentation\n",
    "2. Train your own CLIP-like model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
